{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQFmTc45aGHn"
      },
      "source": [
        "### Install custom version of simulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hosUAwLdqHr0",
        "outputId": "5732eb95-608b-4c43-df66-0f3f916e08ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libglew-dev is already the newest version (2.0.0-5).\n",
            "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libosmesa6-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "software-properties-common is already the newest version (0.96.24.32.18).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "patchelf is already the newest version (0.9-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61oSVQHUaOGc",
        "outputId": "8f03daba-091d-4a3b-bb6b-ee42818ffb16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4FQVdzlaWr6",
        "outputId": "acadc5c0-5071-4294-c28a-c01245321e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: free-mujoco-py in /usr/local/lib/python3.7/dist-packages (2.1.6)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: fasteners==0.15 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.15)\n",
            "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.29.30)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (2.19.2)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.12.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install free-mujoco-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuXOKel-aYs9"
      },
      "outputs": [],
      "source": [
        "import mujoco_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9PjnzJnDw5K",
        "outputId": "cb2e469a-7a4e-4263-d2aa-967f7bc61055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (0.21.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.11.0+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3) (2022.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrsTmY6KoyCg",
        "outputId": "ec3f246d-35c4-40ea-bb2e-7d7b0640038b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybullet==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pybullet==3.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSZa-9ahlOIa"
      },
      "outputs": [],
      "source": [
        "!rm -rf surrol "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glj7D3hxZCPw",
        "outputId": "abaca337-4ff2-42f0-b743-c40c73026c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'surrol'...\n",
            "remote: Enumerating objects: 817, done.\u001b[K\n",
            "remote: Counting objects: 100% (340/340), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 817 (delta 215), reused 253 (delta 161), pack-reused 477\u001b[K\n",
            "Receiving objects: 100% (817/817), 23.38 MiB | 18.78 MiB/s, done.\n",
            "Resolving deltas: 100% (438/438), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bogatovam/surrol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YxgPrOsZQQQ",
        "outputId": "8eeb19d7-a8e2-4ac2-9643-ce4f1644b62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/surrol\n"
          ]
        }
      ],
      "source": [
        "%cd surrol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHHbB3Uyptr5",
        "outputId": "22a21c48-5343-4aa5-bb18-4fc1f4bcce52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'aif-dev' set up to track remote branch 'aif-dev' from 'origin'.\n",
            "Switched to a new branch 'aif-dev'\n"
          ]
        }
      ],
      "source": [
        "!git checkout aif-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMWROgXxZY3O"
      },
      "outputs": [],
      "source": [
        "!git fetch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-VvuHSZqBq",
        "outputId": "d826c5f9-6ad6-4503-ca11-d3a504c1c972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping surrol as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall surrol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8H6B6jqZb9Y",
        "outputId": "d1baa5fc-b81b-4bbb-8e11-d1ef555127d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/surrol\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (0.4.7)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (4.1.2.30)\n",
            "Requirement already satisfied: roboticstoolbox-python in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (1.7.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio->surrol==0.1.0) (9.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio->surrol==0.1.0) (1.21.6)\n",
            "Requirement already satisfied: rtb-data in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: pgraph-python in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (0.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: spatialgeometry~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: ansitable in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (0.9.6)\n",
            "Requirement already satisfied: spatialmath-python~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.6)\n",
            "Requirement already satisfied: swift-sim~=1.0.0 in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: colored in /usr/local/lib/python3.7/dist-packages (from spatialmath-python~=1.0.0->roboticstoolbox-python->surrol==0.1.0) (1.4.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.7/dist-packages (from swift-sim~=1.0.0->roboticstoolbox-python->surrol==0.1.0) (10.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->roboticstoolbox-python->surrol==0.1.0) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->roboticstoolbox-python->surrol==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->surrol==0.1.0) (1.2.1)\n",
            "Installing collected packages: surrol\n",
            "  Running setup.py develop for surrol\n",
            "Successfully installed surrol-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M_YVGw5bEyM",
        "outputId": "85705664-7708-4361-9535-73d057b704ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 24\n",
            "drwxr-xr-x  1 root root 4096 May 26 12:16 .\n",
            "drwxr-xr-x  1 root root 4096 May 26 12:05 ..\n",
            "drwxr-xr-x  4 root root 4096 May 17 13:38 .config\n",
            "drwx------  5 root root 4096 May 26 12:10 drive\n",
            "drwxr-xr-x  1 root root 4096 May 17 13:39 sample_data\n",
            "drwxr-xr-x 10 root root 4096 May 26 12:16 surrol\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5OZo3XyzE3c"
      },
      "source": [
        "### Base code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTml9fxe5bM"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMvevkgnNCiW"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from abc import abstractmethod\n",
        "import torch\n",
        "from stable_baselines3.common.preprocessing import maybe_transpose, is_image_space\n",
        "from stable_baselines3.common.utils import get_device, obs_as_tensor, is_vectorized_observation\n",
        "from typing import Union\n",
        "import warnings\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for all models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def set_training_mode(self, mode: bool) -> None:\n",
        "        self.train(mode)\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *inputs):\n",
        "        \"\"\"\n",
        "        Forward pass logic\n",
        "\n",
        "        :return: Model output\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _get_constructor_parameters(self):\n",
        "        return dict()\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save model to a given location.\n",
        "\n",
        "        :param path:\n",
        "        \"\"\"\n",
        "        torch.save({\"state_dict\": self.state_dict(), \"data\": self._get_constructor_parameters()}, path)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str, device: Union[torch.device, str] = \"auto\") -> \"BaseModel\":\n",
        "        \"\"\"\n",
        "        Load model from path.\n",
        "\n",
        "        :param path:\n",
        "        :param device: Device on which the policy should be loaded.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        device = get_device(device)\n",
        "        saved_variables = torch.load(path, map_location=device)\n",
        "\n",
        "        # Create policy object\n",
        "        model = cls(**saved_variables[\"data\"])  # pytype: disable=not-instantiable\n",
        "        # Load weights\n",
        "        model.load_state_dict(saved_variables[\"state_dict\"])\n",
        "        model.to(device)\n",
        "        return model\n",
        "\n",
        "\n",
        "    def load_to_current_model(self, path: str, device: Union[torch.device, str] = \"auto\"):\n",
        "        \"\"\"\n",
        "        Load model from path.\n",
        "\n",
        "        :param path:\n",
        "        :param device: Device on which the policy should be loaded.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        device = get_device(device)\n",
        "        saved_variables = torch.load(path, map_location=device)\n",
        "\n",
        "        # Load weights\n",
        "        self.load_state_dict(saved_variables[\"state_dict\"])\n",
        "        self.to(device)\n",
        "        return self\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Model prints with number of trainable parameters\n",
        "        \"\"\"\n",
        "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yONHMB1NCV8"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class TensorboardWriter():\n",
        "    def __init__(self, log_dir, enabled):\n",
        "        self.writer = None\n",
        "        self.selected_module = \"\"\n",
        "\n",
        "        if enabled:\n",
        "            log_dir = str(log_dir)\n",
        "\n",
        "            # Retrieve vizualization writer.\n",
        "            succeeded = False\n",
        "            for module in [\"torch.utils.tensorboard\", \"tensorboardX\"]:\n",
        "                try:\n",
        "                    self.writer = importlib.import_module(module).SummaryWriter(log_dir)\n",
        "                    succeeded = True\n",
        "                    break\n",
        "                except ImportError:\n",
        "                    succeeded = False\n",
        "                self.selected_module = module\n",
        "\n",
        "            if not succeeded:\n",
        "                message = \"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \" \\\n",
        "                    \"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \" \\\n",
        "                    \"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"\n",
        "                print(message)\n",
        "\n",
        "        self.step = 0\n",
        "        self.mode = ''\n",
        "\n",
        "        self.tb_writer_ftns = {\n",
        "            'add_scalar', 'add_scalars', 'add_image', 'add_video', 'add_images', 'add_audio',\n",
        "            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding'\n",
        "        }\n",
        "        self.tag_mode_exceptions = {'add_histogram', 'add_embedding'}\n",
        "        self.timer = datetime.now()\n",
        "\n",
        "    def set_step(self, step, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.step = step\n",
        "        if step == 0:\n",
        "            self.timer = datetime.now()\n",
        "        else:\n",
        "            duration = datetime.now() - self.timer\n",
        "            self.add_scalar('steps_per_sec', 1 / duration.total_seconds())\n",
        "            self.timer = datetime.now()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        \"\"\"\n",
        "        If visualization is configured to use:\n",
        "            return add_data() methods of tensorboard with additional information (step, tag) added.\n",
        "        Otherwise:\n",
        "            return a blank function handle that does nothing\n",
        "        \"\"\"\n",
        "        if name in self.tb_writer_ftns:\n",
        "            add_data = getattr(self.writer, name, None)\n",
        "\n",
        "            def wrapper(tag, data, *args, **kwargs):\n",
        "                if add_data is not None:\n",
        "                    # add mode(train/valid) tag\n",
        "                    if name not in self.tag_mode_exceptions:\n",
        "                        tag = '{}/{}'.format(tag, self.mode)\n",
        "                    add_data(tag, data, self.step, *args, **kwargs)\n",
        "            return wrapper\n",
        "        else:\n",
        "            # default action for returning methods defined in this class, set_step() for instance.\n",
        "            try:\n",
        "                attr = object.__getattr__(name)\n",
        "            except AttributeError:\n",
        "                raise AttributeError(\"type object '{}' has no attribute '{}'\".format(self.selected_module, name))\n",
        "            return attr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Ggwi7-nNKoO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from itertools import repeat\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "def ensure_dir(dirname):\n",
        "    dirname = Path(dirname)\n",
        "    if not dirname.is_dir():\n",
        "        dirname.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "\n",
        "def get_all_files_from_dir(dirname):\n",
        "    return [join(dirname, f) for f in listdir(dirname) if isfile(join(dirname, f))]\n",
        "\n",
        "\n",
        "def read_json(fname):\n",
        "    fname = Path(fname)\n",
        "    with fname.open('rt') as handle:\n",
        "        return json.load(handle, object_hook=OrderedDict)\n",
        "\n",
        "\n",
        "def write_json(content, fname):\n",
        "    fname = Path(fname)\n",
        "    with fname.open('wt') as handle:\n",
        "        json.dump(content, handle, indent=4, sort_keys=False)\n",
        "\n",
        "\n",
        "def inf_loop(data_loader):\n",
        "    ''' wrapper function for endless data loader. '''\n",
        "    for loader in repeat(data_loader):\n",
        "        yield from loader\n",
        "\n",
        "\n",
        "def prepare_device(n_gpu_use):\n",
        "    \"\"\"\n",
        "    setup GPU device if available. get gpu device indices which are used for DataParallel\n",
        "    \"\"\"\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    if n_gpu_use > 0 and n_gpu == 0:\n",
        "        print(\"Warning: There\\'s no GPU available on this machine,\"\n",
        "              \"training will be performed on CPU.\")\n",
        "        n_gpu_use = 0\n",
        "    if n_gpu_use > n_gpu:\n",
        "        print(f\"Warning: The number of GPU\\'s configured to use is {n_gpu_use}, but only {n_gpu} are \"\n",
        "              \"available on this machine.\")\n",
        "        n_gpu_use = n_gpu\n",
        "    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
        "    list_ids = list(range(n_gpu_use))\n",
        "    return device, list_ids\n",
        "\n",
        "\n",
        "class MetricTracker:\n",
        "    def __init__(self, *keys, writer=None):\n",
        "        self.writer = writer\n",
        "        self._data = pd.DataFrame(index=keys, columns=['total', 'counts', 'average'])\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        for col in self._data.columns:\n",
        "            self._data[col].values[:] = 0\n",
        "\n",
        "    def update(self, key, value, n=1):\n",
        "        if self.writer is not None:\n",
        "            self.writer.add_scalar(key, value)\n",
        "        self._data.total[key] += value * n\n",
        "        self._data.counts[key] += n\n",
        "        self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
        "\n",
        "    def avg(self, key):\n",
        "        return self._data.average[key]\n",
        "\n",
        "    def result(self):\n",
        "        return dict(self._data.average)\n",
        "\n",
        "\n",
        "def set_random_seed(seed: int, device: str = 'cpu') -> None:\n",
        "    # Seed python RNG\n",
        "    random.seed(seed)\n",
        "    # Seed numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if device == 'gpu':\n",
        "        # Deterministic operations for CuDNN, it may impact performances\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def create_dirs(dirs):\n",
        "    try:\n",
        "        for dir_ in dirs:\n",
        "            if not os.path.exists(dir_):\n",
        "                os.makedirs(dir_)\n",
        "        return 0\n",
        "    except Exception as err:\n",
        "        print(\"Creating directories error: {0}\".format(err))\n",
        "        exit(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IQN-i3OzJ3k"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE5f5BPucovD"
      },
      "outputs": [],
      "source": [
        "!pip install omegaconf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NL6dZRWbmBA"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR_PATH = '/content/drive/MyDrive/thesis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aPqqK1VcklS"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "from os.path import join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UZiaWgcdCWT"
      },
      "outputs": [],
      "source": [
        "_global_config_yaml = \"\"\"\n",
        "experiment_description: \"test mdp sac with this task\"\n",
        "\n",
        "tb_log_folder: logs/mujoco/tensorboard/{experiment_name}\n",
        "seed: 42\n",
        "\n",
        "hparams:\n",
        "  n_epochs: 150\n",
        "  steps_per_epoch: 100\n",
        "  n_training_iterations: 10\n",
        "  n_rollout_episodes: 10\n",
        "  n_warmap_episodes: 50\n",
        "\n",
        "should_save_model: true\n",
        "model_path: models/mujoco/{experiment_name}\n",
        "model_save_timer: 5\n",
        "\n",
        "\n",
        "should_save_episode_video: true\n",
        "episode_video_timer: 5\n",
        "video_log_folder: logs/mujoco/gif/{experiment_name}\n",
        "\n",
        "\"\"\"\n",
        "_global_config = OmegaConf.create(_global_config_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLTOgcE1ddcz"
      },
      "outputs": [],
      "source": [
        "_cloud_yaml = \"\"\"\n",
        "device_id: cpu\n",
        "render_mode: none\n",
        "\n",
        "hparams:\n",
        "  memory_capacity: 50000\n",
        "  batch_size: 1024\n",
        "\n",
        "\"\"\"\n",
        "_cloud_config = OmegaConf.create(_cloud_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhOj1oIwdTeE"
      },
      "outputs": [],
      "source": [
        "_env_yaml = \"\"\"\n",
        "hparams:\n",
        "  polyak: 0.95\n",
        "  # HER\n",
        "  replay_strategy: future\n",
        "  replay_k: 4\n",
        "\n",
        "  #  Networks\n",
        "  actor_layers: [ 256, 512, 256 ]\n",
        "  transition_net_layers: [32, 128, 256, 128, 32]\n",
        "  value_net_layers: [ 256, 512, 256 ]\n",
        "  vae_layer_sizes: [64, 64, 64]\n",
        "\n",
        "  actor_lr: 0.0003\n",
        "  transition_net_lr: 0.000001\n",
        "  value_net_lr: 0.0003\n",
        "  alpha_lr: 0.0003\n",
        "  vae_lr: 0.00001\n",
        "\n",
        "  alpha: auto\n",
        "  gamma: 1.00\n",
        "  beta: 0.99\n",
        "\n",
        "  #  available values list:\n",
        "  #  sac_maximize\n",
        "  #  sac_minimise\n",
        "  #  sac_minimize_entropy\n",
        "  #  adapted_daif\n",
        "  efe_approximation_approach: adapted_daif\n",
        "\n",
        "  #  available values list:\n",
        "  #  SquashedDiagGaussianDistribution\n",
        "  #  StateDependentNoiseDistribution\n",
        "  actor_action_distribution: SquashedDiagGaussianDistribution\n",
        "\n",
        "  #  available values list:\n",
        "  #  mlp\n",
        "  #  gru\n",
        "  #  lstm\n",
        "  transition_network_type: mlp\n",
        "\n",
        "  vae_type: beta\n",
        "  # vae_pretrained_path: pretrained-vae/NeedleGrasp-v0_Beta821.pth\n",
        "\n",
        "vae:\n",
        "  tb_log_folder: logs/vae/tensorboard/{experiment_name}\n",
        "  hparams:\n",
        "    memory_capacity: 50000\n",
        "    batch_size: 4\n",
        "    image_shape: [ 64, 64, 3 ]\n",
        "    vae_seq_len: 4\n",
        "    vae_n_latent_dims: 64\n",
        "\n",
        "\"\"\"\n",
        "_env_config = OmegaConf.create(_env_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7vOWLz8cbiI"
      },
      "outputs": [],
      "source": [
        "def get_config(experiment_name, env_id, transition_network_type):\n",
        "    config = OmegaConf.create()\n",
        "    config = OmegaConf.merge(config, _global_config)\n",
        "    config = OmegaConf.merge(config, _env_config)\n",
        "    config = OmegaConf.merge(config, _cloud_config)\n",
        "\n",
        "    config.experiment_name = experiment_name\n",
        "    config.env_id = env_id\n",
        "    config.hparams.transition_network_type = transition_network_type\n",
        "    config.hparams.rnn_seq_len = 1 if transition_network_type == 'mlp' else 3\n",
        "    config.hparams.transition_net_layers = [32, 128, 256, 128, 32] if transition_network_type == 'mlp' else [128, 128, 128]\n",
        "\n",
        "    print(OmegaConf.to_yaml(config))\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvVNIZY8zLwJ"
      },
      "source": [
        "### hparams tuning loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6rjacSoi3jY"
      },
      "source": [
        "#### Main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hugnCD7PTaR5"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os.path\n",
        "from abc import abstractmethod, ABC\n",
        "from typing import Tuple, Optional, NamedTuple\n",
        "\n",
        "from stable_baselines3.common.utils import polyak_update\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from stable_baselines3.common.distributions import StateDependentNoiseDistribution, \\\n",
        "    TanhBijector, DiagGaussianDistribution, SquashedDiagGaussianDistribution\n",
        "from stable_baselines3.common.preprocessing import get_action_dim, is_image_space, maybe_transpose\n",
        "from stable_baselines3.common.torch_layers import create_mlp\n",
        "\n",
        "from datetime import datetime\n",
        "import os.path\n",
        "\n",
        "import gym\n",
        "\n",
        "import os.path\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import gym\n",
        "\n",
        "import threading\n",
        "import numpy as np\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vouX-hOcNNkL"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os.path\n",
        "from abc import abstractmethod, ABC\n",
        "from typing import Tuple, Optional, NamedTuple\n",
        "\n",
        "from stable_baselines3.common.utils import polyak_update\n",
        "\n",
        "import surrol.gym as surrol_gym\n",
        "from omegaconf import OmegaConf\n",
        "from stable_baselines3.common.distributions import StateDependentNoiseDistribution, \\\n",
        "    TanhBijector, DiagGaussianDistribution, SquashedDiagGaussianDistribution\n",
        "from stable_baselines3.common.preprocessing import get_action_dim, is_image_space, maybe_transpose\n",
        "from stable_baselines3.common.torch_layers import create_mlp\n",
        "\n",
        "from datetime import datetime\n",
        "import os.path\n",
        "\n",
        "import gym\n",
        "\n",
        "import os.path\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import gym\n",
        "\n",
        "import threading\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, env, max_episode_steps, buffer_size, sample_func, device):\n",
        "        self.observation_dim, self.goal_dim, self.action_dim, self.action_max = get_env_parameters(env)\n",
        "        self.device = device\n",
        "\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        self.size = buffer_size // self.max_episode_steps\n",
        "        # memory management\n",
        "        self.current_size = 0\n",
        "        self.n_transitions_stored = 0\n",
        "        self.sample_func = sample_func\n",
        "        # create the buffer to store info\n",
        "\n",
        "        self.observation_memory = np.empty([self.size, self.max_episode_steps, self.observation_dim], dtype=np.float32)\n",
        "        self.achieved_goal_memory = np.empty([self.size, self.max_episode_steps, self.goal_dim], dtype=np.float32)\n",
        "        self.desired_goal_memory = np.empty([self.size, self.max_episode_steps, self.goal_dim], dtype=np.float32)\n",
        "        self.actions_memory = np.empty([self.size, self.max_episode_steps, self.action_dim], dtype=np.float32)\n",
        "        self.info_memory = np.empty([self.size, self.max_episode_steps, 1], dtype=object)\n",
        "\n",
        "    # store the episode\n",
        "    def store_episode(self, observation, achieved_goal, desired_goal, action, n_episodes_to_store, info):\n",
        "        ids = self._get_storage_idx(inc=n_episodes_to_store)\n",
        "        # store the information\n",
        "        self.observation_memory[ids] = observation\n",
        "        self.achieved_goal_memory[ids] = achieved_goal\n",
        "        self.desired_goal_memory[ids] = desired_goal\n",
        "        self.actions_memory[ids] = action\n",
        "        self.info_memory[ids] = np.expand_dims(info, -1)\n",
        "\n",
        "        self.n_transitions_stored += self.max_episode_steps * n_episodes_to_store\n",
        "\n",
        "    # sample the data from the replay buffer\n",
        "    def sample(self, batch_size):\n",
        "        observation_buffer = self.observation_memory[:self.current_size]\n",
        "        achieved_goal_buffer = self.achieved_goal_memory[:self.current_size]\n",
        "        desired_goal_buffer = self.desired_goal_memory[:self.current_size]\n",
        "        actions_buffer = self.actions_memory[:self.current_size]\n",
        "        info_buffer = self.info_memory[:self.current_size]\n",
        "\n",
        "        return self.sample_func(observation_buffer,\n",
        "                                achieved_goal_buffer, desired_goal_buffer,\n",
        "                                actions_buffer, info_buffer,\n",
        "                                batch_size)\n",
        "\n",
        "    def _get_storage_idx(self, inc=None):\n",
        "        inc = inc or 1\n",
        "        if self.current_size + inc <= self.size:\n",
        "            idx = np.arange(self.current_size, self.current_size + inc)\n",
        "        elif self.current_size < self.size:\n",
        "            overflow = inc - (self.size - self.current_size)\n",
        "            idx_a = np.arange(self.current_size, self.size)\n",
        "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
        "            idx = np.concatenate([idx_a, idx_b])\n",
        "        else:\n",
        "            idx = np.random.randint(0, self.size, inc)\n",
        "        self.current_size = min(self.size, self.current_size + inc)\n",
        "        if inc == 1:\n",
        "            idx = idx[0]\n",
        "        return idx\n",
        "\n",
        "\n",
        "class HERSampler:\n",
        "    def __init__(self, replay_strategy, replay_k, rnn_seq_len, reward_func=None):\n",
        "        self.replay_strategy = replay_strategy\n",
        "        self.replay_k = replay_k\n",
        "        if self.replay_strategy == 'future':\n",
        "            self.future_p = 1 - (1. / (1 + replay_k))\n",
        "        else:\n",
        "            self.future_p = 0\n",
        "        self.reward_func = reward_func\n",
        "        self.rnn_seq_len = rnn_seq_len\n",
        "        self.total_seq_len = rnn_seq_len + 2\n",
        "\n",
        "    def sample_her_transitions(self, observation_buffer,\n",
        "                               achieved_goal_buffer, desired_goal_buffer,\n",
        "                               actions_buffer, info_buffer, batch_size):\n",
        "        # Trajectory length\n",
        "        trajectory_length = actions_buffer.shape[1]\n",
        "\n",
        "        # Buffer size\n",
        "        buffer_length = actions_buffer.shape[0]\n",
        "\n",
        "        # generate ids which trajectories to use\n",
        "        episode_ids = np.random.randint(low=0, high=buffer_length, size=batch_size)\n",
        "\n",
        "        # generate ids which timestamps to use\n",
        "        # - 2 because we sample for 3 sequential timestamps\n",
        "        t_samples = np.random.randint(low=0, high=trajectory_length - self.total_seq_len, size=batch_size)\n",
        "\n",
        "        # her idx\n",
        "        her_indexes = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
        "\n",
        "        # Sample 'future' timestamps for each 't_samples'\n",
        "        future_offset = np.random.uniform(size=batch_size) * (trajectory_length - self.total_seq_len - t_samples)\n",
        "        future_offset = future_offset.astype(int)\n",
        "        future_t = (t_samples + future_offset)[her_indexes]\n",
        "\n",
        "        sequential_batches = []\n",
        "        for time_i in range(self.total_seq_len):\n",
        "            t_i = self._sample_for_time(observation_buffer, achieved_goal_buffer, desired_goal_buffer, actions_buffer,\n",
        "                                        info_buffer,\n",
        "                                        episode_ids, t_samples, her_indexes, future_t,\n",
        "                                        batch_size=batch_size, time=time_i)\n",
        "            sequential_batches.append(t_i)\n",
        "\n",
        "        (_, achieved_goal_batch_t1, desired_goal_batch_t1, _, info) = sequential_batches[-2]\n",
        "\n",
        "        # Recompute the reward for the augmented 'desired_goal'\n",
        "        # todo use achieved_goal_batch_t2 and desired_goal_batch_t1?\n",
        "        reward_batch = self.reward_func(achieved_goal_batch_t1, desired_goal_batch_t1, info=info)\n",
        "        # Recompute the termination state for the augmented 'desired_goal'\n",
        "        done_batch = reward_batch == 0\n",
        "\n",
        "        # Reshape the batch\n",
        "        reward_batch = reward_batch.reshape(batch_size, *reward_batch.shape[1:])\n",
        "        done_batch = done_batch.reshape(batch_size, *done_batch.shape[1:])\n",
        "\n",
        "        if len(done_batch.shape) == 1:\n",
        "            done_batch = done_batch.reshape(batch_size, 1)\n",
        "\n",
        "        if len(reward_batch.shape) == 1:\n",
        "            reward_batch = reward_batch.reshape(batch_size, 1)\n",
        "\n",
        "        return sequential_batches, reward_batch, done_batch\n",
        "\n",
        "    def _sample_for_time(self, observation_buffer, achieved_goal_buffer, desired_goal_buffer, actions_buffer,\n",
        "                         info_buffer,\n",
        "                         episode_idxs, t_samples, her_indexes, future_t, batch_size, time):\n",
        "        observation_batch = observation_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
        "        achieved_goal_batch = achieved_goal_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
        "        desired_goal_batch = desired_goal_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
        "        actions_batch = actions_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
        "        info_batch = info_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
        "\n",
        "        # Reshape the batch\n",
        "        observation_batch = observation_batch.reshape(batch_size, *observation_batch.shape[1:])\n",
        "        achieved_goal_batch = achieved_goal_batch.reshape(batch_size, *achieved_goal_batch.shape[1:])\n",
        "        desired_goal_batch = desired_goal_batch.reshape(batch_size, *desired_goal_batch.shape[1:])\n",
        "        actions_batch = actions_batch.reshape(batch_size, *actions_batch.shape[1:])\n",
        "\n",
        "        # Get the achieved_goal at the 'future' timestamps\n",
        "        next_achieved_goal = achieved_goal_buffer[:, time:, :][episode_idxs[her_indexes], future_t]\n",
        "        next_info_batch = info_buffer[:, time:, :][episode_idxs[her_indexes], future_t]\n",
        "        # Replace the 'desired_goal' with the 'next_achieved_goal'\n",
        "        desired_goal_batch[her_indexes] = next_achieved_goal\n",
        "        info_batch[her_indexes] = next_info_batch\n",
        "\n",
        "        info_batch = info_batch.reshape(batch_size)\n",
        "        info_batch = {k: [dic[k] for dic in info_batch] for k in info_batch[0]}\n",
        "\n",
        "        for k, v in info_batch.items():\n",
        "            info_batch[k] = np.expand_dims(np.asarray(v), -1)\n",
        "\n",
        "        return observation_batch, achieved_goal_batch, desired_goal_batch, actions_batch, info_batch\n",
        "\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n",
        "        self.size = size\n",
        "        self.eps = eps\n",
        "        self.default_clip_range = default_clip_range\n",
        "\n",
        "        # some local information\n",
        "        self.local_sum = np.zeros(self.size, np.float32)\n",
        "        self.local_sumsq = np.zeros(self.size, np.float32)\n",
        "        self.local_count = np.zeros(1, np.float32)\n",
        "\n",
        "        # get the mean and std\n",
        "        self.mean = np.zeros(self.size, np.float32)\n",
        "        self.std = np.ones(self.size, np.float32)\n",
        "\n",
        "    # update the parameters of the normalizer\n",
        "    def update(self, v):\n",
        "        v = v.reshape(-1, self.size)\n",
        "        # do the computing\n",
        "        self.local_sum += v.sum(axis=0)\n",
        "        self.local_sumsq += (np.square(v)).sum(axis=0)\n",
        "        self.local_count[0] += v.shape[0]\n",
        "\n",
        "    def recompute_stats(self):\n",
        "        # calculate the new mean and std\n",
        "        self.mean = self.local_sum / self.local_count\n",
        "        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.local_sumsq / self.local_count) - np.square(\n",
        "            self.local_sum / self.local_count)))\n",
        "\n",
        "    # normalize the observation\n",
        "    def normalize(self, v):\n",
        "        return (v - self.mean) / np.clip(self.std, 1e-9, None)\n",
        "\n",
        "\n",
        "class BasePolicy(BaseModel, ABC):\n",
        "\n",
        "    def __init__(self,\n",
        "                 observation_space,\n",
        "                 action_space,\n",
        "                 normalize_images: bool = True,\n",
        "                 squash_output: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.normalize_images = normalize_images\n",
        "        self._squash_output = squash_output\n",
        "\n",
        "    @property\n",
        "    def squash_output(self) -> bool:\n",
        "        return self._squash_output\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(module: nn.Module, gain: float = 1) -> None:\n",
        "        \"\"\"\n",
        "        Orthogonal initialization (used in PPO and A2C)\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            nn.init.orthogonal_(module.weight, gain=gain)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.fill_(0.0)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _predict(self, observation: torch.Tensor, deterministic: bool = False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, observation):\n",
        "        return self._predict(observation)\n",
        "\n",
        "    def scale_action(self, action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Rescale the action from [low, high] to [-1, 1]\n",
        "        (no need for symmetric action space)\n",
        "\n",
        "        :param action: Action to scale\n",
        "        :return: Scaled action\n",
        "        \"\"\"\n",
        "        low, high = self.action_space.low, self.action_space.high\n",
        "        return 2.0 * ((action - low) / (high - low)) - 1.0\n",
        "\n",
        "    def unscale_action(self, scaled_action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Rescale the action from [-1, 1] to [low, high]\n",
        "        (no need for symmetric action space)\n",
        "\n",
        "        :param scaled_action: Action to un-scale\n",
        "        \"\"\"\n",
        "        low, high = self.action_space.low, self.action_space.high\n",
        "        return low + (0.5 * (scaled_action + 1.0) * (high - low))\n",
        "\n",
        "    def obs_to_tensor(self, observation):\n",
        "        if isinstance(observation, dict):\n",
        "            observation = copy.deepcopy(observation)\n",
        "            for key, obs in observation.items():\n",
        "                obs_space = self.observation_space.spaces[key]\n",
        "                if is_image_space(obs_space):\n",
        "                    obs_ = maybe_transpose(obs, obs_space)\n",
        "                else:\n",
        "                    obs_ = np.array(obs)\n",
        "                observation[key] = obs_.reshape((-1,) + self.observation_space[key].shape)\n",
        "\n",
        "        elif is_image_space(self.observation_space):\n",
        "            observation = maybe_transpose(observation, self.observation_space)\n",
        "\n",
        "        else:\n",
        "            observation = np.array(observation)\n",
        "\n",
        "        return observation\n",
        "\n",
        "\n",
        "class Actor(BasePolicy):\n",
        "    def __init__(\n",
        "            self,\n",
        "            observation_space: gym.spaces.Space,\n",
        "            action_space: gym.spaces.Space,\n",
        "            input_size,\n",
        "            net_arch,\n",
        "            action_distribution_type,\n",
        "            device,\n",
        "            weight_decay=0.00001,\n",
        "            lr=0.0001,\n",
        "            activation_fn=nn.ReLU,\n",
        "            log_std_init: float = -1,\n",
        "            full_std: bool = True,\n",
        "            sde_net_arch=None,\n",
        "            use_expln: bool = False,\n",
        "            clip_mean: float = 2.0,\n",
        "            normalize_images: bool = True,\n",
        "    ):\n",
        "        super().__init__(observation_space, action_space, normalize_images=normalize_images, squash_output=True)\n",
        "\n",
        "        self.sde_features_extractor = None\n",
        "        self.net_arch = net_arch\n",
        "        self.activation_fn = activation_fn\n",
        "        self.log_std_init = log_std_init\n",
        "        self.sde_net_arch = sde_net_arch\n",
        "        self.use_expln = use_expln\n",
        "        self.full_std = full_std\n",
        "        self.clip_mean = clip_mean\n",
        "        self.action_distribution_type = action_distribution_type\n",
        "\n",
        "        self.LOG_STD_MAX = 2\n",
        "        self.LOG_STD_MIN = -20\n",
        "\n",
        "        action_dim = get_action_dim(self.action_space)\n",
        "        # here will be vae\n",
        "        self.latent_pi = nn.Sequential(*create_mlp(input_size, -1, net_arch, activation_fn))\n",
        "        self.device = device\n",
        "        last_layer_dim = net_arch[-1]\n",
        "\n",
        "        if self.action_distribution_type == 'StateDependentNoiseDistribution':\n",
        "            self.action_dist = StateDependentNoiseDistribution(\n",
        "                action_dim, full_std=full_std, use_expln=use_expln, learn_features=True, squash_output=True\n",
        "            )\n",
        "            self.mu, self.log_std = self.action_dist.proba_distribution_net(\n",
        "                latent_dim=last_layer_dim, latent_sde_dim=last_layer_dim, log_std_init=log_std_init\n",
        "            )\n",
        "            if clip_mean > 0.0:\n",
        "                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n",
        "        elif self.action_distribution_type == 'SquashedDiagGaussianDistribution':\n",
        "            self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
        "            self.mu = nn.Linear(last_layer_dim, action_dim)\n",
        "            self.log_std = nn.Linear(last_layer_dim, action_dim)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.input_size = input_size\n",
        "        self.net_arch = net_arch\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.activation_fn = activation_fn\n",
        "        self.log_std_init = log_std_init\n",
        "        self.full_std = full_std\n",
        "        self.sde_net_arch = sde_net_arch\n",
        "        self.use_expln = use_expln\n",
        "        self.clip_mean = clip_mean\n",
        "        self.normalize_images = normalize_images\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def _get_constructor_parameters(self):\n",
        "        data = super()._get_constructor_parameters()\n",
        "\n",
        "        data.update(\n",
        "            dict(\n",
        "                action_distribution_type=self.action_distribution_type,\n",
        "                observation_space=self.observation_space,\n",
        "                action_space=self.action_space,\n",
        "                input_size=self.input_size,\n",
        "                net_arch=self.net_arch,\n",
        "                weight_decay=self.weight_decay,\n",
        "                lr=self.lr,\n",
        "                activation_fn=self.activation_fn,\n",
        "                log_std_init=self.log_std_init,\n",
        "                full_std=self.full_std,\n",
        "                sde_net_arch=self.sde_net_arch,\n",
        "                use_expln=self.use_expln,\n",
        "                clip_mean=self.clip_mean,\n",
        "                normalize_images=self.normalize_images,\n",
        "                device=self.device,\n",
        "            )\n",
        "        )\n",
        "        return data\n",
        "\n",
        "    def get_std(self) -> torch.Tensor:\n",
        "        return self.action_dist.get_std(self.log_std)\n",
        "\n",
        "    def reset_noise(self, batch_size: int = 1) -> None:\n",
        "        self.action_dist.sample_weights(self.log_std, batch_size=batch_size)\n",
        "\n",
        "    def get_action_dist_params(self, observations: torch.Tensor):\n",
        "        # features = self.extract_features(observations)\n",
        "        latent_pi = self.latent_pi(observations)\n",
        "        mean_actions = self.mu(latent_pi)\n",
        "\n",
        "        if self.action_distribution_type == 'StateDependentNoiseDistribution':\n",
        "            return mean_actions, self.log_std, dict(latent_sde=latent_pi)\n",
        "\n",
        "        log_std = self.log_std(latent_pi)\n",
        "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
        "        return mean_actions, log_std, {}\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
        "        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n",
        "        # Note: the action is squashed\n",
        "        return self.action_dist.actions_from_params(mean_actions, log_std, deterministic=deterministic, **kwargs)\n",
        "\n",
        "    def action_log_prob(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n",
        "        # return action and associated log prob\n",
        "        return self.action_dist.log_prob_from_params(mean_actions, log_std, **kwargs)\n",
        "\n",
        "    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
        "        return self(observation, deterministic)\n",
        "\n",
        "\n",
        "class MLP(BaseModel):\n",
        "    def __init__(self, input_size, layer_sizes, output_size, lr=0.0001, output_activation=torch.nn.Identity,\n",
        "                 activation=torch.nn.ReLU, weight_decay=1e-6, device='cpu'):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.layers = create_mlp(input_size, output_size, layer_sizes, activation)\n",
        "        self.layers = nn.ModuleList(self.layers)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr)  # Adam optimizer\n",
        "\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.output_size = output_size\n",
        "        self.lr = lr\n",
        "        self.output_activation = output_activation\n",
        "        self.activation = activation\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def _get_constructor_parameters(self):\n",
        "        data = super()._get_constructor_parameters()\n",
        "\n",
        "        data.update(\n",
        "            dict(\n",
        "                input_size=self.input_size,\n",
        "                layer_sizes=self.layer_sizes,\n",
        "                output_size=self.output_size,\n",
        "                lr=self.lr,\n",
        "                output_activation=self.output_activation,\n",
        "                activation=self.activation,\n",
        "                weight_decay=self.weight_decay,\n",
        "            )\n",
        "        )\n",
        "        return data\n",
        "\n",
        "\n",
        "class TransitionModelMlpPreprocessor:\n",
        "    def __init__(self, preprocess_func, device):\n",
        "        super(TransitionModelMlpPreprocessor, self).__init__()\n",
        "        self.preprocess_func = preprocess_func\n",
        "        self.device = device\n",
        "\n",
        "    def preprocess(self, sequence_of_batches):\n",
        "        (observation_batch, _, desired_goal_batch, actions_batch_t0, _) = sequence_of_batches[-1]\n",
        "        state_batch_t0 = self.preprocess_func(observation_batch, desired_goal_batch)\n",
        "        return torch.cat((state_batch_t0, as_tensor(actions_batch_t0, self.device)), dim=1)\n",
        "\n",
        "\n",
        "class TransitionModelRnnPreprocessor:\n",
        "    def __init__(self, preprocess_func, device):\n",
        "        super(TransitionModelRnnPreprocessor, self).__init__()\n",
        "        self.preprocess_func = preprocess_func\n",
        "        self.device = device\n",
        "\n",
        "    def preprocess(self, sequence_of_batches):\n",
        "        final_batch = []\n",
        "        for time in range(len(sequence_of_batches)):\n",
        "            (observation_batch, _, desired_goal_batch, actions_batch, _) = sequence_of_batches[time]\n",
        "            state_batch = self.preprocess_func(observation_batch, desired_goal_batch)\n",
        "            final_batch.append(torch.cat((state_batch, as_tensor(actions_batch, self.device)), dim=1))\n",
        "        return torch.stack(final_batch, dim=1)\n",
        "\n",
        "\n",
        "class LSTM(BaseModel):\n",
        "    def __init__(self, input_size, layer_sizes, output_size, lr=0.0001, output_activation=torch.nn.Identity,\n",
        "                 activation=torch.nn.ReLU, drop_prob=0.2, device='cpu'):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # Defining the number of layers and the nodes in each layer\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.layer_dim = len(layer_sizes)\n",
        "        self.hidden_dim = layer_sizes[0]\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, self.hidden_dim, self.layer_dim, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr)  # Adam optimizer\n",
        "\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.output_size = output_size\n",
        "        self.lr = lr\n",
        "        self.drop_prob = drop_prob\n",
        "        self.output_activation = output_activation\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "    def _get_constructor_parameters(self):\n",
        "        data = super()._get_constructor_parameters()\n",
        "\n",
        "        data.update(\n",
        "            dict(\n",
        "                input_size=self.input_size,\n",
        "                layer_sizes=self.layer_sizes,\n",
        "                output_size=self.output_size,\n",
        "                lr=self.lr,\n",
        "                output_activation=self.output_activation,\n",
        "                activation=self.activation,\n",
        "                drop_prob=self.drop_prob,\n",
        "                device=self.device,\n",
        "            )\n",
        "        )\n",
        "        return data\n",
        "\n",
        "\n",
        "class GRU(BaseModel):\n",
        "    def __init__(self, input_size, layer_sizes, output_size,\n",
        "                 lr=0.0001, output_activation=torch.nn.Identity,\n",
        "                 activation=torch.nn.ReLU, drop_prob=0.2, device='cpu'):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        # Defining the number of layers and the nodes in each layer\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.layer_dim = len(layer_sizes)\n",
        "        self.hidden_dim = layer_sizes[0]\n",
        "\n",
        "        # GRU layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, self.hidden_dim, self.layer_dim, batch_first=True, dropout=drop_prob\n",
        "        )\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr)\n",
        "\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.output_size = output_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "        self.output_activation = output_activation\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initializing hidden state for first input with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # Forward propagation by passing in the input and hidden state into the model\n",
        "        out, _ = self.gru(x, h0.detach())\n",
        "\n",
        "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
        "        # so that it can fit into the fully connected layer\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "    def _get_constructor_parameters(self):\n",
        "        data = super()._get_constructor_parameters()\n",
        "\n",
        "        data.update(\n",
        "            dict(\n",
        "                input_size=self.input_size,\n",
        "                layer_sizes=self.layer_sizes,\n",
        "                output_size=self.output_size,\n",
        "                lr=self.lr,\n",
        "                output_activation=self.output_activation,\n",
        "                activation=self.activation,\n",
        "                drop_prob=self.drop_prob,\n",
        "                device=self.device,\n",
        "            )\n",
        "        )\n",
        "        return data\n",
        "\n",
        "\n",
        "class EpisodeData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.observation = []\n",
        "        self.achieved_goal = []\n",
        "        self.desired_goal = []\n",
        "        self.action = []\n",
        "        self.info = []\n",
        "\n",
        "    def add(self, observation, achieved_goal, desired_goal, action, info):\n",
        "        self.observation.append(observation)\n",
        "        self.achieved_goal.append(achieved_goal)\n",
        "        self.desired_goal.append(desired_goal)\n",
        "        self.action.append(action)\n",
        "        self.info.append(info)\n",
        "\n",
        "    def as_numpy_arrays(self):\n",
        "        return np.asarray(self.observation), np.asarray(self.achieved_goal), \\\n",
        "               np.asarray(self.desired_goal), np.asarray(self.action), np.asarray(self.info)\n",
        "\n",
        "    @classmethod\n",
        "    def as_dict_of_numpy_arrays(cls, collected_step_episodes):\n",
        "        data = EpisodeData()\n",
        "\n",
        "        for elem in collected_step_episodes:\n",
        "            data.add(*elem.as_numpy_arrays())\n",
        "\n",
        "        data_as_dict = data.__dict__\n",
        "        for key, value in data.__dict__.items():\n",
        "            data_as_dict[key] = np.asarray(value)\n",
        "        return data_as_dict\n",
        "\n",
        "\n",
        "class EpisodeSummary:\n",
        "    def __init__(self):\n",
        "        self.done = []\n",
        "        self.reward = []\n",
        "        self.goal_distance = []\n",
        "        self.jaw_state = []\n",
        "\n",
        "    def add(self, reward, info):\n",
        "        if 'goal_distance' in info:\n",
        "            self.goal_distance.append(info['goal_distance'])\n",
        "        if 'jaw_state' in info:\n",
        "            self.jaw_state.append(info['jaw_state'])\n",
        "        self.done.append(info['is_success'])\n",
        "        self.reward.append(reward)\n",
        "\n",
        "    def add_with_no_transformation(self, reward, done, goal_distance, jaw_state):\n",
        "        self.reward.append(reward)\n",
        "        self.done.append(done)\n",
        "        self.goal_distance.append(goal_distance)\n",
        "        self.jaw_state.append(jaw_state)\n",
        "\n",
        "    def calc_summary(self):\n",
        "        return np.mean(self.reward), np.mean(self.done), \\\n",
        "               np.mean(self.goal_distance) if len(self.goal_distance) > 0 else -1, \\\n",
        "               np.mean(self.jaw_state) if len(self.jaw_state) > 0 else -1\n",
        "\n",
        "    @classmethod\n",
        "    def as_dict_of_values(cls, collected_step_summary):\n",
        "        data = EpisodeSummary()\n",
        "\n",
        "        for elem in collected_step_summary:\n",
        "            data.add_with_no_transformation(*elem.calc_summary())\n",
        "\n",
        "        data_as_dict = data.__dict__\n",
        "        for key, value in data.__dict__.items():\n",
        "            data_as_dict[key] = np.mean(value)\n",
        "        return data_as_dict\n",
        "\n",
        "\n",
        "class SacMaximise:\n",
        "\n",
        "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
        "        self.actor = actor\n",
        "        self.value_net = value_net\n",
        "        self.target_net = target_net\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
        "                               actions_batch_t1,\n",
        "                               reward_batch, done_batch,\n",
        "                               pred_error_batch_t0t1, alpha):\n",
        "        with torch.no_grad():\n",
        "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
        "\n",
        "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
        "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
        "\n",
        "            # H_t2 ~ -log_prob_t2\n",
        "            weighted_targets = target_expected_free_energies_batch_t2 - alpha * log_prob_t2.reshape(-1, 1)\n",
        "\n",
        "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
        "            expected_free_energy_estimate_batch = (\n",
        "                    reward_batch - pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
        "\n",
        "        # Determine the Expected free energy at time t1 according to the value network:\n",
        "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
        "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
        "\n",
        "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
        "        mse = F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
        "        return mse\n",
        "\n",
        "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
        "                                        pred_error_batch_t0t1, alpha):\n",
        "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
        "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
        "\n",
        "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 - self.gamma * expected_free_energy_t1\n",
        "        return torch.mean(vfe_batch)\n",
        "\n",
        "\n",
        "class SacMinimise:\n",
        "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
        "        self.actor = actor\n",
        "        self.value_net = value_net\n",
        "        self.target_net = target_net\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
        "                               actions_batch_t1,\n",
        "                               reward_batch, done_batch,\n",
        "                               pred_error_batch_t0t1, alpha):\n",
        "        with torch.no_grad():\n",
        "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
        "\n",
        "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
        "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
        "\n",
        "            # H_t2 ~ -log_prob_t2\n",
        "            weighted_targets = target_expected_free_energies_batch_t2 + alpha * log_prob_t2.reshape(-1, 1)\n",
        "\n",
        "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
        "            expected_free_energy_estimate_batch = (\n",
        "                    -reward_batch + pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
        "\n",
        "        # Determine the Expected free energy at time t1 according to the value network:\n",
        "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
        "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
        "\n",
        "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
        "        mse = F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
        "        return mse\n",
        "\n",
        "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
        "                                        pred_error_batch_t0t1, alpha):\n",
        "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
        "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
        "\n",
        "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 + self.gamma * expected_free_energy_t1\n",
        "        return torch.mean(vfe_batch)\n",
        "\n",
        "class AdaptedDaif:\n",
        "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
        "        self.actor = actor\n",
        "        self.value_net = value_net\n",
        "        self.target_net = target_net\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
        "                               actions_batch_t1,\n",
        "                               reward_batch, done_batch,\n",
        "                               pred_error_batch_t0t1, alpha):\n",
        "        with torch.no_grad():\n",
        "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
        "\n",
        "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
        "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
        "\n",
        "            weighted_targets = target_expected_free_energies_batch_t2\n",
        "\n",
        "            expected_free_energy_estimate_batch = (\n",
        "                    -reward_batch + pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
        "\n",
        "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
        "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
        "\n",
        "        mse = F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
        "        return mse\n",
        "\n",
        "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
        "                                        pred_error_batch_t0t1, alpha):\n",
        "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
        "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
        "\n",
        "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 + self.gamma * expected_free_energy_t1\n",
        "        return torch.mean(vfe_batch)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, config):\n",
        "        self.env = env\n",
        "        self.experiment_name = config.experiment_name\n",
        "        self.experiment_description = config.experiment_description\n",
        "        self.observation_dim, self.goal_dim, self.action_dim, self.action_max = get_env_parameters(env)\n",
        "\n",
        "        self.device = config.device_id\n",
        "\n",
        "        self.polyak = int(config.hparams.polyak)\n",
        "\n",
        "        self.actor_action_distribution = config.hparams.actor_action_distribution\n",
        "        self.n_warmap_episodes = int(config.hparams.n_warmap_episodes)\n",
        "\n",
        "        self.n_epochs = int(config.hparams.n_epochs)\n",
        "        self.steps_per_epoch = int(config.hparams.steps_per_epoch)\n",
        "        self.n_rollout_episodes = int(config.hparams.n_rollout_episodes)\n",
        "        self.n_training_iterations = int(config.hparams.n_training_iterations)\n",
        "        self._max_episode_steps = env._max_episode_steps\n",
        "\n",
        "        self.batch_size = int(config.hparams.batch_size)\n",
        "        self.memory_capacity = int(config.hparams.memory_capacity)\n",
        "\n",
        "        self.gamma = float(config.hparams.gamma)  # A precision parameter\n",
        "        self.beta = float(config.hparams.beta)  # The discount rate\n",
        "        self.alpha = config.hparams.alpha  # The discount rate\n",
        "\n",
        "        self.should_save_model = interpret_boolean(config.should_save_model)\n",
        "        self.model_path = prepare_path(config.model_path, experiment_name=config.experiment_name)\n",
        "        self.video_log_path = os.path.join(\n",
        "            prepare_path(config.video_log_folder, experiment_name=config.experiment_name), \"epoch-{}.gif\")\n",
        "        self.model_save_timer = int(config.model_save_timer)\n",
        "\n",
        "        self.should_save_episode_video = interpret_boolean(config.should_save_episode_video)\n",
        "        self.episode_video_timer = int(config.episode_video_timer)\n",
        "\n",
        "        self.state_shape = np.add(self.env.observation_space['observation'].shape,\n",
        "                                  self.env.observation_space['desired_goal'].shape)\n",
        "        self.state_size = np.prod(self.state_shape)\n",
        "\n",
        "        self.actions_shape = self.env.action_space.shape\n",
        "        self.action_dim = self.env.action_space.shape[-1]\n",
        "        self.rnn_seq_len = config.hparams.rnn_seq_len  # The discount rate\n",
        "\n",
        "        assert (self.n_rollout_episodes >= self.rnn_seq_len)\n",
        "        assert (self.n_warmap_episodes >= self.rnn_seq_len)\n",
        "\n",
        "        self.current_epoch = 0\n",
        "        self.actor = Actor(env.observation_space, env.action_space,\n",
        "                           self.state_size,\n",
        "                           OmegaConf.to_object(config.hparams.actor_layers),\n",
        "                           action_distribution_type=self.actor_action_distribution,\n",
        "                           lr=config.hparams.actor_lr,\n",
        "                           device=self.device)\n",
        "\n",
        "        self.value_net = MLP(self.state_size + self.action_dim,\n",
        "                             OmegaConf.to_object(config.hparams.value_net_layers),\n",
        "                             1,\n",
        "                             lr=config.hparams.value_net_lr,\n",
        "                             device=self.device)\n",
        "        self.target_net = MLP(self.state_size + self.action_dim,\n",
        "                              OmegaConf.to_object(config.hparams.value_net_layers),\n",
        "                              1,\n",
        "                              lr=config.hparams.value_net_lr,\n",
        "                              device=self.device)\n",
        "\n",
        "        self.transition_network_type = config.hparams.transition_network_type\n",
        "\n",
        "        if self.transition_network_type == 'mlp':\n",
        "            self.transition_net = MLP(self.state_size + self.action_dim,\n",
        "                                      OmegaConf.to_object(config.hparams.transition_net_layers),\n",
        "                                      self.state_size,\n",
        "                                      lr=config.hparams.value_net_lr,\n",
        "                                      device=self.device)\n",
        "            self.transition_preprocessor = TransitionModelMlpPreprocessor(self._preprocess_batch_inputs, self.device)\n",
        "        elif self.transition_network_type == 'lstm':\n",
        "            self.transition_net = LSTM(self.state_size + self.action_dim,\n",
        "                                       OmegaConf.to_object(config.hparams.transition_net_layers),\n",
        "                                       self.state_size,\n",
        "                                       lr=config.hparams.value_net_lr,\n",
        "                                       device=self.device)\n",
        "            self.transition_preprocessor = TransitionModelRnnPreprocessor(self._preprocess_batch_inputs, self.device)\n",
        "\n",
        "        elif self.transition_network_type == 'gru':\n",
        "            self.transition_net = GRU(self.state_size + self.action_dim,\n",
        "                                      OmegaConf.to_object(config.hparams.transition_net_layers),\n",
        "                                      self.state_size,\n",
        "                                      lr=config.hparams.value_net_lr,\n",
        "                                      device=self.device)\n",
        "            self.transition_preprocessor = TransitionModelRnnPreprocessor(self._preprocess_batch_inputs, self.device)\n",
        "\n",
        "        # entropy coeff settings\n",
        "        self.log_alpha = None\n",
        "        self.alpha_optimizer = None\n",
        "        self.alpha_tensor = None\n",
        "        self.target_entropy = -np.prod(self.env.action_space.shape).astype(np.float32)\n",
        "        if isinstance(self.alpha, str) and self.alpha.startswith(\"auto\"):\n",
        "            init_value = 1.0\n",
        "            self.log_alpha = torch.log(torch.ones(1, device=self.device) * init_value).requires_grad_(True)\n",
        "            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.hparams.alpha_lr)\n",
        "        else:\n",
        "            self.alpha_tensor = torch.tensor(float(self.alpha)).to(self.device)\n",
        "\n",
        "        if config.hparams.efe_approximation_approach == 'sac_maximize':\n",
        "            self.efe_approximation_approach = SacMaximise(self.actor, self.value_net, self.target_net,\n",
        "                                                          self.beta, self.gamma)\n",
        "        elif config.hparams.efe_approximation_approach == 'sac_minimise':\n",
        "            self.efe_approximation_approach = SacMinimise(self.actor, self.value_net, self.target_net,\n",
        "                                                          self.beta, self.gamma)\n",
        "        elif config.hparams.efe_approximation_approach == 'adapted_daif':\n",
        "            self.efe_approximation_approach = AdaptedDaif(self.actor, self.value_net, self.target_net,\n",
        "                                                          self.beta, self.gamma)\n",
        "\n",
        "        self.her_module = HERSampler(config.hparams.replay_strategy, config.hparams.replay_k, self.rnn_seq_len,\n",
        "                                     self.env.compute_reward)\n",
        "        # create the replay buffer\n",
        "        self.buffer = ReplayBuffer(self.env, self._max_episode_steps, self.memory_capacity,\n",
        "                                   self.her_module.sample_her_transitions, config.device_id)\n",
        "\n",
        "        self.o_norm = Normalizer(size=env.observation_space.spaces['observation'].shape[0])\n",
        "        self.g_norm = Normalizer(size=env.observation_space.spaces['desired_goal'].shape[0])\n",
        "        self.a_norm = Normalizer(size=self.action_dim)\n",
        "        self.target_update_interval = 1\n",
        "\n",
        "        self.writer = TensorboardWriter(prepare_path(config.tb_log_folder, experiment_name=config.experiment_name),\n",
        "                                        True)\n",
        "\n",
        "        self.train_metrics = MetricTracker('vfe', 'value_net_loss', 'alpha', 'alpha_loss', 'success_rate', 'reward',\n",
        "                                           'transition_net_grad', 'actor_grad_acc', 'value_net_grad',\n",
        "                                           'sde_std', 'transition_net_loss', 'goal_distance', 'jaw_state',\n",
        "                                           writer=self.writer)\n",
        "\n",
        "        self.val_metrics = MetricTracker('val/success_rate', 'val/reward', writer=self.writer)\n",
        "\n",
        "        with open(os.path.join(self.model_path, \"config.yaml\"), 'w+') as file:\n",
        "            OmegaConf.save(config, file)\n",
        "\n",
        "    def restore(self):\n",
        "        self.transition_net = self.transition_net.load(os.path.join(self.model_path, 'transition_net.pth'), self.device)\n",
        "        self.actor = self.actor.load(os.path.join(self.model_path, 'actor.pth'), self.device)\n",
        "        self.value_net = self.value_net.load(os.path.join(self.model_path, 'value_net.pth'), self.device)\n",
        "        self.target_net.load_state_dict(self.value_net.state_dict(), self.device)\n",
        "        self.current_epoch = int(self.model_path.split('epoch-')[1]) + 1\n",
        "\n",
        "        if self.log_alpha is not None:\n",
        "            saved_log_alpha = torch.load(os.path.join(self.model_path, 'log_alpha.pt'))\n",
        "            self.log_alpha = saved_log_alpha\n",
        "\n",
        "        if self.alpha_optimizer is not None:\n",
        "            saved_alpha_optimizer = torch.load(os.path.join(self.model_path, 'alpha_optimizer.pt'))\n",
        "            self.alpha_optimizer.load_state_dict(state_dict=saved_alpha_optimizer[\"state_dict\"])\n",
        "\n",
        "        if self.alpha_tensor is not None:\n",
        "            saved_alpha_tensor = torch.load(os.path.join(self.model_path, 'alpha_tensor.pt'))\n",
        "            self.alpha_tensor = saved_alpha_tensor\n",
        "\n",
        "    def get_mini_batches(self):\n",
        "        (sequential_batches, reward_batch, done_batch) = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        transition_model_raw_input, t1, t2 = sequential_batches[:-2], sequential_batches[-2], sequential_batches[-1]\n",
        "\n",
        "        (observation_batch_t1, achieved_goal_batch_t1, desired_goal_batch_t1, actions_batch_t1, _) = t1\n",
        "        (observation_batch_t2, achieved_goal_batch_t2, desired_goal_batch_t2, actions_batch_t2, _) = t2\n",
        "\n",
        "        state_batch_t1 = self._preprocess_batch_inputs(observation_batch_t1, desired_goal_batch_t1)\n",
        "        state_batch_t2 = self._preprocess_batch_inputs(observation_batch_t2, desired_goal_batch_t2)\n",
        "\n",
        "        transition_net_input = self.transition_preprocessor.preprocess(transition_model_raw_input)\n",
        "        pred_batch_t0t1 = self.transition_net(transition_net_input)\n",
        "\n",
        "        pred_error_batch_t0t1 = torch.mean(\n",
        "            F.mse_loss(pred_batch_t0t1, state_batch_t1, reduction='none'), dim=1).unsqueeze(1)\n",
        "\n",
        "        return (state_batch_t1, state_batch_t2,\n",
        "                as_tensor(actions_batch_t1, self.device),\n",
        "                as_tensor(reward_batch, self.device),\n",
        "                as_tensor(done_batch, self.device),\n",
        "                pred_error_batch_t0t1)\n",
        "\n",
        "    def _update_network(self):\n",
        "        if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
        "            self.actor.reset_noise()\n",
        "\n",
        "        # Retrieve transition data in mini batches:\n",
        "        (state_batch_t1, state_batch_t2, actions_batch_t1,\n",
        "         reward_batch, done_batch, pred_error_batch_t0t1) = self.get_mini_batches()\n",
        "        # Compute the value network loss:\n",
        "\n",
        "        # Action by the current actor for the sampled state\n",
        "        sampled_actions_t1, sampled_actions_log_prob_t1 = self.actor.action_log_prob(state_batch_t1)\n",
        "        sampled_actions_log_prob_t1 = sampled_actions_log_prob_t1.reshape(-1, 1)\n",
        "\n",
        "        alpha_loss = None\n",
        "        if self.alpha_optimizer is not None:\n",
        "            # Important: detach the variable from the graph\n",
        "            # so we don't change it with other losses\n",
        "            # see https://github.com/rail-berkeley/softlearning/issues/60\n",
        "            alpha = torch.exp(self.log_alpha.detach())\n",
        "            alpha_loss = -(self.log_alpha * (sampled_actions_log_prob_t1 + self.target_entropy).detach()).mean()\n",
        "        else:\n",
        "            alpha = self.alpha_tensor\n",
        "\n",
        "        # Optimize entropy coefficient, also called\n",
        "        # entropy temperature or alpha in the paper\n",
        "        if alpha_loss is not None:\n",
        "            self.alpha_optimizer.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optimizer.step()\n",
        "\n",
        "        value_net_loss = self.efe_approximation_approach.compute_value_net_loss(state_batch_t1, state_batch_t2,\n",
        "                                                                                actions_batch_t1,\n",
        "                                                                                reward_batch, done_batch,\n",
        "                                                                                pred_error_batch_t0t1,\n",
        "                                                                                alpha)\n",
        "        self.transition_net.optimizer.zero_grad()\n",
        "\n",
        "        self.value_net.optimizer.zero_grad()\n",
        "        value_net_loss.backward()\n",
        "        value_net_grad = torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 100000.)\n",
        "        self.value_net.optimizer.step()\n",
        "\n",
        "        # Optimize the actor\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        # Compute the variational free energy:\n",
        "        vfe = self.efe_approximation_approach.compute_variational_free_energy(state_batch_t1,\n",
        "                                                                              sampled_actions_t1,\n",
        "                                                                              sampled_actions_log_prob_t1,\n",
        "                                                                              pred_error_batch_t0t1, alpha)\n",
        "        vfe.backward()\n",
        "        actor_grad = torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100000.)\n",
        "        transition_net_grad = torch.nn.utils.clip_grad_norm_(self.transition_net.parameters(), 100000.)\n",
        "        self.actor.optimizer.step()\n",
        "        self.transition_net.optimizer.step()\n",
        "\n",
        "        metrics = dict(\n",
        "            vfe=vfe.item(),\n",
        "            actor_grad_acc=actor_grad.item(),\n",
        "            alpha=alpha.item(),\n",
        "            alpha_loss=alpha_loss.detach().item(),\n",
        "            value_net_loss=value_net_loss.item(),\n",
        "            value_net_grad=value_net_grad.item(),\n",
        "            transition_net_loss=pred_error_batch_t0t1.mean().item(),\n",
        "            transition_net_grad=transition_net_grad.item(),\n",
        "        )\n",
        "        return metrics\n",
        "\n",
        "    # soft update\n",
        "    def _soft_update_target_network(self, target, source):\n",
        "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "            target_param.data.copy_((1 - self.polyak) * param.data + self.polyak * target_param.data)\n",
        "\n",
        "    # do the evaluation\n",
        "    def _eval_agent(self, epoch):\n",
        "        images = []\n",
        "        reward_array = []\n",
        "        done = []\n",
        "        episode_step = 0\n",
        "\n",
        "        observation, _, desired_goal, _, _ = self._reset()\n",
        "        while episode_step < self._max_episode_steps:\n",
        "            input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
        "            action = self._select_action(input_tensor)\n",
        "\n",
        "            new_observation, reward, _, info = self.env.step(action)\n",
        "\n",
        "            observation = new_observation['observation']\n",
        "            reward_array.append(reward)\n",
        "            done.append(info['is_success'])\n",
        "            episode_step += 1\n",
        "\n",
        "            if self.should_save_episode_video and epoch % self.episode_video_timer == 0:\n",
        "                images += [self.env.render(mode='rgb_array')]\n",
        "\n",
        "        return np.mean(np.asarray(done)), np.mean(np.asarray(reward_array)), np.asarray(images)\n",
        "\n",
        "    def train(self):\n",
        "        self.writer.add_text(self.experiment_name, self.experiment_description)\n",
        "        print(\"Environment is: {}\\nTraining started at {}\".format(self.env.unwrapped.spec.id, datetime.now()))\n",
        "\n",
        "        self.warmup()\n",
        "        for epoch in range(self.current_epoch, self.n_epochs + 1):\n",
        "            for cycle in range(self.steps_per_epoch):\n",
        "                step = self.steps_per_epoch * epoch + cycle\n",
        "                self.writer.set_step(step)\n",
        "\n",
        "                collected_step_summary = []\n",
        "                collected_step_episodes = []\n",
        "                for _ in range(self.n_rollout_episodes):\n",
        "\n",
        "                    episode_data, episode_summary = EpisodeData(), EpisodeSummary()\n",
        "                    observation, achieved_goal, desired_goal, done, reward = self._reset()\n",
        "\n",
        "                    for episode_step in range(self._max_episode_steps):\n",
        "                        input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
        "                        action = self._select_action(input_tensor)\n",
        "\n",
        "                        # feed the actions into the environment\n",
        "                        new_observation, reward, _, info = self.env.step(action)\n",
        "\n",
        "                        episode_data.add(observation.copy(), achieved_goal.copy(), desired_goal.copy(), action.copy(),\n",
        "                                         info)\n",
        "                        episode_summary.add(np.mean(reward), info)\n",
        "\n",
        "                        observation = new_observation['observation']\n",
        "                        achieved_goal = new_observation['achieved_goal']\n",
        "\n",
        "                    collected_step_episodes.append(episode_data)\n",
        "                    collected_step_summary.append(episode_summary)\n",
        "\n",
        "                collected_step_episodes = EpisodeData.as_dict_of_numpy_arrays(collected_step_episodes)\n",
        "                collected_step_summary = EpisodeSummary.as_dict_of_values(collected_step_summary)\n",
        "\n",
        "                # store the episodes\n",
        "                self.buffer.store_episode(**collected_step_episodes, n_episodes_to_store=self.n_rollout_episodes)\n",
        "                self._update_normalizer(**collected_step_episodes)\n",
        "\n",
        "                train_iteration_metrics = []\n",
        "                for _ in range(self.n_training_iterations):\n",
        "                    # train the network\n",
        "                    metrics_dict = self._update_network()\n",
        "                    train_iteration_metrics.append(metrics_dict)\n",
        "\n",
        "                train_iteration_metrics = {k: [dic[k] for dic in train_iteration_metrics]\n",
        "                                           for k in train_iteration_metrics[0]}\n",
        "\n",
        "                for metric, value in train_iteration_metrics.items():\n",
        "                    self.train_metrics.update(metric, np.mean(value))\n",
        "\n",
        "                if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
        "                    self.train_metrics.update('sde_std', (self.actor.get_std()).mean().item())\n",
        "\n",
        "                # soft update\n",
        "                if cycle % self.target_update_interval == 0:\n",
        "                    polyak_update(self.value_net.parameters(), self.target_net.parameters(), 0.005)\n",
        "\n",
        "                success_rate = collected_step_summary['done']\n",
        "                reward = collected_step_summary['reward']\n",
        "\n",
        "                self.train_metrics.update('success_rate', success_rate)\n",
        "                self.train_metrics.update('reward', reward)\n",
        "                self.train_metrics.update('goal_distance', collected_step_summary['goal_distance'])\n",
        "                self.train_metrics.update('jaw_state', collected_step_summary['jaw_state'])\n",
        "                self.log_models_parameters()\n",
        "\n",
        "                print(\"Epoch: {:4d}, Step: {:4d}, reward: {:3.2f}, success_rate: {:3.2f}\".format(epoch, cycle,\n",
        "                                                                                                 reward,\n",
        "                                                                                                 success_rate))\n",
        "\n",
        "            success_rate, reward, images = self._eval_agent(epoch)\n",
        "\n",
        "            self.val_metrics.update('val/success_rate', success_rate)\n",
        "            self.val_metrics.update('val/reward', reward)\n",
        "\n",
        "            if self.should_save_episode_video and epoch % self.episode_video_timer == 0:\n",
        "                imageio.mimsave(self.video_log_path.format(epoch), images)\n",
        "\n",
        "            if self.should_save_model and epoch > 0 and epoch % self.model_save_timer == 0:\n",
        "                epoch_path = self.model_path + \"/epoch_\" + str(epoch)\n",
        "                create_dirs([epoch_path])\n",
        "                self.transition_net.save(os.path.join(epoch_path, 'transition_net.pth'))\n",
        "                self.actor.save(os.path.join(epoch_path, 'actor.pth'))\n",
        "                self.value_net.save(os.path.join(epoch_path, 'value_net.pth'))\n",
        "\n",
        "                if self.log_alpha is not None:\n",
        "                    torch.save(self.log_alpha, os.path.join(epoch_path, 'log_alpha.pt'))\n",
        "\n",
        "                if self.alpha_optimizer is not None:\n",
        "                    torch.save({\"state_dict\": self.alpha_optimizer.state_dict()},\n",
        "                               os.path.join(epoch_path, 'alpha_optimizer.pt'))\n",
        "\n",
        "                if self.alpha_tensor is not None:\n",
        "                    torch.save(self.alpha_tensor, os.path.join(epoch_path, 'alpha_tensor.pt'))\n",
        "\n",
        "                torch.save(torch.tensor(self.o_norm.std, dtype=torch.float32, device=self.device),\n",
        "                           os.path.join(epoch_path, 'o_norm_std_tensor.pt'))\n",
        "\n",
        "                torch.save(torch.tensor(self.o_norm.mean, dtype=torch.float32, device=self.device),\n",
        "                           os.path.join(epoch_path, 'o_norm_mean_tensor.pt'))\n",
        "\n",
        "                torch.save(torch.tensor(self.g_norm.std, dtype=torch.float32, device=self.device),\n",
        "                           os.path.join(epoch_path, 'g_norm_std_tensor.pt'))\n",
        "\n",
        "                torch.save(torch.tensor(self.g_norm.mean, dtype=torch.float32, device=self.device),\n",
        "                           os.path.join(epoch_path, 'g_norm_mean_tensor.pt'))\n",
        "\n",
        "\n",
        "        self.env.close()\n",
        "        print(\"Training finished at {}\".format(datetime.now()))\n",
        "        return success_rate, reward\n",
        "\n",
        "    def log_models_parameters(self):\n",
        "        # add histogram of model parameters to the tensorboard\n",
        "        for name, p in self.transition_net.named_parameters():\n",
        "            self.writer.add_histogram('transition_net_net_' + name, p, bins='auto')\n",
        "        for name, p in self.actor.named_parameters():\n",
        "            self.writer.add_histogram('actor_' + name, p, bins='auto')\n",
        "        for name, p in self.value_net.named_parameters():\n",
        "            self.writer.add_histogram('value_net_' + name, p, bins='auto')\n",
        "\n",
        "    def _reset(self):\n",
        "        self.train_metrics.reset()\n",
        "        native_observation = self.env.reset()\n",
        "\n",
        "        observation = native_observation['observation']\n",
        "        achieved_goal = native_observation['achieved_goal']\n",
        "        desired_goal = native_observation['desired_goal']\n",
        "\n",
        "        if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
        "            self.actor.reset_noise()\n",
        "\n",
        "        return observation, achieved_goal, desired_goal, False, 0\n",
        "\n",
        "    def _select_action(self, input_tensor):\n",
        "        with torch.no_grad():\n",
        "            action = self.actor.predict(input_tensor)\n",
        "            return action.cpu().numpy().flatten()\n",
        "\n",
        "    def _preprocess_inputs(self, observation, goal):\n",
        "        # concatenate the stuffs\n",
        "        inputs = np.concatenate([observation, goal])\n",
        "        return torch.tensor(inputs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "    def _preprocess_batch_inputs(self, observation_batch, goal_batch):\n",
        "        # concatenate the stuffs\n",
        "        inputs = np.concatenate([observation_batch, goal_batch], axis=1)\n",
        "        return torch.tensor(inputs, dtype=torch.float32, device=self.device)\n",
        "\n",
        "    def _update_normalizer(self, observation, achieved_goal, desired_goal, action, info):\n",
        "        # get the number of normalization transitions\n",
        "        num_transitions = action.shape[0]\n",
        "        # create the new buffer to store them\n",
        "        sequential_batches, reward_batch, done_batch = self.her_module.sample_her_transitions(observation,\n",
        "                                                                                              achieved_goal,\n",
        "                                                                                              desired_goal,\n",
        "                                                                                              action,\n",
        "                                                                                              np.expand_dims(info, -1),\n",
        "                                                                                              num_transitions)\n",
        "\n",
        "        (observation_batch, _, desired_goal_batch, _, _) = sequential_batches[0]\n",
        "\n",
        "        # update\n",
        "        self.o_norm.update(observation_batch)\n",
        "        self.g_norm.update(desired_goal_batch)\n",
        "        # recompute the stats\n",
        "        self.o_norm.recompute_stats()\n",
        "        self.g_norm.recompute_stats()\n",
        "\n",
        "    def warmup(self):\n",
        "        collected_step_episodes = []\n",
        "        for _ in range(self.n_warmap_episodes):\n",
        "\n",
        "            episode_data = EpisodeData()\n",
        "            observation, achieved_goal, desired_goal, done, reward = self._reset()\n",
        "\n",
        "            for episode_step in range(self._max_episode_steps):\n",
        "                input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
        "                action = self._select_action(input_tensor)\n",
        "\n",
        "                # feed the actions into the environment\n",
        "                new_observation, reward, _, info = self.env.step(action)\n",
        "\n",
        "                episode_data.add(observation.copy(), achieved_goal.copy(), desired_goal.copy(), action.copy(),\n",
        "                                 info)\n",
        "\n",
        "                observation = new_observation['observation']\n",
        "                achieved_goal = new_observation['achieved_goal']\n",
        "\n",
        "            collected_step_episodes.append(episode_data)\n",
        "\n",
        "        collected_step_episodes = EpisodeData.as_dict_of_numpy_arrays(collected_step_episodes)\n",
        "\n",
        "        # store the episodes\n",
        "        self.buffer.store_episode(**collected_step_episodes, n_episodes_to_store=self.n_warmap_episodes)\n",
        "        self._update_normalizer(**collected_step_episodes)\n",
        "\n",
        "\n",
        "def make_env(config):\n",
        "    if config.render_mode == 'none':\n",
        "        env = gym.make(config.env_id)\n",
        "    else:\n",
        "        env = gym.make(config.env_id, render_mode=config.render_mode)\n",
        "    # env = Monitor(env, prepare_path(config.monitor_file, experiment_name=config.experiment_name))\n",
        "    env.seed(config.seed)\n",
        "    return env\n",
        "\n",
        "\n",
        "def as_tensor(numpy_array, device):\n",
        "    return torch.tensor(numpy_array, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "def interpret_boolean(param):\n",
        "    if type(param) == bool:\n",
        "        return param\n",
        "    elif param in ['True', 'true', '1']:\n",
        "        return True\n",
        "    elif param in ['False', 'false', '0']:\n",
        "        return False\n",
        "    else:\n",
        "        sys.exit(\"param '{}' cannot be interpreted as boolean\".format(param))\n",
        "\n",
        "\n",
        "def get_env_parameters(env):\n",
        "    # Get spaces parameters\n",
        "    observation_dim = env.observation_space.spaces['observation'].shape[0]\n",
        "    goal_dim = env.observation_space.spaces['desired_goal'].shape[0]\n",
        "\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action_value = float(env.action_space.high[0])\n",
        "\n",
        "    return observation_dim, goal_dim, action_dim, max_action_value\n",
        "\n",
        "\n",
        "def create_dirs(dirs):\n",
        "    try:\n",
        "        for dir_ in dirs:\n",
        "            if not os.path.exists(dir_):\n",
        "                os.makedirs(dir_)\n",
        "        return 0\n",
        "    except Exception as err:\n",
        "        print(\"Creating directories error: {0}\".format(err))\n",
        "        exit(-1)\n",
        "\n",
        "\n",
        "def prepare_path(path, **args):\n",
        "    res = os.path.join(ROOT_DIR_PATH, path.format(**args))\n",
        "    create_dirs([res])\n",
        "    return res\n",
        "\n",
        "\n",
        "def set_random_seed(seed: int, device: str = 'cpu') -> None:\n",
        "    # Seed python RNG\n",
        "    random.seed(seed)\n",
        "    # Seed numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        # Deterministic operations for CuDNN, it may impact performances\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def train_agent_according_config(config):\n",
        "    env = make_env(config)\n",
        "    set_random_seed(config.seed, config.device_id)\n",
        "    print(f'Actions count: {env.action_space.shape}')\n",
        "    print(f'Action UB:   {float(env.action_space.high[0])}')\n",
        "    print(f'Action LB: {float(env.action_space.low[0])}')\n",
        "\n",
        "    agent = Agent(env, config)\n",
        "    return agent.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1KqPJCvi8YC"
      },
      "source": [
        "#### main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKA8hDqWd78v"
      },
      "outputs": [],
      "source": [
        "# env_ids = ['FetchPickAndPlace-v1', 'FetchReach-v1', 'FetchSlide-v1', 'FetchPush-v1']\n",
        "env_ids = ['FetchPickAndPlace-v1']\n",
        "efe_approximation_approaches = ['adapted_daif']\n",
        "transition_network_types = ['mlp', 'gru', 'lstm']\n",
        "actor_action_distributions = ['SquashedDiagGaussianDistribution']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL93lbPhgqed"
      },
      "outputs": [],
      "source": [
        "for distribution in actor_action_distributions:\n",
        "  for transition_network_type in transition_network_types:\n",
        "    for efe_approximation_approach in efe_approximation_approaches:\n",
        "      for env_id in env_ids:\n",
        "        experiment_name = env_id + '_' + efe_approximation_approach + '_' + transition_network_type + '_' + distribution + str(np.random.randint(1000))\n",
        "        \n",
        "        print(f'Current experiment: {experiment_name}')\n",
        "        current_config = get_config(experiment_name, env_id, transition_network_type)\n",
        "        sr, r = train_agent_according_config(current_config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qQFmTc45aGHn",
        "o5OZo3XyzE3c",
        "_IQN-i3OzJ3k",
        "h6rjacSoi3jY"
      ],
      "name": "mdp_daif_agent_mujoco_adapted-FetchPickAndPlace.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}