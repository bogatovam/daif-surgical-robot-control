{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQFmTc45aGHn"
   },
   "source": [
    "### Install custom version of simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hosUAwLdqHr0",
    "outputId": "3b4d28d9-5a4f-4771-e2cd-9213e652d4cf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
      "libgl1-mesa-dev set to manually installed.\n",
      "software-properties-common is already the newest version (0.96.24.32.18).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "Suggested packages:\n",
      "  glew-utils\n",
      "The following NEW packages will be installed:\n",
      "  libgl1-mesa-glx libglew-dev libglew2.0 libosmesa6 libosmesa6-dev\n",
      "0 upgraded, 5 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 2,916 kB of archives.\n",
      "After this operation, 12.6 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew2.0 amd64 2.0.0-5 [140 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew-dev amd64 2.0.0-5 [120 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6 amd64 20.0.8-0ubuntu1~18.04.1 [2,641 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6-dev amd64 20.0.8-0ubuntu1~18.04.1 [8,828 B]\n",
      "Fetched 2,916 kB in 0s (11.0 MB/s)\n",
      "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "(Reading database ... 155629 files and directories currently installed.)\n",
      "Preparing to unpack .../libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libglew2.0:amd64.\n",
      "Preparing to unpack .../libglew2.0_2.0.0-5_amd64.deb ...\n",
      "Unpacking libglew2.0:amd64 (2.0.0-5) ...\n",
      "Selecting previously unselected package libglew-dev:amd64.\n",
      "Preparing to unpack .../libglew-dev_2.0.0-5_amd64.deb ...\n",
      "Unpacking libglew-dev:amd64 (2.0.0-5) ...\n",
      "Selecting previously unselected package libosmesa6:amd64.\n",
      "Preparing to unpack .../libosmesa6_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libosmesa6-dev:amd64.\n",
      "Preparing to unpack .../libosmesa6-dev_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
      "Unpacking libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Setting up libglew2.0:amd64 (2.0.0-5) ...\n",
      "Setting up libglew-dev:amd64 (2.0.0-5) ...\n",
      "Setting up libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following NEW packages will be installed:\n",
      "  patchelf\n",
      "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 46.5 kB of archives.\n",
      "After this operation, 130 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 patchelf amd64 0.9-1 [46.5 kB]\n",
      "Fetched 46.5 kB in 0s (860 kB/s)\n",
      "Selecting previously unselected package patchelf.\n",
      "(Reading database ... 155667 files and directories currently installed.)\n",
      "Preparing to unpack .../patchelf_0.9-1_amd64.deb ...\n",
      "Unpacking patchelf (0.9-1) ...\n",
      "Setting up patchelf (0.9-1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61oSVQHUaOGc",
    "outputId": "c7496540-24d7-4fbd-efdb-ec465615e8f5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "c4FQVdzlaWr6",
    "outputId": "4d19300a-ce6e-41c4-f6a8-9506b9deab2f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting free-mujoco-py\n",
      "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 14.1 MB 6.7 MB/s \n",
      "\u001B[?25hCollecting fasteners==0.15\n",
      "  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
      "Collecting glfw<2.0.0,>=1.4.0\n",
      "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
      "\u001B[K     |████████████████████████████████| 203 kB 50.3 MB/s \n",
      "\u001B[?25hCollecting imageio<3.0.0,>=2.9.0\n",
      "  Downloading imageio-2.19.2-py3-none-any.whl (3.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.4 MB 41.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.21.6)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.15.0)\n",
      "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.29.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
      "Collecting monotonic>=0.1\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
      "Collecting pillow>=8.3.2\n",
      "  Downloading Pillow-9.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.1 MB 55.5 MB/s \n",
      "\u001B[?25hInstalling collected packages: pillow, monotonic, imageio, glfw, fasteners, free-mujoco-py\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.4.1\n",
      "    Uninstalling imageio-2.4.1:\n",
      "      Successfully uninstalled imageio-2.4.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001B[0m\n",
      "Successfully installed fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.2 monotonic-1.6 pillow-9.1.1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install free-mujoco-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuXOKel-aYs9",
    "outputId": "d3f44322-a3b4-4a37-b0a5-d1d3e676ee6e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compiling /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
      "[1/1] Cythonizing /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.pyx\n",
      "running build_ext\n",
      "building 'mujoco_py.cymj' extension\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/mujoco_py -I/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c /usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7\n",
      "creating /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/usr/local/lib/python3.7/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -Wl,--enable-new-dtags,-R/usr/local/lib/python3.7/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.7/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py/cymj.cpython-37m-x86_64-linux-gnu.so -fopenmp\n"
     ]
    }
   ],
   "source": [
    "import mujoco_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9PjnzJnDw5K",
    "outputId": "8ea96899-0267-4734-b00e-73e534248647"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting stable-baselines3\n",
      "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
      "\u001B[?25l\r\u001B[K     |█▉                              | 10 kB 17.1 MB/s eta 0:00:01\r\u001B[K     |███▊                            | 20 kB 19.7 MB/s eta 0:00:01\r\u001B[K     |█████▌                          | 30 kB 23.1 MB/s eta 0:00:01\r\u001B[K     |███████▍                        | 40 kB 20.0 MB/s eta 0:00:01\r\u001B[K     |█████████▏                      | 51 kB 11.9 MB/s eta 0:00:01\r\u001B[K     |███████████                     | 61 kB 11.8 MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 71 kB 13.2 MB/s eta 0:00:01\r\u001B[K     |██████████████▊                 | 81 kB 11.1 MB/s eta 0:00:01\r\u001B[K     |████████████████▋               | 92 kB 12.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████▍             | 102 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████▎           | 112 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 122 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████        | 133 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▉      | 143 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▋    | 153 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▌  | 163 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▍| 174 kB 13.1 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 177 kB 13.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (3.2.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.21.6)\n",
      "Collecting gym==0.21\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.5 MB 41.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.11.0+cu113)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3) (4.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3) (2022.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616823 sha256=1aac7da489720236727676d89fd15ab473ef8ca6e3f26c21d107cc4a9de305de\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
      "Successfully built gym\n",
      "Installing collected packages: gym, stable-baselines3\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.17.3\n",
      "    Uninstalling gym-0.17.3:\n",
      "      Successfully uninstalled gym-0.17.3\n",
      "Successfully installed gym-0.21.0 stable-baselines3-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrsTmY6KoyCg",
    "outputId": "b3852bca-d55c-4980-93f3-8967f0f63edc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pybullet==3.2.2\n",
      "  Downloading pybullet-3.2.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 91.7 MB 102 kB/s \n",
      "\u001B[?25hInstalling collected packages: pybullet\n",
      "Successfully installed pybullet-3.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSZa-9ahlOIa"
   },
   "outputs": [],
   "source": [
    "!rm -rf surrol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Glj7D3hxZCPw",
    "outputId": "6b563760-8b28-40cd-c87c-a706aca49ffb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'surrol'...\n",
      "remote: Enumerating objects: 809, done.\u001B[K\n",
      "remote: Counting objects: 100% (332/332), done.\u001B[K\n",
      "remote: Compressing objects: 100% (166/166), done.\u001B[K\n",
      "remote: Total 809 (delta 209), reused 252 (delta 162), pack-reused 477\u001B[K\n",
      "Receiving objects: 100% (809/809), 23.38 MiB | 28.44 MiB/s, done.\n",
      "Resolving deltas: 100% (432/432), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/bogatovam/surrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YxgPrOsZQQQ",
    "outputId": "759c7305-0b8c-49e4-a5e2-45671e564938"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/surrol\n"
     ]
    }
   ],
   "source": [
    "%cd surrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHHbB3Uyptr5",
    "outputId": "043f26e7-6818-4a10-a130-9a3829678ee9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Branch 'aif-dev' set up to track remote branch 'aif-dev' from 'origin'.\n",
      "Switched to a new branch 'aif-dev'\n"
     ]
    }
   ],
   "source": [
    "!git checkout aif-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMWROgXxZY3O"
   },
   "outputs": [],
   "source": [
    "!git fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW-VvuHSZqBq",
    "outputId": "4f96168b-57a2-46d6-9867-d3c3ee334912"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33mWARNING: Skipping surrol as it is not installed.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall surrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8H6B6jqZb9Y",
    "outputId": "21e00bfb-b86e-45c0-c12c-fe1698f92308"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Obtaining file:///content/surrol\n",
      "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (3.2.2)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (2.19.2)\n",
      "Collecting imageio-ffmpeg\n",
      "  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 26.9 MB 1.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (4.1.2.30)\n",
      "Collecting roboticstoolbox-python\n",
      "  Downloading roboticstoolbox_python-1.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.7 MB 41.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from surrol==0.1.0) (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio->surrol==0.1.0) (1.21.6)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio->surrol==0.1.0) (9.1.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (1.4.1)\n",
      "Collecting spatialmath-python~=1.0.0\n",
      "  Downloading spatialmath_python-1.0.0-py3-none-any.whl (178 kB)\n",
      "\u001B[K     |████████████████████████████████| 178 kB 31.8 MB/s \n",
      "\u001B[?25hCollecting pgraph-python\n",
      "  Downloading pgraph-python-0.6.1.tar.gz (16 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboticstoolbox-python->surrol==0.1.0) (3.2.2)\n",
      "Collecting swift-sim~=1.0.0\n",
      "  Downloading swift_sim-1.0.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 13.5 MB 30.7 MB/s \n",
      "\u001B[?25hCollecting spatialgeometry~=1.0.0\n",
      "  Downloading spatialgeometry-1.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.5 MB 46.0 MB/s \n",
      "\u001B[?25hCollecting rtb-data\n",
      "  Downloading rtb_data-1.0.0-py3-none-any.whl (112.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 112.2 MB 68 kB/s \n",
      "\u001B[?25hCollecting progress\n",
      "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
      "Collecting ansitable\n",
      "  Downloading ansitable-0.9.6.tar.gz (17 kB)\n",
      "Collecting colored\n",
      "  Downloading colored-1.4.3.tar.gz (29 kB)\n",
      "Collecting websockets\n",
      "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
      "\u001B[K     |████████████████████████████████| 112 kB 64.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->roboticstoolbox-python->surrol==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->roboticstoolbox-python->surrol==0.1.0) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->roboticstoolbox-python->surrol==0.1.0) (1.15.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->surrol==0.1.0) (1.2.1)\n",
      "Building wheels for collected packages: ansitable, colored, pgraph-python, progress\n",
      "  Building wheel for ansitable (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for ansitable: filename=ansitable-0.9.6-py3-none-any.whl size=12621 sha256=8a8263332b8487d12ed8ca5049f51803fed87e69452eae9c8d8d7883143c3f0a\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/a4/a9/84ed983596f261a978c880aba60eba145e76aa5a59e455f189\n",
      "  Building wheel for colored (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for colored: filename=colored-1.4.3-py3-none-any.whl size=14342 sha256=e43fcb2c7be70d8b474e5948557066c0d354cbb187f54f774fa2d60c81cf048e\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/f6/00/835e81851bc345428a253721c8bdad0062721dfb861bc6e752\n",
      "  Building wheel for pgraph-python (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pgraph-python: filename=pgraph_python-0.6.1-py3-none-any.whl size=11482 sha256=053a3cc3dff4ac5c1a20b21528065d6527f6a838af195e90b1085a89658dedcb\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/96/4a/6802f53d94902fa9431fe1d0ca769f5da8faee8894188c1fd5\n",
      "  Building wheel for progress (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9632 sha256=8dd233e5f29b7dce6d75bfd8cbff6a85d9f0ed3729fa027daa19ef84984a2834\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
      "Successfully built ansitable colored pgraph-python progress\n",
      "Installing collected packages: colored, ansitable, spatialmath-python, websockets, spatialgeometry, swift-sim, rtb-data, progress, pgraph-python, roboticstoolbox-python, imageio-ffmpeg, surrol\n",
      "  Running setup.py develop for surrol\n",
      "Successfully installed ansitable-0.9.6 colored-1.4.3 imageio-ffmpeg-0.4.7 pgraph-python-0.6.1 progress-1.6 roboticstoolbox-python-1.0.1 rtb-data-1.0.0 spatialgeometry-1.0.1 spatialmath-python-1.0.0 surrol-0.1.0 swift-sim-1.0.0 websockets-10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3M_YVGw5bEyM",
    "outputId": "fd629626-280e-4970-b8f6-79ce6fdf7727"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "total 24\n",
      "drwxr-xr-x  1 root root 4096 May 23 05:23 .\n",
      "drwxr-xr-x  1 root root 4096 May 23 05:20 ..\n",
      "drwxr-xr-x  4 root root 4096 May 17 13:38 .config\n",
      "drwx------  5 root root 4096 May 23 05:21 drive\n",
      "drwxr-xr-x  1 root root 4096 May 17 13:39 sample_data\n",
      "drwxr-xr-x 10 root root 4096 May 23 05:24 surrol\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5OZo3XyzE3c"
   },
   "source": [
    "### Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcTml9fxe5bM"
   },
   "outputs": [],
   "source": [
    "!pip install stable_baselines3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMvevkgnNCiW"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "import torch\n",
    "from stable_baselines3.common.preprocessing import maybe_transpose, is_image_space\n",
    "from stable_baselines3.common.utils import get_device, obs_as_tensor, is_vectorized_observation\n",
    "from typing import Union\n",
    "import warnings\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for all models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def set_training_mode(self, mode: bool) -> None:\n",
    "        self.train(mode)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs):\n",
    "        \"\"\"\n",
    "        Forward pass logic\n",
    "\n",
    "        :return: Model output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_constructor_parameters(self):\n",
    "        return dict()\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save model to a given location.\n",
    "\n",
    "        :param path:\n",
    "        \"\"\"\n",
    "        torch.save({\"state_dict\": self.state_dict(), \"data\": self._get_constructor_parameters()}, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, device: Union[torch.device, str] = \"auto\") -> \"BaseModel\":\n",
    "        \"\"\"\n",
    "        Load model from path.\n",
    "\n",
    "        :param path:\n",
    "        :param device: Device on which the policy should be loaded.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        device = get_device(device)\n",
    "        saved_variables = torch.load(path, map_location=device)\n",
    "\n",
    "        # Create policy object\n",
    "        model = cls(**saved_variables[\"data\"])  # pytype: disable=not-instantiable\n",
    "        # Load weights\n",
    "        model.load_state_dict(saved_variables[\"state_dict\"])\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Model prints with number of trainable parameters\n",
    "        \"\"\"\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yONHMB1NCV8"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TensorboardWriter():\n",
    "    def __init__(self, log_dir, enabled):\n",
    "        self.writer = None\n",
    "        self.selected_module = \"\"\n",
    "\n",
    "        if enabled:\n",
    "            log_dir = str(log_dir)\n",
    "\n",
    "            # Retrieve vizualization writer.\n",
    "            succeeded = False\n",
    "            for module in [\"torch.utils.tensorboard\", \"tensorboardX\"]:\n",
    "                try:\n",
    "                    self.writer = importlib.import_module(module).SummaryWriter(log_dir)\n",
    "                    succeeded = True\n",
    "                    break\n",
    "                except ImportError:\n",
    "                    succeeded = False\n",
    "                self.selected_module = module\n",
    "\n",
    "            if not succeeded:\n",
    "                message = \"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \" \\\n",
    "                    \"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \" \\\n",
    "                    \"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"\n",
    "                print(message)\n",
    "\n",
    "        self.step = 0\n",
    "        self.mode = ''\n",
    "\n",
    "        self.tb_writer_ftns = {\n",
    "            'add_scalar', 'add_scalars', 'add_image', 'add_video', 'add_images', 'add_audio',\n",
    "            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding'\n",
    "        }\n",
    "        self.tag_mode_exceptions = {'add_histogram', 'add_embedding'}\n",
    "        self.timer = datetime.now()\n",
    "\n",
    "    def set_step(self, step, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        if step == 0:\n",
    "            self.timer = datetime.now()\n",
    "        else:\n",
    "            duration = datetime.now() - self.timer\n",
    "            self.add_scalar('steps_per_sec', 1 / duration.total_seconds())\n",
    "            self.timer = datetime.now()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        If visualization is configured to use:\n",
    "            return add_data() methods of tensorboard with additional information (step, tag) added.\n",
    "        Otherwise:\n",
    "            return a blank function handle that does nothing\n",
    "        \"\"\"\n",
    "        if name in self.tb_writer_ftns:\n",
    "            add_data = getattr(self.writer, name, None)\n",
    "\n",
    "            def wrapper(tag, data, *args, **kwargs):\n",
    "                if add_data is not None:\n",
    "                    # add mode(train/valid) tag\n",
    "                    if name not in self.tag_mode_exceptions:\n",
    "                        tag = '{}/{}'.format(tag, self.mode)\n",
    "                    add_data(tag, data, self.step, *args, **kwargs)\n",
    "            return wrapper\n",
    "        else:\n",
    "            # default action for returning methods defined in this class, set_step() for instance.\n",
    "            try:\n",
    "                attr = object.__getattr__(name)\n",
    "            except AttributeError:\n",
    "                raise AttributeError(\"type object '{}' has no attribute '{}'\".format(self.selected_module, name))\n",
    "            return attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ggwi7-nNKoO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def ensure_dir(dirname):\n",
    "    dirname = Path(dirname)\n",
    "    if not dirname.is_dir():\n",
    "        dirname.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def get_all_files_from_dir(dirname):\n",
    "    return [join(dirname, f) for f in listdir(dirname) if isfile(join(dirname, f))]\n",
    "\n",
    "\n",
    "def read_json(fname):\n",
    "    fname = Path(fname)\n",
    "    with fname.open('rt') as handle:\n",
    "        return json.load(handle, object_hook=OrderedDict)\n",
    "\n",
    "\n",
    "def write_json(content, fname):\n",
    "    fname = Path(fname)\n",
    "    with fname.open('wt') as handle:\n",
    "        json.dump(content, handle, indent=4, sort_keys=False)\n",
    "\n",
    "\n",
    "def inf_loop(data_loader):\n",
    "    ''' wrapper function for endless data loader. '''\n",
    "    for loader in repeat(data_loader):\n",
    "        yield from loader\n",
    "\n",
    "\n",
    "def prepare_device(n_gpu_use):\n",
    "    \"\"\"\n",
    "    setup GPU device if available. get gpu device indices which are used for DataParallel\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu_use > 0 and n_gpu == 0:\n",
    "        print(\"Warning: There\\'s no GPU available on this machine,\"\n",
    "              \"training will be performed on CPU.\")\n",
    "        n_gpu_use = 0\n",
    "    if n_gpu_use > n_gpu:\n",
    "        print(f\"Warning: The number of GPU\\'s configured to use is {n_gpu_use}, but only {n_gpu} are \"\n",
    "              \"available on this machine.\")\n",
    "        n_gpu_use = n_gpu\n",
    "    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
    "    list_ids = list(range(n_gpu_use))\n",
    "    return device, list_ids\n",
    "\n",
    "\n",
    "class MetricTracker:\n",
    "    def __init__(self, *keys, writer=None):\n",
    "        self.writer = writer\n",
    "        self._data = pd.DataFrame(index=keys, columns=['total', 'counts', 'average'])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for col in self._data.columns:\n",
    "            self._data[col].values[:] = 0\n",
    "\n",
    "    def update(self, key, value, n=1):\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(key, value)\n",
    "        self._data.total[key] += value * n\n",
    "        self._data.counts[key] += n\n",
    "        self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
    "\n",
    "    def avg(self, key):\n",
    "        return self._data.average[key]\n",
    "\n",
    "    def result(self):\n",
    "        return dict(self._data.average)\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int, device: str = 'cpu') -> None:\n",
    "    # Seed python RNG\n",
    "    random.seed(seed)\n",
    "    # Seed numpy RNG\n",
    "    np.random.seed(seed)\n",
    "    # seed the RNG for all devices (both CPU and CUDA)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if device == 'gpu':\n",
    "        # Deterministic operations for CuDNN, it may impact performances\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def create_dirs(dirs):\n",
    "    try:\n",
    "        for dir_ in dirs:\n",
    "            if not os.path.exists(dir_):\n",
    "                os.makedirs(dir_)\n",
    "        return 0\n",
    "    except Exception as err:\n",
    "        print(\"Creating directories error: {0}\".format(err))\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IQN-i3OzJ3k"
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE5f5BPucovD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90754e72-a05c-4bab-99be-b1d034f8e88d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 78 kB 7.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 596 kB 53.4 MB/s \n",
      "\u001B[K     |████████████████████████████████| 117 kB 61.8 MB/s \n",
      "\u001B[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install omegaconf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NL6dZRWbmBA"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR_PATH = '/content/drive/MyDrive/thesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aPqqK1VcklS"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UZiaWgcdCWT"
   },
   "outputs": [],
   "source": [
    "_global_config_yaml = \"\"\"\n",
    "experiment_description: \"first good experiment\"\n",
    "\n",
    "monitor_file: logs/{experiment_name}/monitor.csv\n",
    "tb_log_folder: logs/sac/tensorboard/{experiment_name}\n",
    "seed: 42\n",
    "\n",
    "hparams:\n",
    "  n_epochs: 20\n",
    "  steps_per_epoch: 100\n",
    "  n_training_iterations: 10\n",
    "  n_rollout_episodes: 10\n",
    "  n_warmap_episodes: 50\n",
    "\n",
    "should_save_model: true\n",
    "model_path: models/sac/{experiment_name}\n",
    "model_save_timer: 1\n",
    "\n",
    "\n",
    "should_save_episode_video: true\n",
    "episode_video_timer: 5\n",
    "video_log_folder: logs/sac/gif/{experiment_name}\n",
    "\n",
    "\"\"\"\n",
    "_global_config = OmegaConf.create(_global_config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLTOgcE1ddcz"
   },
   "outputs": [],
   "source": [
    "_cloud_yaml = \"\"\"\n",
    "device_id: cpu\n",
    "render_mode: none\n",
    "\n",
    "hparams:\n",
    "  memory_capacity: 50000\n",
    "  batch_size: 512\n",
    "\n",
    "\"\"\"\n",
    "_cloud_config = OmegaConf.create(_cloud_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhOj1oIwdTeE"
   },
   "outputs": [],
   "source": [
    "_env_yaml = \"\"\"\n",
    "hparams:\n",
    "  polyak: 0.95\n",
    "\n",
    "# HER\n",
    "  replay_strategy: future\n",
    "  replay_k: 4\n",
    "\n",
    "#  Networks\n",
    "  actor_layers:  [128, 256, 128]\n",
    "  transition_net_layers: [128, 128, 128]\n",
    "  value_net_layers: [128, 256, 128]\n",
    "\n",
    "  actor_lr: 0.0003\n",
    "  transition_net_lr: 0.0003\n",
    "  value_net_lr: 0.0003\n",
    "  alpha_lr: 0.0003\n",
    "\n",
    "  alpha: auto\n",
    "  gamma: 1.00\n",
    "  beta: 0.99\n",
    "\n",
    "#  available values list:\n",
    "#  sac_maximize\n",
    "#  sac_minimise\n",
    "#  adapted_daif\n",
    "#  efe_approximation_approach: sac_minimise\n",
    "\n",
    "#  available values list:\n",
    "#  SquashedDiagGaussianDistribution\n",
    "#  StateDependentNoiseDistribution\n",
    "#  actor_action_distribution: SquashedDiagGaussianDistribution\n",
    "\n",
    "#  available values list:\n",
    "#  mlp\n",
    "#  gru\n",
    "#  lstm\n",
    "#  transition_network_type: lstm\n",
    "\n",
    "#  available values list:\n",
    "# Disentangled Beta-VAE\n",
    "# LogCosh\n",
    "# original\n",
    "  vae_type: none\n",
    "\n",
    "\"\"\"\n",
    "_env_config = OmegaConf.create(_env_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7vOWLz8cbiI"
   },
   "outputs": [],
   "source": [
    "def get_config(experiment_name, env_id, efe_approximation_approach, transition_network_type, actor_action_distribution):\n",
    "    config = OmegaConf.create()\n",
    "    config = OmegaConf.merge(config, _global_config)\n",
    "    config = OmegaConf.merge(config, _env_config)\n",
    "    config = OmegaConf.merge(config, _cloud_config)\n",
    "\n",
    "    config.experiment_name = experiment_name\n",
    "    config.env_id = env_id\n",
    "    config.hparams.efe_approximation_approach = efe_approximation_approach\n",
    "    config.hparams.transition_network_type = transition_network_type\n",
    "    config.hparams.actor_action_distribution = actor_action_distribution\n",
    "    config.hparams.observations_seq_len = 3 if transition_network_type == 'mlp' else 8\n",
    "    print(OmegaConf.to_yaml(config))\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvVNIZY8zLwJ"
   },
   "source": [
    "### hparams tuning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Main model"
   ],
   "metadata": {
    "id": "h6rjacSoi3jY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hugnCD7PTaR5"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import Tuple, Optional, NamedTuple\n",
    "\n",
    "from stable_baselines3.common.utils import polyak_update\n",
    "\n",
    "import surrol.gym as surrol_gym\n",
    "from omegaconf import OmegaConf\n",
    "from stable_baselines3.common.distributions import StateDependentNoiseDistribution, \\\n",
    "    TanhBijector, DiagGaussianDistribution, SquashedDiagGaussianDistribution\n",
    "from stable_baselines3.common.preprocessing import get_action_dim, is_image_space, maybe_transpose\n",
    "from stable_baselines3.common.torch_layers import create_mlp\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "import gym\n",
    "\n",
    "import os.path\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import threading\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vouX-hOcNNkL"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, env, max_episode_steps, buffer_size, sample_func, device):\n",
    "        self.observation_dim, self.goal_dim, self.action_dim, self.action_max = get_env_parameters(env)\n",
    "        self.device = device\n",
    "\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.size = buffer_size // self.max_episode_steps\n",
    "        # memory management\n",
    "        self.current_size = 0\n",
    "        self.n_transitions_stored = 0\n",
    "        self.sample_func = sample_func\n",
    "        # create the buffer to store info\n",
    "\n",
    "        self.observation_memory = np.empty([self.size, self.max_episode_steps, self.observation_dim], dtype=np.float32)\n",
    "        self.achieved_goal_memory = np.empty([self.size, self.max_episode_steps, self.goal_dim], dtype=np.float32)\n",
    "        self.desired_goal_memory = np.empty([self.size, self.max_episode_steps, self.goal_dim], dtype=np.float32)\n",
    "        self.actions_memory = np.empty([self.size, self.max_episode_steps, self.action_dim], dtype=np.float32)\n",
    "\n",
    "        # thread lock\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # store the episode\n",
    "    def store_episode(self, observation, achieved_goal, desired_goal, action, n_episodes_to_store):\n",
    "        with self.lock:\n",
    "            ids = self._get_storage_idx(inc=n_episodes_to_store)\n",
    "            # store the information\n",
    "            self.observation_memory[ids] = observation\n",
    "            self.achieved_goal_memory[ids] = achieved_goal\n",
    "            self.desired_goal_memory[ids] = desired_goal\n",
    "            self.actions_memory[ids] = action\n",
    "\n",
    "            self.n_transitions_stored += self.max_episode_steps * n_episodes_to_store\n",
    "\n",
    "    # sample the data from the replay buffer\n",
    "    def sample(self, batch_size):\n",
    "        observation_buffer = self.observation_memory[:self.current_size]\n",
    "        achieved_goal_buffer = self.achieved_goal_memory[:self.current_size]\n",
    "        desired_goal_buffer = self.desired_goal_memory[:self.current_size]\n",
    "        actions_buffer = self.actions_memory[:self.current_size]\n",
    "\n",
    "        return self.sample_func(observation_buffer,\n",
    "                                achieved_goal_buffer, desired_goal_buffer,\n",
    "                                actions_buffer,\n",
    "                                batch_size)\n",
    "\n",
    "    def _get_storage_idx(self, inc=None):\n",
    "        inc = inc or 1\n",
    "        if self.current_size + inc <= self.size:\n",
    "            idx = np.arange(self.current_size, self.current_size + inc)\n",
    "        elif self.current_size < self.size:\n",
    "            overflow = inc - (self.size - self.current_size)\n",
    "            idx_a = np.arange(self.current_size, self.size)\n",
    "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
    "            idx = np.concatenate([idx_a, idx_b])\n",
    "        else:\n",
    "            idx = np.random.randint(0, self.size, inc)\n",
    "        self.current_size = min(self.size, self.current_size + inc)\n",
    "        if inc == 1:\n",
    "            idx = idx[0]\n",
    "        return idx\n",
    "\n",
    "\n",
    "class HERSampler:\n",
    "    def __init__(self, replay_strategy, replay_k, seq_len, reward_func=None):\n",
    "        self.replay_strategy = replay_strategy\n",
    "        self.replay_k = replay_k\n",
    "        if self.replay_strategy == 'future':\n",
    "            self.future_p = 1 - (1. / (1 + replay_k))\n",
    "        else:\n",
    "            self.future_p = 0\n",
    "        self.reward_func = reward_func\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def sample_her_transitions(self, observation_buffer,\n",
    "                               achieved_goal_buffer, desired_goal_buffer,\n",
    "                               actions_buffer, batch_size):\n",
    "        # Trajectory length\n",
    "        trajectory_length = actions_buffer.shape[1]\n",
    "\n",
    "        # Buffer size\n",
    "        buffer_length = actions_buffer.shape[0]\n",
    "\n",
    "        # generate ids which trajectories to use\n",
    "        episode_ids = np.random.randint(low=0, high=buffer_length, size=batch_size)\n",
    "\n",
    "        # generate ids which timestamps to use\n",
    "        # - 2 because we sample for 3 sequential timestamps\n",
    "        t_samples = np.random.randint(low=0, high=trajectory_length - self.seq_len, size=batch_size)\n",
    "\n",
    "        # her idx\n",
    "        her_indexes = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
    "\n",
    "        # Sample 'future' timestamps for each 't_samples'\n",
    "        future_offset = np.random.uniform(size=batch_size) * (trajectory_length - self.seq_len - t_samples)\n",
    "        future_offset = future_offset.astype(int)\n",
    "        future_t = (t_samples + future_offset)[her_indexes]\n",
    "\n",
    "        sequential_batches = []\n",
    "        for time_i in range(self.seq_len):\n",
    "            t_i = self._sample_for_time(observation_buffer, achieved_goal_buffer, desired_goal_buffer, actions_buffer,\n",
    "                                        episode_ids, t_samples, her_indexes, future_t,\n",
    "                                        batch_size=batch_size, time=time_i)\n",
    "            sequential_batches.append(t_i)\n",
    "\n",
    "        (_, achieved_goal_batch_t1, desired_goal_batch_t1, _) = sequential_batches[-2]\n",
    "\n",
    "        # Recompute the reward for the augmented 'desired_goal'\n",
    "        # todo use achieved_goal_batch_t2 and desired_goal_batch_t1?\n",
    "        reward_batch = self.reward_func(achieved_goal_batch_t1, desired_goal_batch_t1, info=None)\n",
    "        # Recompute the termination state for the augmented 'desired_goal'\n",
    "        done_batch = reward_batch == 0\n",
    "\n",
    "        # Reshape the batch\n",
    "        reward_batch = reward_batch.reshape(batch_size, *reward_batch.shape[1:])\n",
    "        done_batch = done_batch.reshape(batch_size, *done_batch.shape[1:])\n",
    "\n",
    "        if len(done_batch.shape) == 1:\n",
    "            done_batch = done_batch.reshape(batch_size, 1)\n",
    "\n",
    "        if len(reward_batch.shape) == 1:\n",
    "            reward_batch = reward_batch.reshape(batch_size, 1)\n",
    "\n",
    "        return sequential_batches, reward_batch, done_batch\n",
    "\n",
    "    def _sample_for_time(self, observation_buffer, achieved_goal_buffer, desired_goal_buffer, actions_buffer,\n",
    "                         episode_idxs, t_samples, her_indexes, future_t, batch_size, time):\n",
    "        observation_batch = observation_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
    "        achieved_goal_batch = achieved_goal_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
    "        desired_goal_batch = desired_goal_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
    "        actions_batch = actions_buffer[:, time:, :][episode_idxs, t_samples].copy()\n",
    "\n",
    "        # Reshape the batch\n",
    "        observation_batch = observation_batch.reshape(batch_size, *observation_batch.shape[1:])\n",
    "        achieved_goal_batch = achieved_goal_batch.reshape(batch_size, *achieved_goal_batch.shape[1:])\n",
    "        desired_goal_batch = desired_goal_batch.reshape(batch_size, *desired_goal_batch.shape[1:])\n",
    "        actions_batch = actions_batch.reshape(batch_size, *actions_batch.shape[1:])\n",
    "\n",
    "        # Get the achieved_goal at the 'future' timestamps\n",
    "        next_achieved_goal = achieved_goal_buffer[:, time:, :][episode_idxs[her_indexes], future_t]\n",
    "        # Replace the 'desired_goal' with the 'next_achieved_goal'\n",
    "        desired_goal_batch[her_indexes] = next_achieved_goal\n",
    "\n",
    "        return observation_batch, achieved_goal_batch, desired_goal_batch, actions_batch\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, size, eps=1e-2, default_clip_range=np.inf):\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "        self.default_clip_range = default_clip_range\n",
    "\n",
    "        # some local information\n",
    "        self.local_sum = np.zeros(self.size, np.float32)\n",
    "        self.local_sumsq = np.zeros(self.size, np.float32)\n",
    "        self.local_count = np.zeros(1, np.float32)\n",
    "\n",
    "        # get the mean and std\n",
    "        self.mean = np.zeros(self.size, np.float32)\n",
    "        self.std = np.ones(self.size, np.float32)\n",
    "\n",
    "    # update the parameters of the normalizer\n",
    "    def update(self, v):\n",
    "        v = v.reshape(-1, self.size)\n",
    "        # do the computing\n",
    "        self.local_sum += v.sum(axis=0)\n",
    "        self.local_sumsq += (np.square(v)).sum(axis=0)\n",
    "        self.local_count[0] += v.shape[0]\n",
    "\n",
    "    def recompute_stats(self):\n",
    "        # calculate the new mean and std\n",
    "        self.mean = self.local_sum / self.local_count\n",
    "        self.std = np.sqrt(np.maximum(np.square(self.eps), (self.local_sumsq / self.local_count) - np.square(\n",
    "            self.local_sum / self.local_count)))\n",
    "\n",
    "    # normalize the observation\n",
    "    def normalize(self, v):\n",
    "        return (v - self.mean) / np.clip(self.std, 1e-9, None)\n",
    "\n",
    "\n",
    "class BasePolicy(BaseModel, ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_space,\n",
    "                 action_space,\n",
    "                 normalize_images: bool = True,\n",
    "                 squash_output: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.normalize_images = normalize_images\n",
    "        self._squash_output = squash_output\n",
    "\n",
    "    @property\n",
    "    def squash_output(self) -> bool:\n",
    "        return self._squash_output\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(module: nn.Module, gain: float = 1) -> None:\n",
    "        \"\"\"\n",
    "        Orthogonal initialization (used in PPO and A2C)\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.orthogonal_(module.weight, gain=gain)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, observation: torch.Tensor, deterministic: bool = False):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, observation):\n",
    "        return self._predict(observation)\n",
    "\n",
    "    def scale_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rescale the action from [low, high] to [-1, 1]\n",
    "        (no need for symmetric action space)\n",
    "\n",
    "        :param action: Action to scale\n",
    "        :return: Scaled action\n",
    "        \"\"\"\n",
    "        low, high = self.action_space.low, self.action_space.high\n",
    "        return 2.0 * ((action - low) / (high - low)) - 1.0\n",
    "\n",
    "    def unscale_action(self, scaled_action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "\n",
    "        :param scaled_action: Action to un-scale\n",
    "        \"\"\"\n",
    "        low, high = self.action_space.low, self.action_space.high\n",
    "        return low + (0.5 * (scaled_action + 1.0) * (high - low))\n",
    "\n",
    "    def obs_to_tensor(self, observation):\n",
    "        if isinstance(observation, dict):\n",
    "            observation = copy.deepcopy(observation)\n",
    "            for key, obs in observation.items():\n",
    "                obs_space = self.observation_space.spaces[key]\n",
    "                if is_image_space(obs_space):\n",
    "                    obs_ = maybe_transpose(obs, obs_space)\n",
    "                else:\n",
    "                    obs_ = np.array(obs)\n",
    "                observation[key] = obs_.reshape((-1,) + self.observation_space[key].shape)\n",
    "\n",
    "        elif is_image_space(self.observation_space):\n",
    "            observation = maybe_transpose(observation, self.observation_space)\n",
    "\n",
    "        else:\n",
    "            observation = np.array(observation)\n",
    "\n",
    "        return observation\n",
    "\n",
    "\n",
    "class Actor(BasePolicy):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_space: gym.spaces.Space,\n",
    "            action_space: gym.spaces.Space,\n",
    "            input_size,\n",
    "            net_arch,\n",
    "            action_distribution_type,\n",
    "            device,\n",
    "            weight_decay=0.00001,\n",
    "            lr=0.0001,\n",
    "            activation_fn=nn.ReLU,\n",
    "            log_std_init: float = -1,\n",
    "            full_std: bool = True,\n",
    "            sde_net_arch=None,\n",
    "            use_expln: bool = False,\n",
    "            clip_mean: float = 2.0,\n",
    "            normalize_images: bool = True,\n",
    "    ):\n",
    "        super().__init__(observation_space, action_space, normalize_images=normalize_images, squash_output=True)\n",
    "\n",
    "        self.sde_features_extractor = None\n",
    "        self.net_arch = net_arch\n",
    "        self.activation_fn = activation_fn\n",
    "        self.log_std_init = log_std_init\n",
    "        self.sde_net_arch = sde_net_arch\n",
    "        self.use_expln = use_expln\n",
    "        self.full_std = full_std\n",
    "        self.clip_mean = clip_mean\n",
    "        self.action_distribution_type = action_distribution_type\n",
    "\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.LOG_STD_MIN = -20\n",
    "\n",
    "        action_dim = get_action_dim(self.action_space)\n",
    "        # here will be vae\n",
    "        self.latent_pi = nn.Sequential(*create_mlp(input_size, -1, net_arch, activation_fn))\n",
    "        self.device = device\n",
    "        last_layer_dim = net_arch[-1]\n",
    "\n",
    "        if self.action_distribution_type == 'StateDependentNoiseDistribution':\n",
    "            self.action_dist = StateDependentNoiseDistribution(\n",
    "                action_dim, full_std=full_std, use_expln=use_expln, learn_features=True, squash_output=True\n",
    "            )\n",
    "            self.mu, self.log_std = self.action_dist.proba_distribution_net(\n",
    "                latent_dim=last_layer_dim, latent_sde_dim=last_layer_dim, log_std_init=log_std_init\n",
    "            )\n",
    "            if clip_mean > 0.0:\n",
    "                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n",
    "        elif self.action_distribution_type == 'SquashedDiagGaussianDistribution':\n",
    "            self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
    "            self.mu = nn.Linear(last_layer_dim, action_dim)\n",
    "            self.log_std = nn.Linear(last_layer_dim, action_dim)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.input_size = input_size\n",
    "        self.net_arch = net_arch\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "        self.activation_fn = activation_fn\n",
    "        self.log_std_init = log_std_init\n",
    "        self.full_std = full_std\n",
    "        self.sde_net_arch = sde_net_arch\n",
    "        self.use_expln = use_expln\n",
    "        self.clip_mean = clip_mean\n",
    "        self.normalize_images = normalize_images\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def _get_constructor_parameters(self):\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                action_distribution_type=self.action_distribution_type,\n",
    "                observation_space=self.observation_space,\n",
    "                action_space=self.action_space,\n",
    "                input_size=self.input_size,\n",
    "                net_arch=self.net_arch,\n",
    "                weight_decay=self.weight_decay,\n",
    "                lr=self.lr,\n",
    "                activation_fn=self.activation_fn,\n",
    "                log_std_init=self.log_std_init,\n",
    "                full_std=self.full_std,\n",
    "                sde_net_arch=self.sde_net_arch,\n",
    "                use_expln=self.use_expln,\n",
    "                clip_mean=self.clip_mean,\n",
    "                normalize_images=self.normalize_images,\n",
    "                device=self.device,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def get_std(self) -> torch.Tensor:\n",
    "        return self.action_dist.get_std(self.log_std)\n",
    "\n",
    "    def reset_noise(self, batch_size: int = 1) -> None:\n",
    "        self.action_dist.sample_weights(self.log_std, batch_size=batch_size)\n",
    "\n",
    "    def get_action_dist_params(self, observations: torch.Tensor):\n",
    "        # features = self.extract_features(observations)\n",
    "        latent_pi = self.latent_pi(observations)\n",
    "        mean_actions = self.mu(latent_pi)\n",
    "\n",
    "        if self.action_distribution_type == 'StateDependentNoiseDistribution':\n",
    "            return mean_actions, self.log_std, dict(latent_sde=latent_pi)\n",
    "\n",
    "        log_std = self.log_std(latent_pi)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean_actions, log_std, {}\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n",
    "        # Note: the action is squashed\n",
    "        return self.action_dist.actions_from_params(mean_actions, log_std, deterministic=deterministic, **kwargs)\n",
    "\n",
    "    def action_log_prob(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n",
    "        # return action and associated log prob\n",
    "        return self.action_dist.log_prob_from_params(mean_actions, log_std, **kwargs)\n",
    "\n",
    "    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        return self(observation, deterministic)\n",
    "\n",
    "\n",
    "class MLP(BaseModel):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, lr=0.0001, output_activation=torch.nn.Identity,\n",
    "                 activation=torch.nn.ReLU, weight_decay=1e-6, device='cpu'):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = create_mlp(input_size, output_size, layer_sizes, activation)\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr)  # Adam optimizer\n",
    "\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activation\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransitionModelMlpPreprocessor:\n",
    "    def __init__(self, preprocess_func, device):\n",
    "        super(TransitionModelMlpPreprocessor, self).__init__()\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.device = device\n",
    "\n",
    "    def preprocess(self, sequence_of_batches):\n",
    "        (observation_batch, _, desired_goal_batch, actions_batch_t0) = sequence_of_batches[0]\n",
    "        state_batch_t0 = self.preprocess_func(observation_batch, desired_goal_batch)\n",
    "        return torch.cat((state_batch_t0, as_tensor(actions_batch_t0, self.device)), dim=1)\n",
    "\n",
    "\n",
    "class TransitionModelRnnPreprocessor:\n",
    "    def __init__(self, preprocess_func, device):\n",
    "        super(TransitionModelRnnPreprocessor, self).__init__()\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.device = device\n",
    "\n",
    "    def preprocess(self, sequence_of_batches):\n",
    "        final_batch = []\n",
    "        for time in range(len(sequence_of_batches)):\n",
    "            (observation_batch, _, desired_goal_batch, actions_batch) = sequence_of_batches[time]\n",
    "            state_batch = self.preprocess_func(observation_batch, desired_goal_batch)\n",
    "            final_batch.append(torch.cat((state_batch, as_tensor(actions_batch, self.device)), dim=1))\n",
    "        return torch.stack(final_batch, dim=1)\n",
    "\n",
    "\n",
    "class LSTM(BaseModel):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, lr=0.0001, output_activation=torch.nn.Identity,\n",
    "                 activation=torch.nn.ReLU, drop_prob=0.2, device='cpu'):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layer_dim = len(layer_sizes)\n",
    "        self.hidden_dim = layer_sizes[0]\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, self.hidden_dim, self.layer_dim, batch_first=True, dropout=drop_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr)  # Adam optimizer\n",
    "\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.drop_prob = drop_prob\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def _get_constructor_parameters(self):\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                input_size=self.input_size,\n",
    "                layer_sizes=self.layer_sizes,\n",
    "                output_size=self.output_size,\n",
    "                lr=self.lr,\n",
    "                output_activation=self.output_activation,\n",
    "                activation=self.activation,\n",
    "                drop_prob=self.drop_prob,\n",
    "                device=self.device,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "\n",
    "class GRU(BaseModel):\n",
    "    def __init__(self, input_size, layer_sizes, output_size,\n",
    "                 lr=0.0001, output_activation=torch.nn.Identity,\n",
    "                 activation=torch.nn.ReLU, drop_prob=0.2, device='cpu'):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.layer_dim = len(layer_sizes)\n",
    "        self.hidden_dim = layer_sizes[0]\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, self.hidden_dim, self.layer_dim, batch_first=True, dropout=drop_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        self.output_activation = output_activation\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def _get_constructor_parameters(self):\n",
    "        data = super()._get_constructor_parameters()\n",
    "\n",
    "        data.update(\n",
    "            dict(\n",
    "                input_size=self.input_size,\n",
    "                layer_sizes=self.layer_sizes,\n",
    "                output_size=self.output_size,\n",
    "                lr=self.lr,\n",
    "                output_activation=self.output_activation,\n",
    "                activation=self.activation,\n",
    "                drop_prob=self.drop_prob,\n",
    "                device=self.device,\n",
    "            )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "\n",
    "class EpisodeData:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observation = []\n",
    "        self.achieved_goal = []\n",
    "        self.desired_goal = []\n",
    "        self.action = []\n",
    "\n",
    "    def add(self, observation, achieved_goal, desired_goal, action):\n",
    "        self.observation.append(observation)\n",
    "        self.achieved_goal.append(achieved_goal)\n",
    "        self.desired_goal.append(desired_goal)\n",
    "        self.action.append(action)\n",
    "\n",
    "    def as_numpy_arrays(self):\n",
    "        return np.asarray(self.observation), np.asarray(self.achieved_goal), \\\n",
    "               np.asarray(self.desired_goal), np.asarray(self.action)\n",
    "\n",
    "    @classmethod\n",
    "    def as_dict_of_numpy_arrays(cls, collected_step_episodes):\n",
    "        data = EpisodeData()\n",
    "\n",
    "        for elem in collected_step_episodes:\n",
    "            data.add(*elem.as_numpy_arrays())\n",
    "\n",
    "        data_as_dict = data.__dict__\n",
    "        for key, value in data.__dict__.items():\n",
    "            data_as_dict[key] = np.asarray(value)\n",
    "        return data_as_dict\n",
    "\n",
    "\n",
    "class EpisodeSummary:\n",
    "    def __init__(self):\n",
    "        self.done = []\n",
    "        self.reward = []\n",
    "\n",
    "    def add(self, reward, done):\n",
    "        self.done.append(done)\n",
    "        self.reward.append(reward)\n",
    "\n",
    "    def calc_summary(self):\n",
    "        return np.mean(self.reward), np.mean(self.done)\n",
    "\n",
    "    @classmethod\n",
    "    def as_dict_of_values(cls, collected_step_summary):\n",
    "        data = EpisodeSummary()\n",
    "\n",
    "        for elem in collected_step_summary:\n",
    "            data.add(*elem.calc_summary())\n",
    "\n",
    "        data_as_dict = data.__dict__\n",
    "        for key, value in data.__dict__.items():\n",
    "            data_as_dict[key] = np.mean(value)\n",
    "        return data_as_dict\n",
    "\n",
    "\n",
    "class SacMaximise:\n",
    "\n",
    "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
    "        self.actor = actor\n",
    "        self.value_net = value_net\n",
    "        self.target_net = target_net\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
    "                               actions_batch_t1,\n",
    "                               reward_batch, done_batch,\n",
    "                               pred_error_batch_t0t1, alpha):\n",
    "        with torch.no_grad():\n",
    "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
    "\n",
    "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
    "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
    "\n",
    "            # H_t2 ~ -log_prob_t2\n",
    "            weighted_targets = target_expected_free_energies_batch_t2 - alpha * log_prob_t2.reshape(-1, 1)\n",
    "\n",
    "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
    "            expected_free_energy_estimate_batch = (\n",
    "                    reward_batch - pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
    "\n",
    "        # Determine the Expected free energy at time t1 according to the value network:\n",
    "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
    "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
    "\n",
    "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
    "        mse = 0.5 * F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
    "        return mse\n",
    "\n",
    "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
    "                                        pred_error_batch_t0t1, alpha):\n",
    "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
    "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
    "\n",
    "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 - self.gamma * expected_free_energy_t1\n",
    "        return torch.mean(vfe_batch)\n",
    "\n",
    "\n",
    "class SacMinimise:\n",
    "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
    "        self.actor = actor\n",
    "        self.value_net = value_net\n",
    "        self.target_net = target_net\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
    "                               actions_batch_t1,\n",
    "                               reward_batch, done_batch,\n",
    "                               pred_error_batch_t0t1, alpha):\n",
    "        with torch.no_grad():\n",
    "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
    "\n",
    "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
    "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
    "\n",
    "            # H_t2 ~ -log_prob_t2\n",
    "            weighted_targets = target_expected_free_energies_batch_t2 + alpha * log_prob_t2.reshape(-1, 1)\n",
    "\n",
    "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
    "            expected_free_energy_estimate_batch = (\n",
    "                    -reward_batch + pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
    "\n",
    "        # Determine the Expected free energy at time t1 according to the value network:\n",
    "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
    "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
    "\n",
    "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
    "        mse = 0.5 * F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
    "        return mse\n",
    "\n",
    "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
    "                                        pred_error_batch_t0t1, alpha):\n",
    "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
    "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
    "\n",
    "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 + self.gamma * expected_free_energy_t1\n",
    "        return torch.mean(vfe_batch)\n",
    "\n",
    "\n",
    "class SacMinimiseEntropy:\n",
    "\n",
    "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
    "        self.actor = actor\n",
    "        self.value_net = value_net\n",
    "        self.target_net = target_net\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
    "                               actions_batch_t1,\n",
    "                               reward_batch, done_batch,\n",
    "                               pred_error_batch_t0t1, alpha):\n",
    "        with torch.no_grad():\n",
    "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
    "\n",
    "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
    "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
    "\n",
    "            # H_t2 ~ -log_prob_t2\n",
    "            weighted_targets = target_expected_free_energies_batch_t2 - alpha * log_prob_t2.reshape(-1, 1)\n",
    "\n",
    "            # Determine the batch of bootstrapped estimates of the EFEs:\n",
    "            expected_free_energy_estimate_batch = (\n",
    "                    -reward_batch + pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
    "\n",
    "        # Determine the Expected free energy at time t1 according to the value network:\n",
    "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
    "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
    "\n",
    "        # Determine the MSE loss between the EFE estimates and the value network output:\n",
    "        mse = 0.5 * F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
    "        return mse\n",
    "\n",
    "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
    "                                        pred_error_batch_t0t1, alpha):\n",
    "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
    "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
    "\n",
    "        vfe_batch = pred_error_batch_t0t1 + alpha * pred_log_prob_t1 + self.gamma * expected_free_energy_t1\n",
    "        return torch.mean(vfe_batch)\n",
    "\n",
    "\n",
    "class AdaptedDaif:\n",
    "\n",
    "    def __init__(self, actor, value_net, target_net, beta, gamma):\n",
    "        self.actor = actor\n",
    "        self.value_net = value_net\n",
    "        self.target_net = target_net\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_value_net_loss(self, state_batch_t1, state_batch_t2,\n",
    "                               actions_batch_t1,\n",
    "                               reward_batch, done_batch,\n",
    "                               pred_error_batch_t0t1, alpha):\n",
    "        with torch.no_grad():\n",
    "            actions_t2, log_prob_t2 = self.actor.action_log_prob(state_batch_t2)\n",
    "\n",
    "            targe_net_input = torch.cat([state_batch_t2, actions_t2], dim=1)\n",
    "            target_expected_free_energies_batch_t2 = self.target_net(targe_net_input)\n",
    "\n",
    "            weighted_targets = -log_prob_t2 * target_expected_free_energies_batch_t2\n",
    "\n",
    "            expected_free_energy_estimate_batch = (\n",
    "                    -reward_batch + pred_error_batch_t0t1 + (1 - done_batch) * self.beta * weighted_targets)\n",
    "\n",
    "        value_net_input_t1 = torch.cat([state_batch_t1, actions_batch_t1], dim=1)\n",
    "        value_net_output_t1 = self.value_net(value_net_input_t1)\n",
    "\n",
    "        mse = 0.5 * F.mse_loss(expected_free_energy_estimate_batch, value_net_output_t1)\n",
    "        return mse\n",
    "\n",
    "    def compute_variational_free_energy(self, state_batch_t1, predicted_actions_t1, pred_log_prob_t1,\n",
    "                                        pred_error_batch_t0t1, alpha):\n",
    "        value_net_input = torch.cat([state_batch_t1, predicted_actions_t1], dim=1)\n",
    "        expected_free_energy_t1 = self.value_net(value_net_input)\n",
    "\n",
    "        # Weigh them according to the action distribution:\n",
    "        energy_batch = (-self.gamma * expected_free_energy_t1)\n",
    "\n",
    "        # Determine the entropy of the action distribution\n",
    "        entropy_batch = -pred_log_prob_t1 * alpha\n",
    "\n",
    "        # Determine the Variable Free Energy, then take the mean over all batch samples:\n",
    "        vfe_batch = pred_error_batch_t0t1 + (energy_batch - entropy_batch)\n",
    "        vfe = torch.mean(vfe_batch)\n",
    "        return vfe\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, config):\n",
    "        self.env = env\n",
    "        self.experiment_name = config.experiment_name\n",
    "        self.experiment_description = config.experiment_description\n",
    "        self.observation_dim, self.goal_dim, self.action_dim, self.action_max = get_env_parameters(env)\n",
    "\n",
    "        self.device = config.device_id\n",
    "\n",
    "        self.polyak = int(config.hparams.polyak)\n",
    "\n",
    "        self.actor_action_distribution = config.hparams.actor_action_distribution\n",
    "        self.n_warmap_episodes = int(config.hparams.n_warmap_episodes)\n",
    "\n",
    "        self.n_epochs = int(config.hparams.n_epochs)\n",
    "        self.steps_per_epoch = int(config.hparams.steps_per_epoch)\n",
    "        self.n_rollout_episodes = int(config.hparams.n_rollout_episodes)\n",
    "        self.n_training_iterations = int(config.hparams.n_training_iterations)\n",
    "        self._max_episode_steps = env._max_episode_steps\n",
    "\n",
    "        self.batch_size = int(config.hparams.batch_size)\n",
    "        self.memory_capacity = int(config.hparams.memory_capacity)\n",
    "\n",
    "        self.gamma = float(config.hparams.gamma)  # A precision parameter\n",
    "        self.beta = float(config.hparams.beta)  # The discount rate\n",
    "        self.alpha = config.hparams.alpha  # The discount rate\n",
    "\n",
    "        self.should_save_model = interpret_boolean(config.should_save_model)\n",
    "        self.model_path = prepare_path(config.model_path, experiment_name=config.experiment_name)\n",
    "        self.video_log_path = os.path.join(\n",
    "            prepare_path(config.video_log_folder, experiment_name=config.experiment_name), \"epoch-{}.gif\")\n",
    "        self.model_save_timer = int(config.model_save_timer)\n",
    "\n",
    "        self.should_save_episode_video = interpret_boolean(config.should_save_episode_video)\n",
    "        self.episode_video_timer = int(config.episode_video_timer)\n",
    "\n",
    "        self.state_shape = np.add(self.env.observation_space['observation'].shape,\n",
    "                                  self.env.observation_space['desired_goal'].shape)\n",
    "        self.state_size = np.prod(self.state_shape)\n",
    "\n",
    "        self.actions_shape = self.env.action_space.shape\n",
    "        self.action_dim = self.env.action_space.shape[-1]\n",
    "        self.observations_seq_len = config.hparams.observations_seq_len  # The discount rate\n",
    "\n",
    "        assert (self.n_rollout_episodes >= self.observations_seq_len)\n",
    "        assert (self.n_warmap_episodes >= self.observations_seq_len)\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.actor = Actor(env.observation_space, env.action_space,\n",
    "                           self.state_size,\n",
    "                           OmegaConf.to_object(config.hparams.actor_layers),\n",
    "                           action_distribution_type=self.actor_action_distribution,\n",
    "                           lr=config.hparams.actor_lr,\n",
    "                           device=self.device)\n",
    "\n",
    "        self.value_net = MLP(self.state_size + self.action_dim,\n",
    "                             OmegaConf.to_object(config.hparams.value_net_layers),\n",
    "                             1,\n",
    "                             lr=config.hparams.value_net_lr,\n",
    "                             device=self.device)\n",
    "        self.target_net = MLP(self.state_size + self.action_dim,\n",
    "                              OmegaConf.to_object(config.hparams.value_net_layers),\n",
    "                              1,\n",
    "                              lr=config.hparams.value_net_lr,\n",
    "                              device=self.device)\n",
    "\n",
    "        # entropy coeff settings\n",
    "        self.log_alpha = None\n",
    "        self.alpha_optimizer = None\n",
    "        self.alpha_tensor = None\n",
    "        self.target_entropy = -np.prod(self.env.action_space.shape).astype(np.float32)\n",
    "        if isinstance(self.alpha, str) and self.alpha.startswith(\"auto\"):\n",
    "            init_value = 1.0\n",
    "            self.log_alpha = torch.log(torch.ones(1, device=self.device) * init_value).requires_grad_(True)\n",
    "            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.hparams.alpha_lr)\n",
    "        else:\n",
    "            self.alpha_tensor = torch.tensor(float(self.alpha)).to(self.device)\n",
    "\n",
    "        if config.hparams.efe_approximation_approach == 'sac_maximize':\n",
    "            self.efe_approximation_approach = SacMaximise(self.actor, self.value_net, self.target_net,\n",
    "                                                          self.beta, self.gamma)\n",
    "        elif config.hparams.efe_approximation_approach == 'sac_minimise':\n",
    "            self.efe_approximation_approach = SacMinimise(self.actor, self.value_net, self.target_net,\n",
    "                                                          self.beta, self.gamma)\n",
    "        elif config.hparams.efe_approximation_approach == 'sac_minimize_entropy':\n",
    "            self.efe_approximation_approach = SacMinimiseEntropy(self.actor, self.value_net, self.target_net,\n",
    "                                                                 self.beta, self.gamma)\n",
    "        elif config.hparams.efe_approximation_approach == 'adapted_daif':\n",
    "            self.efe_approximation_approach = AdaptedDaif(self.actor, self.value_net, self.target_net,\n",
    "                                                          self.beta, self.gamma)\n",
    "\n",
    "        self.her_module = HERSampler(config.hparams.replay_strategy, config.hparams.replay_k, self.observations_seq_len,\n",
    "                                     self.env.compute_reward)\n",
    "        # create the replay buffer\n",
    "        self.buffer = ReplayBuffer(self.env, self._max_episode_steps, self.memory_capacity,\n",
    "                                   self.her_module.sample_her_transitions, config.device_id)\n",
    "\n",
    "        self.o_norm = Normalizer(size=env.observation_space.spaces['observation'].shape[0])\n",
    "        self.g_norm = Normalizer(size=env.observation_space.spaces['desired_goal'].shape[0])\n",
    "        self.a_norm = Normalizer(size=self.action_dim)\n",
    "        self.target_update_interval = 1\n",
    "\n",
    "        self.writer = TensorboardWriter(prepare_path(config.tb_log_folder, experiment_name=config.experiment_name),\n",
    "                                        True)\n",
    "\n",
    "        self.train_metrics = MetricTracker('vfe', 'value_net_loss', 'alpha', 'alpha_loss', 'success_rate', 'reward',\n",
    "                                           'value_net_grad', 'sde_std', \n",
    "                                           writer=self.writer)\n",
    "\n",
    "        self.val_metrics = MetricTracker('val/success_rate', 'val/reward', writer=self.writer)\n",
    "\n",
    "        # just to save model configuration to logs\n",
    "        self.config_as_dict = OmegaConf.to_object(config.hparams)\n",
    "        self.config_as_dict['actor_layers'] = str(self.config_as_dict['actor_layers'])\n",
    "        self.config_as_dict['value_net_layers'] = str(self.config_as_dict['value_net_layers'])\n",
    "        self.config_as_dict['max_episode_steps'] = str(env._max_episode_steps)\n",
    "\n",
    "        with open(os.path.join(self.model_path, \"config.yaml\"), 'w+') as file:\n",
    "            OmegaConf.save(config, file)\n",
    "\n",
    "    def restore(self):\n",
    "        self.actor = self.actor.load(os.path.join(self.model_path, 'actor.pth'), self.device)\n",
    "        self.value_net = self.value_net.load(os.path.join(self.model_path, 'value_net.pth'), self.device)\n",
    "        self.target_net.load_state_dict(self.value_net.state_dict(), self.device)\n",
    "        self.current_epoch = int(self.model_path.split('epoch-')[1]) + 1\n",
    "\n",
    "        if self.log_alpha is not None:\n",
    "            saved_log_alpha = torch.load(os.path.join(self.model_path, 'log_alpha.pt'))\n",
    "            self.log_alpha = saved_log_alpha\n",
    "\n",
    "        if self.alpha_optimizer is not None:\n",
    "            saved_alpha_optimizer = torch.load(os.path.join(self.model_path, 'alpha_optimizer.pt'))\n",
    "            self.alpha_optimizer.load_state_dict(state_dict=saved_alpha_optimizer[\"state_dict\"])\n",
    "\n",
    "        if self.alpha_tensor is not None:\n",
    "            saved_alpha_tensor = torch.load(os.path.join(self.model_path, 'alpha_tensor.pt'))\n",
    "            self.alpha_tensor = saved_alpha_tensor\n",
    "\n",
    "    def get_mini_batches(self):\n",
    "        (sequential_batches, reward_batch, done_batch) = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        _, t1, t2 = sequential_batches[:-2], sequential_batches[-2], sequential_batches[-1]\n",
    "\n",
    "        (observation_batch_t1, achieved_goal_batch_t1, desired_goal_batch_t1, actions_batch_t1) = t1\n",
    "        (observation_batch_t2, achieved_goal_batch_t2, desired_goal_batch_t2, actions_batch_t2) = t2\n",
    "\n",
    "        state_batch_t1 = self._preprocess_batch_inputs(observation_batch_t1, desired_goal_batch_t1)\n",
    "        state_batch_t2 = self._preprocess_batch_inputs(observation_batch_t2, desired_goal_batch_t2)\n",
    "\n",
    "        return (state_batch_t1, state_batch_t2,\n",
    "                as_tensor(actions_batch_t1, self.device),\n",
    "                as_tensor(reward_batch, self.device),\n",
    "                as_tensor(done_batch, self.device))\n",
    "\n",
    "    def _update_network(self):\n",
    "        if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
    "            self.actor.reset_noise()\n",
    "\n",
    "        # Retrieve transition data in mini batches:\n",
    "        (state_batch_t1, state_batch_t2,\n",
    "         actions_batch_t1,\n",
    "         reward_batch, done_batch) = self.get_mini_batches()\n",
    "\n",
    "        # Action by the current actor for the sampled state\n",
    "        actions_pi, log_prob = self.actor.action_log_prob(state_batch_t1)\n",
    "        log_prob = log_prob.reshape(-1, 1)\n",
    "\n",
    "        alpha_loss = None\n",
    "        if self.alpha_optimizer is not None:\n",
    "            # Important: detach the variable from the graph\n",
    "            # so we don't change it with other losses\n",
    "            # see https://github.com/rail-berkeley/softlearning/issues/60\n",
    "            alpha = torch.exp(self.log_alpha.detach())\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "        else:\n",
    "            alpha = self.alpha_tensor\n",
    "\n",
    "        # Optimize entropy coefficient, also called\n",
    "        # entropy temperature or alpha in the paper\n",
    "        if alpha_loss is not None:\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy\n",
    "            next_actions, next_log_prob = self.actor.action_log_prob(state_batch_t2)\n",
    "            # Compute the next Q values: min over all critics targets\n",
    "            next_q_values = self.target_net(torch.cat([state_batch_t2, next_actions], dim=1))\n",
    "            # add entropy term\n",
    "            next_q_values = next_q_values - alpha * next_log_prob.reshape(-1, 1)\n",
    "            # td error + entropy term\n",
    "            target_q_values = reward_batch.reshape(-1, 1) + (\n",
    "                    1.0 - done_batch.reshape(-1, 1)) * self.gamma * next_q_values\n",
    "\n",
    "        # Get current Q-values estimates for each critic network\n",
    "        # using action from the replay buffer\n",
    "\n",
    "        current_q_values = self.value_net(torch.cat([state_batch_t1, actions_batch_t1], dim=1))\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.value_net.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.value_net.optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n",
    "        # Mean over all critic networks\n",
    "        min_qf_pi = self.value_net(torch.cat([state_batch_t1, actions_pi], dim=1))\n",
    "        actor_loss = (alpha * log_prob - min_qf_pi).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        polyak_update(self.value_net.parameters(), self.target_net.parameters(), 0.005)\n",
    "\n",
    "        metrics = dict(\n",
    "            vfe=actor_loss.item(),\n",
    "            alpha=alpha.item(),\n",
    "            alpha_loss=alpha_loss.detach().item(),\n",
    "            value_net_loss=critic_loss.item()\n",
    "        )\n",
    "        return metrics\n",
    "\n",
    "    # soft update\n",
    "    def _soft_update_target_network(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_((1 - self.polyak) * param.data + self.polyak * target_param.data)\n",
    "\n",
    "    # do the evaluation\n",
    "    def _eval_agent(self, epoch):\n",
    "        images = []\n",
    "        reward_array = []\n",
    "        done = []\n",
    "        episode_step = 0\n",
    "\n",
    "        observation, _, desired_goal, _, _ = self._reset()\n",
    "        while episode_step < self._max_episode_steps:\n",
    "            input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
    "            action = self._select_action(input_tensor)\n",
    "\n",
    "            new_observation, reward, _, info = self.env.step(action)\n",
    "\n",
    "            observation = new_observation['observation']\n",
    "            reward_array.append(reward)\n",
    "            done.append(info['is_success'])\n",
    "            episode_step += 1\n",
    "\n",
    "            if self.should_save_episode_video and epoch % self.episode_video_timer == 0:\n",
    "                images += [self.env.render(mode='rgb_array')]\n",
    "\n",
    "        return np.mean(np.asarray(done)), np.mean(np.asarray(reward_array)), np.asarray(images)\n",
    "\n",
    "    def train(self):\n",
    "        self.writer.add_text(self.experiment_name, self.experiment_description)\n",
    "        print(\"Environment is: {}\\nTraining started at {}\".format(self.env.unwrapped.spec.id, datetime.now()))\n",
    "\n",
    "        self.warmup()\n",
    "        for epoch in range(self.current_epoch, self.n_epochs + 1):\n",
    "            for cycle in range(self.steps_per_epoch):\n",
    "                step = self.steps_per_epoch * epoch + cycle\n",
    "                self.writer.set_step(step)\n",
    "\n",
    "                collected_step_summary = []\n",
    "                collected_step_episodes = []\n",
    "                for _ in range(self.n_rollout_episodes):\n",
    "\n",
    "                    episode_data, episode_summary = EpisodeData(), EpisodeSummary()\n",
    "                    observation, achieved_goal, desired_goal, done, reward = self._reset()\n",
    "\n",
    "                    for episode_step in range(self._max_episode_steps):\n",
    "                        input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
    "                        action = self._select_action(input_tensor)\n",
    "\n",
    "                        # feed the actions into the environment\n",
    "                        new_observation, reward, _, info = self.env.step(action)\n",
    "\n",
    "                        episode_data.add(observation.copy(), achieved_goal.copy(), desired_goal.copy(), action.copy())\n",
    "                        episode_summary.add(np.mean(reward), info['is_success'])\n",
    "\n",
    "                        observation = new_observation['observation']\n",
    "                        achieved_goal = new_observation['achieved_goal']\n",
    "\n",
    "                    collected_step_episodes.append(episode_data)\n",
    "                    collected_step_summary.append(episode_summary)\n",
    "\n",
    "                collected_step_episodes = EpisodeData.as_dict_of_numpy_arrays(collected_step_episodes)\n",
    "                collected_step_summary = EpisodeSummary.as_dict_of_values(collected_step_summary)\n",
    "\n",
    "                # store the episodes\n",
    "                self.buffer.store_episode(**collected_step_episodes, n_episodes_to_store=self.n_rollout_episodes)\n",
    "                self._update_normalizer(**collected_step_episodes)\n",
    "\n",
    "                train_iteration_metrics = []\n",
    "                for _ in range(self.n_training_iterations):\n",
    "                    # train the network\n",
    "                    metrics_dict = self._update_network()\n",
    "                    train_iteration_metrics.append(metrics_dict)\n",
    "\n",
    "                train_iteration_metrics = {k: [dic[k] for dic in train_iteration_metrics]\n",
    "                                           for k in train_iteration_metrics[0]}\n",
    "\n",
    "                for metric, value in train_iteration_metrics.items():\n",
    "                    self.train_metrics.update(metric, np.mean(value))\n",
    "\n",
    "                if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
    "                    self.train_metrics.update('sde_std', (self.actor.get_std()).mean().item())\n",
    "\n",
    "                # soft update\n",
    "                if cycle % self.target_update_interval == 0:\n",
    "                    polyak_update(self.value_net.parameters(), self.target_net.parameters(), 0.005)\n",
    "\n",
    "                success_rate = collected_step_summary['done']\n",
    "                reward = collected_step_summary['reward']\n",
    "\n",
    "                self.train_metrics.update('success_rate', success_rate)\n",
    "                self.train_metrics.update('reward', reward)\n",
    "                self.log_models_parameters()\n",
    "\n",
    "                print(\"Epoch: {:4d}, Step: {:4d}, reward: {:3.2f}, success_rate: {:3.2f}\".format(epoch, cycle,\n",
    "                                                                                                 reward,\n",
    "                                                                                                 success_rate))\n",
    "\n",
    "            success_rate, reward, images = self._eval_agent(epoch)\n",
    "\n",
    "            self.val_metrics.update('val/success_rate', success_rate)\n",
    "            self.val_metrics.update('val/reward', reward)\n",
    "\n",
    "            if self.should_save_episode_video and epoch % self.episode_video_timer == 0:\n",
    "                imageio.mimsave(self.video_log_path.format(epoch), images)\n",
    "\n",
    "            if self.should_save_model and epoch > 0 and epoch % self.model_save_timer == 0:\n",
    "                epoch_path = self.model_path + \"/epoch_\" + str(epoch)\n",
    "                create_dirs([epoch_path])\n",
    "                self.actor.save(os.path.join(epoch_path, 'actor.pth'))\n",
    "                self.value_net.save(os.path.join(epoch_path, 'value_net.pth'))\n",
    "\n",
    "                if self.log_alpha is not None:\n",
    "                    torch.save(self.log_alpha, os.path.join(epoch_path, 'log_alpha.pt'))\n",
    "\n",
    "                if self.alpha_optimizer is not None:\n",
    "                    torch.save({\"state_dict\": self.alpha_optimizer.state_dict()},\n",
    "                               os.path.join(epoch_path, 'alpha_optimizer.pt'))\n",
    "\n",
    "                if self.alpha_tensor is not None:\n",
    "                    torch.save(self.alpha_tensor, os.path.join(epoch_path, 'alpha_tensor.pt'))\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training finished at {}\".format(datetime.now()))\n",
    "        return success_rate, reward\n",
    "\n",
    "\n",
    "    def log_models_parameters(self):\n",
    "        # add histogram of model parameters to the tensorboard\n",
    "        for name, p in self.actor.named_parameters():\n",
    "            self.writer.add_histogram('actor_' + name, p, bins='auto')\n",
    "        for name, p in self.value_net.named_parameters():\n",
    "            self.writer.add_histogram('value_net_' + name, p, bins='auto')\n",
    "\n",
    "    def _reset(self):\n",
    "        self.train_metrics.reset()\n",
    "        native_observation = self.env.reset()\n",
    "\n",
    "        observation = native_observation['observation']\n",
    "        achieved_goal = native_observation['achieved_goal']\n",
    "        desired_goal = native_observation['desired_goal']\n",
    "\n",
    "        if self.actor_action_distribution == 'StateDependentNoiseDistribution':\n",
    "            self.actor.reset_noise()\n",
    "\n",
    "        return observation, achieved_goal, desired_goal, False, 0\n",
    "\n",
    "    def _select_action(self, input_tensor):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor.predict(input_tensor)\n",
    "            return action.cpu().numpy().flatten()\n",
    "\n",
    "    def _preprocess_inputs(self, observation, goal):\n",
    "        observation = self.o_norm.normalize(observation)\n",
    "        goal = self.g_norm.normalize(goal)\n",
    "        # concatenate the stuffs\n",
    "        inputs = np.concatenate([observation, goal])\n",
    "        return torch.tensor(inputs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "    def _preprocess_batch_inputs(self, observation_batch, goal_batch):\n",
    "        observation_batch = self.o_norm.normalize(observation_batch)\n",
    "        goal_batch = self.g_norm.normalize(goal_batch)\n",
    "        # concatenate the stuffs\n",
    "        inputs = np.concatenate([observation_batch, goal_batch], axis=1)\n",
    "        return torch.tensor(inputs, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def _update_normalizer(self, observation, achieved_goal, desired_goal, action):\n",
    "        # get the number of normalization transitions\n",
    "        num_transitions = action.shape[0]\n",
    "        # create the new buffer to store them\n",
    "        sequential_batches, reward_batch, done_batch = self.her_module.sample_her_transitions(observation,\n",
    "                                                                                              achieved_goal,\n",
    "                                                                                              desired_goal,\n",
    "                                                                                              action,\n",
    "                                                                                              num_transitions)\n",
    "\n",
    "        (observation_batch, _, desired_goal_batch, actions) = sequential_batches[0]\n",
    "\n",
    "        # update\n",
    "        self.o_norm.update(observation_batch)\n",
    "        self.g_norm.update(desired_goal_batch)\n",
    "        self.a_norm.update(actions)\n",
    "        # recompute the stats\n",
    "        self.o_norm.recompute_stats()\n",
    "        self.g_norm.recompute_stats()\n",
    "        self.a_norm.recompute_stats()\n",
    "\n",
    "    def warmup(self):\n",
    "        for step in range(1):\n",
    "            cycle_summary_data = {'done': [], 'reward': []}\n",
    "            cycle_data = {'observation': [], 'achieved_goal': [], 'desired_goal': [], 'action': []}\n",
    "\n",
    "            for _ in range(self.n_warmap_episodes):\n",
    "                observation, achieved_goal, desired_goal, done, reward = self._reset()\n",
    "\n",
    "                episode_summary_data = {'done': [], 'reward': []}\n",
    "                episode_data = {'observation': [], 'achieved_goal': [], 'desired_goal': [], 'action': []}\n",
    "\n",
    "                for episode_step in range(self._max_episode_steps):\n",
    "                    input_tensor = self._preprocess_inputs(observation, desired_goal)\n",
    "                    action = self._select_action(input_tensor)\n",
    "\n",
    "                    episode_data['observation'].append(observation.copy())\n",
    "                    episode_data['achieved_goal'].append(achieved_goal.copy())\n",
    "                    episode_data['desired_goal'].append(desired_goal.copy())\n",
    "                    episode_data['action'].append(action.copy())\n",
    "\n",
    "                    # feed the actions into the environment\n",
    "                    new_observation, reward, _, info = self.env.step(action)\n",
    "\n",
    "                    episode_summary_data['reward'].append(np.mean(reward))\n",
    "                    episode_summary_data['done'].append(info['is_success'])\n",
    "\n",
    "                    observation = new_observation['observation']\n",
    "                    achieved_goal = new_observation['achieved_goal']\n",
    "\n",
    "                cycle_data['observation'].append(np.asarray(episode_data['observation'], dtype=np.float32))\n",
    "                cycle_data['achieved_goal'].append(np.asarray(episode_data['achieved_goal'], dtype=np.float32))\n",
    "                cycle_data['desired_goal'].append(np.asarray(episode_data['desired_goal'], dtype=np.float32))\n",
    "                cycle_data['action'].append(np.asarray(episode_data['action'], dtype=np.float32))\n",
    "\n",
    "                cycle_summary_data['done'].append(np.mean(episode_summary_data['done']))\n",
    "                cycle_summary_data['reward'].append(np.mean(episode_summary_data['reward']))\n",
    "\n",
    "            cycle_data['observation'] = np.asarray(cycle_data['observation'], dtype=np.float32)\n",
    "            cycle_data['achieved_goal'] = np.asarray(cycle_data['achieved_goal'], dtype=np.float32)\n",
    "            cycle_data['desired_goal'] = np.asarray(cycle_data['desired_goal'], dtype=np.float32)\n",
    "            cycle_data['action'] = np.asarray(cycle_data['action'], dtype=np.float32)\n",
    "\n",
    "            cycle_summary_data['done'] = np.asarray(cycle_summary_data['done'], dtype=np.float32)\n",
    "            cycle_summary_data['reward'] = np.asarray(cycle_summary_data['reward'], dtype=np.float32)\n",
    "\n",
    "            # store the episodes\n",
    "            self.buffer.store_episode(**cycle_data, n_episodes_to_store=self.n_warmap_episodes)\n",
    "            self._update_normalizer(**cycle_data)\n",
    "\n",
    "\n",
    "def make_env(config):\n",
    "    if config.render_mode == 'none':\n",
    "        env = gym.make(config.env_id)\n",
    "    else:\n",
    "        env = gym.make(config.env_id, render_mode=config.render_mode)\n",
    "    # env = Monitor(env, prepare_path(config.monitor_file, experiment_name=config.experiment_name))\n",
    "    env.seed(config.seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "def as_tensor(numpy_array, device):\n",
    "    return torch.tensor(numpy_array, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "def interpret_boolean(param):\n",
    "    if type(param) == bool:\n",
    "        return param\n",
    "    elif param in ['True', 'true', '1']:\n",
    "        return True\n",
    "    elif param in ['False', 'false', '0']:\n",
    "        return False\n",
    "    else:\n",
    "        sys.exit(\"param '{}' cannot be interpreted as boolean\".format(param))\n",
    "\n",
    "\n",
    "def get_env_parameters(env):\n",
    "    # Get spaces parameters\n",
    "    observation_dim = env.observation_space.spaces['observation'].shape[0]\n",
    "    goal_dim = env.observation_space.spaces['desired_goal'].shape[0]\n",
    "\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action_value = float(env.action_space.high[0])\n",
    "\n",
    "    return observation_dim, goal_dim, action_dim, max_action_value\n",
    "\n",
    "\n",
    "def create_dirs(dirs):\n",
    "    try:\n",
    "        for dir_ in dirs:\n",
    "            if not os.path.exists(dir_):\n",
    "                os.makedirs(dir_)\n",
    "        return 0\n",
    "    except Exception as err:\n",
    "        print(\"Creating directories error: {0}\".format(err))\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "def prepare_path(path, **args):\n",
    "    res = os.path.join(ROOT_DIR_PATH, path.format(**args))\n",
    "    create_dirs([res])\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int, device: str = 'cpu') -> None:\n",
    "    # Seed python RNG\n",
    "    random.seed(seed)\n",
    "    # Seed numpy RNG\n",
    "    np.random.seed(seed)\n",
    "    # seed the RNG for all devices (both CPU and CUDA)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        # Deterministic operations for CuDNN, it may impact performances\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_agent_according_config(config):\n",
    "    env = make_env(config)\n",
    "    set_random_seed(config.seed, config.device_id)\n",
    "    print(f'Actions count: {env.action_space.shape}')\n",
    "    print(f'Action UB:   {float(env.action_space.high[0])}')\n",
    "    print(f'Action LB: {float(env.action_space.low[0])}')\n",
    "\n",
    "    agent = Agent(env, config)\n",
    "    return agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### main loop"
   ],
   "metadata": {
    "id": "i1KqPJCvi8YC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env_ids = ['FetchReach-v1']\n",
    "actor_action_distributions = ['SquashedDiagGaussianDistribution']"
   ],
   "metadata": {
    "id": "BKA8hDqWd78v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_from = 0\n",
    "run_num = 0\n",
    "for distribution in actor_action_distributions:\n",
    "  # for transition_network_type in transition_network_types:\n",
    "    # for efe_approximation_approach in efe_approximation_approaches:\n",
    "      for env_id in env_ids:\n",
    "        if run_num < start_from:\n",
    "          run_num+=1\n",
    "          continue\n",
    "\n",
    "        experiment_name = \"rerun\" + env_id +  '_' + distribution + str(run_num)\n",
    "        \n",
    "        print(f'Current experiment: {experiment_name}')\n",
    "        current_config = get_config(experiment_name, env_id, \"\", \"\", distribution)\n",
    "        sr, r = train_agent_according_config(current_config)\n",
    "        run_num+=1\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kL93lbPhgqed",
    "outputId": "aa4a0398-b917-43f5-85d5-8823c7ad9ff8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current experiment: rerunFetchReach-v1_SquashedDiagGaussianDistribution0\n",
      "experiment_description: first good experiment\n",
      "monitor_file: logs/{experiment_name}/monitor.csv\n",
      "tb_log_folder: logs/sac/tensorboard/{experiment_name}\n",
      "seed: 42\n",
      "hparams:\n",
      "  n_epochs: 500\n",
      "  steps_per_epoch: 200\n",
      "  n_training_iterations: 10\n",
      "  n_rollout_episodes: 10\n",
      "  n_warmap_episodes: 50\n",
      "  polyak: 0.95\n",
      "  replay_strategy: future\n",
      "  replay_k: 4\n",
      "  actor_layers:\n",
      "  - 128\n",
      "  - 256\n",
      "  - 128\n",
      "  transition_net_layers:\n",
      "  - 128\n",
      "  - 128\n",
      "  - 128\n",
      "  value_net_layers:\n",
      "  - 128\n",
      "  - 256\n",
      "  - 128\n",
      "  actor_lr: 0.0003\n",
      "  transition_net_lr: 0.0003\n",
      "  value_net_lr: 0.0003\n",
      "  alpha_lr: 0.0003\n",
      "  alpha: auto\n",
      "  gamma: 1.0\n",
      "  beta: 0.99\n",
      "  vae_type: none\n",
      "  memory_capacity: 50000\n",
      "  batch_size: 512\n",
      "  efe_approximation_approach: ''\n",
      "  transition_network_type: ''\n",
      "  actor_action_distribution: SquashedDiagGaussianDistribution\n",
      "  observations_seq_len: 8\n",
      "should_save_model: true\n",
      "model_path: models/sac/{experiment_name}\n",
      "model_save_timer: 1\n",
      "should_save_episode_video: true\n",
      "episode_video_timer: 5\n",
      "video_log_folder: logs/sac/gif/{experiment_name}\n",
      "device_id: cpu\n",
      "render_mode: none\n",
      "experiment_name: rerunFetchReach-v1_SquashedDiagGaussianDistribution0\n",
      "env_id: FetchReach-v1\n",
      "\n",
      "Actions count: (4,)\n",
      "Action UB:   1.0\n",
      "Action LB: -1.0\n",
      "Environment is: FetchReach-v1\n",
      "Training started at 2022-05-23 06:04:47.696053\n",
      "Epoch:    0, Step:    0, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:    1, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:    2, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:    3, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:    4, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:    5, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:    6, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:    7, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:    8, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:    9, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   10, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   11, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   12, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   13, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   14, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   15, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   16, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   17, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   18, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   19, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   20, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   21, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   22, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   23, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   24, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   25, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   26, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   27, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   28, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   29, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   30, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   31, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   32, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   33, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   34, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   35, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   36, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   37, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   38, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   39, reward: -0.96, success_rate: 0.04\n",
      "Epoch:    0, Step:   40, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   41, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   42, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   43, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   44, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   45, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   46, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   47, reward: -0.95, success_rate: 0.05\n",
      "Epoch:    0, Step:   48, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   49, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   50, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   51, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   52, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   53, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   54, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   55, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   56, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   57, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   58, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   59, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   60, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   61, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   62, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   63, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   64, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   65, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   66, reward: -0.96, success_rate: 0.04\n",
      "Epoch:    0, Step:   67, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   68, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   69, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   70, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   71, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   72, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   73, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   74, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    0, Step:   75, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   76, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   77, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   78, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   79, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   80, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:   81, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   82, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   83, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   84, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   85, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   86, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   87, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   88, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   89, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:   90, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   91, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   92, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   93, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   94, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   95, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   96, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   97, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   98, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:   99, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  100, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  101, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  102, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  103, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  104, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  105, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  106, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  107, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  108, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  109, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  110, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  111, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  112, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  113, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  114, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  115, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  116, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  117, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  118, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  119, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  120, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  121, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  122, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  123, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  124, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  125, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  126, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  127, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  128, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  129, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  130, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  131, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  132, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  133, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  134, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  135, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    0, Step:  136, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  137, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  138, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  139, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  140, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  141, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  142, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  143, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  144, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  145, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  146, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  147, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  148, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  149, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  150, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  151, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  152, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  153, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  154, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  155, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  156, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  157, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  158, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  159, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  160, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  161, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  162, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  163, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    0, Step:  164, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  165, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  166, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  167, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  168, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  169, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  170, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  171, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  172, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  173, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  174, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  175, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  176, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  177, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  178, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  179, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  180, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  181, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  182, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  183, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  184, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  185, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  186, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  187, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  188, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  189, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  190, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  191, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  192, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  193, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  194, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  195, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  196, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  197, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  198, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    0, Step:  199, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    0, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    1, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    2, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    3, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    4, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    5, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    6, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    7, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    8, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:    9, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   10, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   11, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   12, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   13, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   14, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   15, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   16, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   17, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   18, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   19, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   20, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   21, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   22, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   23, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   24, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   25, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   26, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   27, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   28, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   29, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   30, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   31, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   32, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   33, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   34, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   35, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   36, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   37, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   38, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   39, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   40, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   41, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   42, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   43, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   44, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   45, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   46, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   47, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   48, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   49, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   50, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   51, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   52, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   53, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   54, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   55, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   56, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   57, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   58, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   59, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   60, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   61, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   62, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   63, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   64, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   65, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   66, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   67, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   68, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   69, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   70, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   71, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   72, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   73, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   74, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   75, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   76, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   77, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   78, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   79, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   80, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   81, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   82, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   83, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   84, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   85, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   86, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   87, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   88, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   89, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   90, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   91, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   92, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   93, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   94, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   95, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   96, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   97, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   98, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:   99, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  100, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  101, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  102, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  103, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  104, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  105, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  106, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  107, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  108, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  109, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  110, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  111, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  112, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  113, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  114, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  115, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  116, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  117, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  118, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  119, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  120, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  121, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  122, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  123, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  124, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  125, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  126, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  127, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  128, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  129, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  130, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  131, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  132, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  133, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  134, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  135, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  136, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  137, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  138, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  139, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  140, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  141, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  142, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  143, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  144, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  145, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  146, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  147, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  148, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  149, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  150, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  151, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  152, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  153, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  154, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  155, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  156, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  157, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  158, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  159, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  160, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  161, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  162, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  163, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  164, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  165, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  166, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  167, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  168, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  169, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  170, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  171, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  172, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  173, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  174, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  175, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  176, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  177, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  178, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  179, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  180, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  181, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  182, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  183, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  184, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  185, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  186, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  187, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  188, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  189, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  190, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  191, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  192, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  193, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  194, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  195, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  196, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  197, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  198, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    1, Step:  199, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    0, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    1, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    2, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    3, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    4, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    5, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    6, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    7, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    8, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:    9, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   10, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   11, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   12, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   13, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   14, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   15, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   16, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   17, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   18, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   19, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   20, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   21, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   22, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   23, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   24, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   25, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   26, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   27, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   28, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   29, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   30, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   31, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   32, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   33, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   34, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   35, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   36, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   37, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   38, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   39, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   40, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   41, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   42, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   43, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   44, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   45, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   46, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   47, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   48, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   49, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   50, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   51, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   52, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   53, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   54, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   55, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   56, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   57, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   58, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   59, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   60, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   61, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   62, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   63, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   64, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   65, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   66, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   67, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   68, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   69, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   70, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   71, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   72, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   73, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   74, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   75, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   76, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   77, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   78, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   79, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   80, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   81, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   82, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   83, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   84, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   85, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   86, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   87, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   88, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   89, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   90, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   91, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   92, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   93, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   94, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   95, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   96, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   97, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   98, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:   99, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  100, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  101, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  102, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  103, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  104, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  105, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  106, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  107, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  108, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  109, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  110, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  111, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  112, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  113, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  114, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  115, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  116, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  117, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  118, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  119, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  120, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  121, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  122, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  123, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  124, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  125, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  126, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  127, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  128, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  129, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  130, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  131, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  132, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  133, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  134, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  135, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  136, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  137, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  138, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  139, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  140, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  141, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  142, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  143, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  144, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  145, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  146, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  147, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  148, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  149, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  150, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  151, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  152, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  153, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  154, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  155, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  156, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  157, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  158, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  159, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  160, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  161, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  162, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  163, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  164, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  165, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  166, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  167, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  168, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  169, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  170, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  171, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  172, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  173, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  174, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  175, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  176, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  177, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  178, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  179, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  180, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  181, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  182, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  183, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  184, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  185, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  186, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  187, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  188, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  189, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  190, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  191, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  192, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  193, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  194, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  195, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  196, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  197, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  198, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    2, Step:  199, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    0, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    1, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    2, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    3, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    4, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    5, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    6, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    7, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    8, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:    9, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   10, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   11, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   12, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   13, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   14, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   15, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   16, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   17, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   18, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   19, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   20, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   21, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   22, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   23, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   24, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   25, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   26, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   27, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   28, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   29, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   30, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   31, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   32, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   33, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   34, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   35, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   36, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   37, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   38, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   39, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   40, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   41, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   42, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   43, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   44, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   45, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   46, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   47, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   48, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   49, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   50, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   51, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   52, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   53, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   54, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   55, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   56, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   57, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   58, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   59, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   60, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   61, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   62, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   63, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   64, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   65, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   66, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   67, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   68, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   69, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   70, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   71, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   72, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   73, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   74, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   75, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   76, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   77, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   78, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   79, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   80, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   81, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   82, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   83, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   84, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   85, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   86, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   87, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   88, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   89, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   90, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   91, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   92, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   93, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   94, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   95, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   96, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   97, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   98, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:   99, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  100, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  101, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  102, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  103, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  104, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  105, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  106, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  107, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  108, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  109, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  110, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  111, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  112, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  113, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  114, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  115, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  116, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  117, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  118, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  119, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  120, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  121, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  122, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  123, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  124, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  125, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  126, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  127, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  128, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  129, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  130, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  131, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  132, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  133, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  134, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  135, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  136, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  137, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  138, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    3, Step:  139, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  140, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  141, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  142, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  143, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  144, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  145, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  146, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  147, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  148, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  149, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  150, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  151, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  152, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  153, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  154, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  155, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  156, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  157, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  158, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  159, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  160, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  161, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  162, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  163, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  164, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  165, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  166, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  167, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  168, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  169, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  170, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  171, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  172, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  173, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    3, Step:  174, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  175, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    3, Step:  176, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  177, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    3, Step:  178, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  179, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  180, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  181, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  182, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  183, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  184, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  185, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  186, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  187, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  188, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  189, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  190, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  191, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  192, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  193, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  194, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  195, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  196, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  197, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  198, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    3, Step:  199, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    0, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    1, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    2, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    3, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:    4, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    5, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    6, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:    7, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    4, Step:    8, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:    9, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:   10, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:   11, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:   12, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    4, Step:   13, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:   14, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:   15, reward: -0.99, success_rate: 0.01\n",
      "Epoch:    4, Step:   16, reward: -0.96, success_rate: 0.04\n",
      "Epoch:    4, Step:   17, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:   18, reward: -0.98, success_rate: 0.02\n",
      "Epoch:    4, Step:   19, reward: -0.88, success_rate: 0.12\n",
      "Epoch:    4, Step:   20, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:   21, reward: -0.97, success_rate: 0.03\n",
      "Epoch:    4, Step:   22, reward: -1.00, success_rate: 0.00\n",
      "Epoch:    4, Step:   23, reward: -0.92, success_rate: 0.08\n",
      "Epoch:    4, Step:   24, reward: -0.93, success_rate: 0.07\n",
      "Epoch:    4, Step:   25, reward: -0.87, success_rate: 0.13\n",
      "Epoch:    4, Step:   26, reward: -0.89, success_rate: 0.11\n",
      "Epoch:    4, Step:   27, reward: -0.76, success_rate: 0.24\n",
      "Epoch:    4, Step:   28, reward: -0.89, success_rate: 0.11\n",
      "Epoch:    4, Step:   29, reward: -0.83, success_rate: 0.17\n",
      "Epoch:    4, Step:   30, reward: -0.78, success_rate: 0.22\n",
      "Epoch:    4, Step:   31, reward: -0.81, success_rate: 0.19\n",
      "Epoch:    4, Step:   32, reward: -0.90, success_rate: 0.10\n",
      "Epoch:    4, Step:   33, reward: -0.83, success_rate: 0.17\n",
      "Epoch:    4, Step:   34, reward: -0.65, success_rate: 0.35\n",
      "Epoch:    4, Step:   35, reward: -0.76, success_rate: 0.24\n",
      "Epoch:    4, Step:   36, reward: -0.68, success_rate: 0.32\n",
      "Epoch:    4, Step:   37, reward: -0.58, success_rate: 0.42\n",
      "Epoch:    4, Step:   38, reward: -0.78, success_rate: 0.22\n",
      "Epoch:    4, Step:   39, reward: -0.63, success_rate: 0.37\n",
      "Epoch:    4, Step:   40, reward: -0.68, success_rate: 0.32\n",
      "Epoch:    4, Step:   41, reward: -0.64, success_rate: 0.36\n",
      "Epoch:    4, Step:   42, reward: -0.63, success_rate: 0.37\n",
      "Epoch:    4, Step:   43, reward: -0.39, success_rate: 0.61\n",
      "Epoch:    4, Step:   44, reward: -0.46, success_rate: 0.54\n",
      "Epoch:    4, Step:   45, reward: -0.31, success_rate: 0.69\n",
      "Epoch:    4, Step:   46, reward: -0.45, success_rate: 0.55\n",
      "Epoch:    4, Step:   47, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    4, Step:   48, reward: -0.39, success_rate: 0.61\n",
      "Epoch:    4, Step:   49, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    4, Step:   50, reward: -0.39, success_rate: 0.61\n",
      "Epoch:    4, Step:   51, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    4, Step:   52, reward: -0.42, success_rate: 0.58\n",
      "Epoch:    4, Step:   53, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    4, Step:   54, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:   55, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:   56, reward: -0.49, success_rate: 0.51\n",
      "Epoch:    4, Step:   57, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    4, Step:   58, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    4, Step:   59, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:   60, reward: -0.33, success_rate: 0.67\n",
      "Epoch:    4, Step:   61, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    4, Step:   62, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    4, Step:   63, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    4, Step:   64, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    4, Step:   65, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    4, Step:   66, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    4, Step:   67, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:   68, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:   69, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   70, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:   71, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    4, Step:   72, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:   73, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    4, Step:   74, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   75, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:   76, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:   77, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:   78, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   79, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:   80, reward: -0.06, success_rate: 0.94\n",
      "Epoch:    4, Step:   81, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:   82, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   83, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   84, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   85, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:   86, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   87, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:   88, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   89, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   90, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:   91, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:   92, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:   93, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   94, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:   95, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   96, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:   97, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    4, Step:   98, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:   99, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  100, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  101, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:  102, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  103, reward: -0.07, success_rate: 0.93\n",
      "Epoch:    4, Step:  104, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:  105, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  106, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:  107, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:  108, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  109, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  110, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  111, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  112, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  113, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  114, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  115, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  116, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  117, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  118, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:  119, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  120, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  121, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  122, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  123, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  124, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  125, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  126, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  127, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:  128, reward: -0.08, success_rate: 0.92\n",
      "Epoch:    4, Step:  129, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  130, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  131, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  132, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  133, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  134, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  135, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  136, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  137, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  138, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  139, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  140, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  141, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  142, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  143, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  144, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  145, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  146, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  147, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  148, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  149, reward: -0.09, success_rate: 0.91\n",
      "Epoch:    4, Step:  150, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    4, Step:  151, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  152, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  153, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  154, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  155, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  156, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  157, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  158, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  159, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  160, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  161, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:  162, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  163, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:  164, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  165, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  166, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  167, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  168, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  169, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  170, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  171, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:  172, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  173, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  174, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:  175, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  176, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  177, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  178, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  179, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    4, Step:  180, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  181, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  182, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  183, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  184, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  185, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  186, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  187, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  188, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  189, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    4, Step:  190, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    4, Step:  191, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  192, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    4, Step:  193, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    4, Step:  194, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  195, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    4, Step:  196, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    4, Step:  197, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    4, Step:  198, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    4, Step:  199, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:    0, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:    1, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    5, Step:    2, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:    3, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:    4, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:    5, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:    6, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:    7, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:    8, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:    9, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:   10, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   11, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   12, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   13, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   14, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:   15, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   16, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   17, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   18, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   19, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   20, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   21, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   22, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   23, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   24, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   25, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   26, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:   27, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   28, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:   29, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   30, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   31, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   32, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   33, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   34, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   35, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   36, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:   37, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   38, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   39, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   40, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   41, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   42, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   43, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:   44, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   45, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   46, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   47, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   48, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    5, Step:   49, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   50, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    5, Step:   51, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   52, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   53, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   54, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   55, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   56, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   57, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   58, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   59, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   60, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   61, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   62, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   63, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   64, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   65, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   66, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   67, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   68, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   69, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   70, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:   71, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   72, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   73, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   74, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   75, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   76, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   77, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:   78, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   79, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   80, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   81, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   82, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:   83, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    5, Step:   84, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   85, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   86, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:   87, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   88, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   89, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   90, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    5, Step:   91, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   92, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   93, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:   94, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:   95, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   96, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:   97, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:   98, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:   99, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  100, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  101, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  102, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    5, Step:  103, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  104, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  105, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  106, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  107, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  108, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  109, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  110, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  111, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  112, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  113, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  114, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  115, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    5, Step:  116, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  117, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  118, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  119, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    5, Step:  120, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  121, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  122, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  123, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  124, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  125, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  126, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  127, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  128, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  129, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  130, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  131, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  132, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  133, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  134, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  135, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  136, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  137, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  138, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  139, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  140, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  141, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  142, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  143, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  144, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  145, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  146, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  147, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  148, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  149, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  150, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  151, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  152, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  153, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  154, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  155, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  156, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  157, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  158, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  159, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  160, reward: -0.10, success_rate: 0.90\n",
      "Epoch:    5, Step:  161, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  162, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  163, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  164, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  165, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  166, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  167, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  168, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  169, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  170, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  171, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  172, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  173, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  174, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  175, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  176, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  177, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  178, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  179, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  180, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  181, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  182, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  183, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  184, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    5, Step:  185, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    5, Step:  186, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    5, Step:  187, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  188, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    5, Step:  189, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    5, Step:  190, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  191, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    5, Step:  192, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    5, Step:  193, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    5, Step:  194, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    5, Step:  195, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    5, Step:  196, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  197, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    5, Step:  198, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    5, Step:  199, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:    0, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:    1, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:    2, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:    3, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:    4, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:    5, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:    6, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:    7, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:    8, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:    9, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   10, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   11, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   12, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   13, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   14, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   15, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   16, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   17, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   18, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:   19, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:   20, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   21, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   22, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   23, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:   24, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   25, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   26, reward: -0.11, success_rate: 0.89\n",
      "Epoch:    6, Step:   27, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:   28, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   29, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   30, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:   31, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   32, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   33, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   34, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:   35, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   36, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   37, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   38, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   39, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   40, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   41, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   42, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   43, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   44, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   45, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   46, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   47, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:   48, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:   49, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   50, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    6, Step:   51, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   52, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   53, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   54, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   55, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:   56, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   57, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:   58, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   59, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   60, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   61, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   62, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   63, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   64, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   65, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   66, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   67, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   68, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   69, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:   70, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   71, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:   72, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   73, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   74, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   75, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:   76, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   77, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   78, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   79, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   80, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   81, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   82, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:   83, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   84, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   85, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:   86, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   87, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   88, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   89, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   90, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:   91, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:   92, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:   93, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   94, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:   95, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:   96, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   97, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:   98, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:   99, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  100, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  101, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  102, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  103, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  104, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  105, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  106, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  107, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  108, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  109, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  110, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    6, Step:  111, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  112, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  113, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  114, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  115, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  116, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  117, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  118, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  119, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:  120, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  121, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  122, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  123, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  124, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  125, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  126, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  127, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  128, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  129, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:  130, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  131, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  132, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  133, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  134, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:  135, reward: -0.13, success_rate: 0.87\n",
      "Epoch:    6, Step:  136, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  137, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  138, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  139, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  140, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  141, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  142, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  143, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  144, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  145, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  146, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  147, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  148, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  149, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  150, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  151, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  152, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  153, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  154, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  155, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  156, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  157, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:  158, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  159, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  160, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  161, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  162, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    6, Step:  163, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    6, Step:  164, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  165, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:  166, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  167, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  168, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  169, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    6, Step:  170, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  171, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  172, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  173, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  174, reward: -0.12, success_rate: 0.88\n",
      "Epoch:    6, Step:  175, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    6, Step:  176, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  177, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  178, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  179, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  180, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  181, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  182, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  183, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  184, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  185, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  186, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  187, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  188, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  189, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  190, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    6, Step:  191, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  192, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    6, Step:  193, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    6, Step:  194, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    6, Step:  195, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    6, Step:  196, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  197, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    6, Step:  198, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    6, Step:  199, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:    0, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:    1, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:    2, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:    3, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:    4, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:    5, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:    6, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:    7, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:    8, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:    9, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   10, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   11, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   12, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   13, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   14, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   15, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    7, Step:   16, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   17, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   18, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   19, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   20, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   21, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   22, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   23, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   24, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   25, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   26, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   27, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   28, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   29, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   30, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   31, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   32, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   33, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   34, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   35, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   36, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   37, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    7, Step:   38, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    7, Step:   39, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   40, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   41, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   42, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   43, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   44, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   45, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:   46, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   47, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   48, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   49, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    7, Step:   50, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   51, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   52, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   53, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   54, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    7, Step:   55, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   56, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   57, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   58, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   59, reward: -0.14, success_rate: 0.86\n",
      "Epoch:    7, Step:   60, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   61, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   62, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   63, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   64, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    7, Step:   65, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:   66, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   67, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   68, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   69, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   70, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    7, Step:   71, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   72, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   73, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   74, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    7, Step:   75, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   76, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   77, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   78, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:   79, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   80, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   81, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   82, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   83, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   84, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   85, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   86, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   87, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   88, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:   89, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:   90, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   91, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   92, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:   93, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:   94, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   95, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:   96, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   97, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:   98, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:   99, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  100, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  101, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  102, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  103, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  104, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  105, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  106, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  107, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  108, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  109, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  110, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  111, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:  112, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  113, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  114, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  115, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  116, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  117, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  118, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  119, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  120, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  121, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  122, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  123, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  124, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  125, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  126, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  127, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  128, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  129, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  130, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  131, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  132, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  133, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  134, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  135, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  136, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  137, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  138, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  139, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  140, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  141, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  142, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  143, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  144, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  145, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  146, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  147, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  148, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  149, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  150, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  151, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    7, Step:  152, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  153, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  154, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  155, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  156, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  157, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    7, Step:  158, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  159, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  160, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  161, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  162, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  163, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    7, Step:  164, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  165, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  166, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  167, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  168, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  169, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    7, Step:  170, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  171, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  172, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  173, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  174, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    7, Step:  175, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  176, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  177, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  178, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  179, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  180, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  181, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  182, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  183, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    7, Step:  184, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  185, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  186, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  187, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  188, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  189, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  190, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  191, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    7, Step:  192, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    7, Step:  193, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  194, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    7, Step:  195, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  196, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    7, Step:  197, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  198, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    7, Step:  199, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:    0, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:    1, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:    2, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:    3, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:    4, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:    5, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:    6, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:    7, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:    8, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:    9, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   10, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   11, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   12, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   13, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   14, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   15, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   16, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   17, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   18, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   19, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   20, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   21, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   22, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   23, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   24, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   25, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   26, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   27, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:   28, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   29, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   30, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   31, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   32, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    8, Step:   33, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   34, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   35, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   36, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   37, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   38, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   39, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   40, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   41, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   42, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    8, Step:   43, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   44, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   45, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   46, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   47, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   48, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   49, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    8, Step:   50, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   51, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   52, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   53, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   54, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   55, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   56, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   57, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   58, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   59, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   60, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:   61, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   62, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   63, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   64, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:   65, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    8, Step:   66, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   67, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   68, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   69, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:   70, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   71, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   72, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   73, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:   74, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   75, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   76, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   77, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   78, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   79, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:   80, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   81, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:   82, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:   83, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   84, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:   85, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   86, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   87, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   88, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   89, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   90, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   91, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   92, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   93, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:   94, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:   95, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:   96, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:   97, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:   98, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:   99, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  100, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  101, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  102, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    8, Step:  103, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:  104, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  105, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  106, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  107, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  108, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  109, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  110, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  111, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:  112, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:  113, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  114, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  115, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  116, reward: -0.15, success_rate: 0.85\n",
      "Epoch:    8, Step:  117, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  118, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  119, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  120, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  121, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:  122, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  123, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  124, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  125, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  126, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  127, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  128, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  129, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    8, Step:  130, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  131, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  132, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  133, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  134, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  135, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  136, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  137, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  138, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  139, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  140, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  141, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  142, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  143, reward: -0.17, success_rate: 0.83\n",
      "Epoch:    8, Step:  144, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  145, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  146, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  147, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  148, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:  149, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  150, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  151, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:  152, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  153, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  154, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  155, reward: -0.16, success_rate: 0.84\n",
      "Epoch:    8, Step:  156, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  157, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  158, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    8, Step:  159, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  160, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  161, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  162, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    8, Step:  163, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  164, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    8, Step:  165, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  166, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  167, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:  168, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  169, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:  170, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  171, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  172, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:  173, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  174, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    8, Step:  175, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  176, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  177, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  178, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  179, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  180, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    8, Step:  181, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  182, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    8, Step:  183, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:  184, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  185, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  186, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  187, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  188, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  189, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  190, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  191, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  192, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  193, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    8, Step:  194, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  195, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    8, Step:  196, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    8, Step:  197, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    8, Step:  198, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    8, Step:  199, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:    0, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:    1, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:    2, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:    3, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:    4, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:    5, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:    6, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:    7, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:    8, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    9, Step:    9, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   10, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   11, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   12, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   13, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   14, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:   15, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    9, Step:   16, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   17, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   18, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   19, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:   20, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   21, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   22, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   23, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   24, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   25, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   26, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   27, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   28, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:   29, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   30, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   31, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   32, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   33, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   34, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   35, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    9, Step:   36, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   37, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   38, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   39, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   40, reward: -0.32, success_rate: 0.68\n",
      "Epoch:    9, Step:   41, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   42, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   43, reward: -0.19, success_rate: 0.81\n",
      "Epoch:    9, Step:   44, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   45, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   46, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   47, reward: -0.30, success_rate: 0.70\n",
      "Epoch:    9, Step:   48, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   49, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   50, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   51, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   52, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    9, Step:   53, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   54, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:   55, reward: -0.30, success_rate: 0.70\n",
      "Epoch:    9, Step:   56, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   57, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   58, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   59, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   60, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   61, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   62, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   63, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   64, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   65, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   66, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   67, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   68, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   69, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   70, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   71, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   72, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   73, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   74, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   75, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   76, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   77, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:   78, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:   79, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   80, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    9, Step:   81, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    9, Step:   82, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   83, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   84, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   85, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   86, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   87, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    9, Step:   88, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:   89, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:   90, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   91, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   92, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:   93, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   94, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:   95, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:   96, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:   97, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   98, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:   99, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  100, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  101, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  102, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  103, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  104, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  105, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  106, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  107, reward: -0.18, success_rate: 0.82\n",
      "Epoch:    9, Step:  108, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  109, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  110, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  111, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  112, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  113, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    9, Step:  114, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  115, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  116, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  117, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  118, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  119, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  120, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  121, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  122, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  123, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:  124, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  125, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  126, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:  127, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  128, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  129, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  130, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  131, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  132, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  133, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  134, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  135, reward: -0.32, success_rate: 0.68\n",
      "Epoch:    9, Step:  136, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  137, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  138, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  139, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  140, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  141, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  142, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  143, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  144, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  145, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  146, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  147, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  148, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  149, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  150, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  151, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  152, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  153, reward: -0.21, success_rate: 0.79\n",
      "Epoch:    9, Step:  154, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  155, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  156, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  157, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  158, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:  159, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  160, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  161, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  162, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  163, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  164, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  165, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    9, Step:  166, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  167, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  168, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  169, reward: -0.31, success_rate: 0.69\n",
      "Epoch:    9, Step:  170, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  171, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  172, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  173, reward: -0.29, success_rate: 0.71\n",
      "Epoch:    9, Step:  174, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  175, reward: -0.20, success_rate: 0.80\n",
      "Epoch:    9, Step:  176, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  177, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  178, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  179, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  180, reward: -0.28, success_rate: 0.72\n",
      "Epoch:    9, Step:  181, reward: -0.25, success_rate: 0.75\n",
      "Epoch:    9, Step:  182, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  183, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  184, reward: -0.22, success_rate: 0.78\n",
      "Epoch:    9, Step:  185, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  186, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  187, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  188, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  189, reward: -0.31, success_rate: 0.69\n",
      "Epoch:    9, Step:  190, reward: -0.32, success_rate: 0.68\n",
      "Epoch:    9, Step:  191, reward: -0.32, success_rate: 0.68\n",
      "Epoch:    9, Step:  192, reward: -0.24, success_rate: 0.76\n",
      "Epoch:    9, Step:  193, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  194, reward: -0.23, success_rate: 0.77\n",
      "Epoch:    9, Step:  195, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  196, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  197, reward: -0.26, success_rate: 0.74\n",
      "Epoch:    9, Step:  198, reward: -0.27, success_rate: 0.73\n",
      "Epoch:    9, Step:  199, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   10, Step:    0, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:    1, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:    2, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:    3, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:    4, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:    5, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:    6, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:    7, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:    8, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:    9, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:   10, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   11, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   12, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   13, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   14, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   15, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   16, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   17, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   18, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   19, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   20, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   21, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:   22, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   10, Step:   23, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   24, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   25, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   26, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   27, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   28, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   29, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   10, Step:   30, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   31, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   32, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   33, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   34, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   35, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   36, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   37, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   38, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   39, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   40, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   41, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   42, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   43, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   44, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   45, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:   46, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   47, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   48, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   49, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   50, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   10, Step:   51, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   52, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   10, Step:   53, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   54, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   55, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   56, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   57, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   58, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   59, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   60, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   10, Step:   61, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   62, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   63, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   64, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   65, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   66, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   67, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   68, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   10, Step:   69, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   70, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   71, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   10, Step:   72, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   73, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   74, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   10, Step:   75, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   10, Step:   76, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   77, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   78, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   79, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   80, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   81, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:   82, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   83, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   84, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   85, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   86, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   10, Step:   87, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   88, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   89, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   90, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   91, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:   92, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:   93, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:   94, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:   95, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   10, Step:   96, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:   97, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:   98, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:   99, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  100, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  101, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  102, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  103, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  104, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  105, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  106, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  107, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  108, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  109, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  110, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  111, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  112, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  113, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  114, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  115, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   10, Step:  116, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  117, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  118, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   10, Step:  119, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   10, Step:  120, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  121, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   10, Step:  122, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:  123, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  124, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  125, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  126, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  127, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  128, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  129, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  130, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  131, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  132, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  133, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   10, Step:  134, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  135, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  136, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  137, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  138, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  139, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  140, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   10, Step:  141, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   10, Step:  142, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   10, Step:  143, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  144, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  145, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  146, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  147, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  148, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  149, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  150, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  151, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  152, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  153, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  154, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  155, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  156, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  157, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  158, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  159, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  160, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  161, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  162, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  163, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  164, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  165, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  166, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  167, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   10, Step:  168, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  169, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  170, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   10, Step:  171, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  172, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  173, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  174, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  175, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  176, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   10, Step:  177, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  178, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  179, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  180, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  181, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  182, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  183, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  184, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  185, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  186, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  187, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  188, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   10, Step:  189, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   10, Step:  190, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   10, Step:  191, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  192, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   10, Step:  193, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   10, Step:  194, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   10, Step:  195, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   10, Step:  196, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  197, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  198, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   10, Step:  199, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:    0, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:    1, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:    2, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:    3, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:    4, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:    5, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:    6, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:    7, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:    8, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:    9, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   10, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   11, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   11, Step:   12, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   11, Step:   13, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   11, Step:   14, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   15, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   16, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   17, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   11, Step:   18, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   19, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   20, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   21, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   22, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   23, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   24, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   11, Step:   25, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:   26, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   27, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   28, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   29, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   30, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   31, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   32, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   33, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:   34, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   35, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   36, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   11, Step:   37, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   38, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:   39, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   40, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   11, Step:   41, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:   42, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   43, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:   44, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   45, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   46, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   47, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   11, Step:   48, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   49, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:   50, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:   51, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   11, Step:   52, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   11, Step:   53, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   54, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   55, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:   56, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   57, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   58, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   59, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   60, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   61, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   62, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   63, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   11, Step:   64, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   65, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   66, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   67, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:   68, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:   69, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:   70, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   71, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   72, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:   73, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   74, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   75, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:   76, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   77, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   78, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   11, Step:   79, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   80, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:   81, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   82, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   83, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   84, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   85, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:   86, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   87, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   11, Step:   88, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:   89, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:   90, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   11, Step:   91, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   92, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   93, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:   94, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:   95, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   96, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:   97, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:   98, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:   99, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  100, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  101, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  102, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  103, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  104, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  105, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  106, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  107, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  108, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  109, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  110, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:  111, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  112, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  113, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  114, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  115, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   11, Step:  116, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  117, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  118, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  119, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:  120, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  121, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  122, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  123, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  124, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  125, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  126, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  127, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:  128, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  129, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:  130, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:  131, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  132, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:  133, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   11, Step:  134, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  135, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  136, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  137, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  138, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  139, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  140, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  141, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:  142, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  143, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  144, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  145, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:  146, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   11, Step:  147, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  148, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  149, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  150, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  151, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  152, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:  153, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   11, Step:  154, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   11, Step:  155, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  156, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  157, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  158, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  159, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  160, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  161, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  162, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  163, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  164, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  165, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  166, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   11, Step:  167, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  168, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  169, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  170, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  171, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   11, Step:  172, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  173, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  174, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  175, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  176, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  177, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  178, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   11, Step:  179, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   11, Step:  180, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  181, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  182, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   11, Step:  183, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  184, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   11, Step:  185, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   11, Step:  186, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  187, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   11, Step:  188, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   11, Step:  189, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  190, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:  191, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  192, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   11, Step:  193, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   11, Step:  194, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:  195, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   11, Step:  196, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   11, Step:  197, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   11, Step:  198, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   11, Step:  199, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:    0, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:    1, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:    2, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:    3, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:    4, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:    5, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:    6, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:    7, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:    8, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:    9, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   10, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   11, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   12, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   12, Step:   13, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:   14, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:   15, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:   16, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   12, Step:   17, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   18, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   19, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   20, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   12, Step:   21, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   22, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   23, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:   24, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   12, Step:   25, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   26, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   12, Step:   27, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:   28, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   29, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   30, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   31, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   32, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   33, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   34, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:   35, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   36, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   37, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:   38, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   12, Step:   39, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   40, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   41, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   42, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   43, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   44, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   45, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:   46, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   12, Step:   47, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   48, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   49, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   50, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   51, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   52, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   12, Step:   53, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:   54, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   55, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   56, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   57, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   58, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   59, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   60, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   61, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   62, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   63, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   64, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:   65, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:   66, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   67, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   12, Step:   68, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:   69, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   70, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   71, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:   72, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   12, Step:   73, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:   74, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   75, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   76, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   77, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:   78, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:   79, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   12, Step:   80, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   12, Step:   81, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:   82, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   83, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:   84, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:   85, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:   86, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   87, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:   88, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   12, Step:   89, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   90, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:   91, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   92, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   93, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   94, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   95, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:   96, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:   97, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:   98, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:   99, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  100, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   12, Step:  101, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  102, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  103, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:  104, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  105, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:  106, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:  107, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:  108, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  109, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:  110, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   12, Step:  111, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  112, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  113, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  114, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  115, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:  116, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:  117, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  118, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   12, Step:  119, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   12, Step:  120, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:  121, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  122, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:  123, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:  124, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  125, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:  126, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  127, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  128, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:  129, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   12, Step:  130, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  131, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  132, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:  133, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:  134, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  135, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  136, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  137, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  138, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  139, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  140, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  141, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   12, Step:  142, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  143, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  144, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:  145, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  146, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   12, Step:  147, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  148, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  149, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:  150, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  151, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  152, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  153, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:  154, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   12, Step:  155, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:  156, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:  157, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  158, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:  159, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   12, Step:  160, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  161, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  162, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  163, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:  164, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  165, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  166, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  167, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   12, Step:  168, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   12, Step:  169, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  170, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:  171, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   12, Step:  172, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  173, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   12, Step:  174, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   12, Step:  175, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  176, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  177, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   12, Step:  178, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   12, Step:  179, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  180, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   12, Step:  181, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   12, Step:  182, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   12, Step:  183, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   12, Step:  184, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  185, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   12, Step:  186, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  187, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   12, Step:  188, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   12, Step:  189, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  190, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   12, Step:  191, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   12, Step:  192, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   12, Step:  193, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   12, Step:  194, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   12, Step:  195, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   12, Step:  196, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   12, Step:  197, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   12, Step:  198, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   12, Step:  199, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   13, Step:    0, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:    1, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:    2, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   13, Step:    3, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   13, Step:    4, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:    5, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:    6, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:    7, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:    8, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:    9, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:   10, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   11, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   12, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:   13, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:   14, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:   15, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   13, Step:   16, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   13, Step:   17, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   13, Step:   18, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   13, Step:   19, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:   20, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:   21, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:   22, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   23, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   24, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   13, Step:   25, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   13, Step:   26, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:   27, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   28, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:   29, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   13, Step:   30, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   13, Step:   31, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   32, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:   33, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   34, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   13, Step:   35, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:   36, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:   37, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   38, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:   39, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:   40, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:   41, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   42, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   13, Step:   43, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   13, Step:   44, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:   45, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:   46, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   47, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   48, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:   49, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:   50, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   51, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:   52, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:   53, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   13, Step:   54, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   13, Step:   55, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:   56, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   13, Step:   57, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:   58, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:   59, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:   60, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:   61, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:   62, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   63, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:   64, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   65, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   13, Step:   66, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   67, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   68, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   69, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   70, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:   71, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   72, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:   73, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:   74, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   75, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   76, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:   77, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   13, Step:   78, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   13, Step:   79, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:   80, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   81, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   82, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:   83, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:   84, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:   85, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:   86, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   87, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   88, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:   89, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   13, Step:   90, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   91, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:   92, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   13, Step:   93, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   13, Step:   94, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:   95, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:   96, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:   97, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   13, Step:   98, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:   99, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:  100, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  101, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:  102, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:  103, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  104, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:  105, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  106, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:  107, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   13, Step:  108, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  109, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  110, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  111, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  112, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  113, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:  114, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  115, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  116, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  117, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  118, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  119, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  120, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  121, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  122, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  123, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:  124, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  125, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  126, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  127, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  128, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  129, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:  130, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  131, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  132, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:  133, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  134, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  135, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:  136, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:  137, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:  138, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:  139, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  140, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   13, Step:  141, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   13, Step:  142, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:  143, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  144, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  145, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:  146, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:  147, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   13, Step:  148, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:  149, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  150, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  151, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  152, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   13, Step:  153, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  154, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:  155, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:  156, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   13, Step:  157, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  158, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   13, Step:  159, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:  160, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:  161, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  162, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  163, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  164, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  165, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   13, Step:  166, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   13, Step:  167, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   13, Step:  168, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   13, Step:  169, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   13, Step:  170, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   13, Step:  171, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   13, Step:  172, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   13, Step:  173, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:  174, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  175, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:  176, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:  177, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:  178, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   13, Step:  179, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   13, Step:  180, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  181, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  182, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   13, Step:  183, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  184, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  185, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:  186, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  187, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   13, Step:  188, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   13, Step:  189, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:  190, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   13, Step:  191, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   13, Step:  192, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   13, Step:  193, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  194, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   13, Step:  195, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  196, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   13, Step:  197, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   13, Step:  198, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   13, Step:  199, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:    0, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:    1, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:    2, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:    3, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   14, Step:    4, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   14, Step:    5, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:    6, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:    7, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:    8, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:    9, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:   10, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   11, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:   12, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   13, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   14, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:   15, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:   16, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:   17, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   18, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:   19, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   14, Step:   20, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   21, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:   22, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   23, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:   24, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   14, Step:   25, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   14, Step:   26, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:   27, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   28, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   14, Step:   29, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   14, Step:   30, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:   31, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   32, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   33, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   34, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   35, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   36, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   37, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   38, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:   39, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:   40, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   14, Step:   41, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:   42, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   14, Step:   43, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   44, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   45, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   46, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:   47, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   48, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   49, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:   50, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   51, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   52, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:   53, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   54, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   55, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:   56, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:   57, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   58, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:   59, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   60, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:   61, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   62, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:   63, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:   64, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   65, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   66, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   67, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   14, Step:   68, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   69, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:   70, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   71, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   72, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   73, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:   74, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   75, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   76, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:   77, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   78, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   79, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:   80, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:   81, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   82, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   14, Step:   83, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:   84, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   85, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:   86, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   87, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   88, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:   89, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:   90, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:   91, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:   92, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   93, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:   94, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:   95, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:   96, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:   97, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   14, Step:   98, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   14, Step:   99, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  100, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   14, Step:  101, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   14, Step:  102, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   14, Step:  103, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   14, Step:  104, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   14, Step:  105, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:  106, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:  107, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  108, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  109, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:  110, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:  111, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:  112, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   14, Step:  113, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  114, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   14, Step:  115, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  116, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  117, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  118, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  119, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  120, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  121, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  122, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   14, Step:  123, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   14, Step:  124, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   14, Step:  125, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:  126, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  127, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  128, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   14, Step:  129, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   14, Step:  130, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  131, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  132, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  133, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   14, Step:  134, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  135, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:  136, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  137, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  138, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   14, Step:  139, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:  140, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  141, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:  142, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   14, Step:  143, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   14, Step:  144, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  145, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  146, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   14, Step:  147, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:  148, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   14, Step:  149, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   14, Step:  150, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:  151, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   14, Step:  152, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  153, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   14, Step:  154, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  155, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  156, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:  157, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  158, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  159, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:  160, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  161, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  162, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:  163, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:  164, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:  165, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  166, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:  167, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  168, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:  169, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  170, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:  171, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  172, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   14, Step:  173, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  174, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  175, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   14, Step:  176, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  177, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  178, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   14, Step:  179, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   14, Step:  180, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   14, Step:  181, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   14, Step:  182, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   14, Step:  183, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   14, Step:  184, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   14, Step:  185, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  186, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  187, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   14, Step:  188, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  189, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   14, Step:  190, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   14, Step:  191, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   14, Step:  192, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:  193, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   14, Step:  194, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   14, Step:  195, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   14, Step:  196, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   14, Step:  197, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  198, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   14, Step:  199, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:    0, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:    1, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:    2, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:    3, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   15, Step:    4, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:    5, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:    6, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:    7, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:    8, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:    9, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   15, Step:   10, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:   11, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   12, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   13, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   14, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:   15, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   15, Step:   16, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   17, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   18, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   19, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   20, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:   21, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:   22, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   23, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   24, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:   25, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   15, Step:   26, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   15, Step:   27, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:   28, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   15, Step:   29, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   15, Step:   30, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   15, Step:   31, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   32, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:   33, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:   34, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   35, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   36, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:   37, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   38, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   39, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:   40, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   41, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   42, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:   43, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:   44, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:   45, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:   46, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:   47, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:   48, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:   49, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:   50, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   51, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   52, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   53, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:   54, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   55, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   15, Step:   56, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:   57, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:   58, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   59, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   60, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   61, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   62, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   15, Step:   63, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:   64, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:   65, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:   66, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   67, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   68, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   69, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:   70, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:   71, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   72, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   73, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:   74, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:   75, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:   76, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   77, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:   78, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   79, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:   80, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:   81, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   82, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   83, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   84, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:   85, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:   86, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   87, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   15, Step:   88, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:   89, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:   90, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:   91, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   92, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:   93, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:   94, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:   95, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:   96, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:   97, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:   98, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:   99, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   15, Step:  100, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:  101, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  102, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  103, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:  104, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:  105, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   15, Step:  106, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  107, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  108, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  109, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  110, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:  111, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  112, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:  113, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:  114, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:  115, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  116, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   15, Step:  117, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   15, Step:  118, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   15, Step:  119, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  120, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   15, Step:  121, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  122, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:  123, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  124, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   15, Step:  125, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:  126, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:  127, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:  128, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   15, Step:  129, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  130, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:  131, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   15, Step:  132, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  133, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:  134, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   15, Step:  135, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   15, Step:  136, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   15, Step:  137, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:  138, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   15, Step:  139, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   15, Step:  140, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  141, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  142, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  143, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   15, Step:  144, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   15, Step:  145, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   15, Step:  146, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  147, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  148, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   15, Step:  149, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:  150, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  151, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  152, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   15, Step:  153, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:  154, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   15, Step:  155, reward: -0.11, success_rate: 0.89\n",
      "Epoch:   15, Step:  156, reward: -0.10, success_rate: 0.90\n",
      "Epoch:   15, Step:  157, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  158, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  159, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   15, Step:  160, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  161, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:  162, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  163, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  164, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   15, Step:  165, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   15, Step:  166, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   15, Step:  167, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  168, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   15, Step:  169, reward: -0.11, success_rate: 0.89\n",
      "Epoch:   15, Step:  170, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   15, Step:  171, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   15, Step:  172, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   15, Step:  173, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   15, Step:  174, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  175, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:  176, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  177, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   15, Step:  178, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   15, Step:  179, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  180, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  181, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:  182, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  183, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   15, Step:  184, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:  185, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   15, Step:  186, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  187, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   15, Step:  188, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:  189, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   15, Step:  190, reward: -0.09, success_rate: 0.91\n",
      "Epoch:   15, Step:  191, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   15, Step:  192, reward: -0.10, success_rate: 0.90\n",
      "Epoch:   15, Step:  193, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   15, Step:  194, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   15, Step:  195, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  196, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   15, Step:  197, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   15, Step:  198, reward: -0.07, success_rate: 0.93\n",
      "Epoch:   15, Step:  199, reward: -0.08, success_rate: 0.92\n",
      "Epoch:   16, Step:    0, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:    1, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:    2, reward: -0.10, success_rate: 0.90\n",
      "Epoch:   16, Step:    3, reward: -0.12, success_rate: 0.88\n",
      "Epoch:   16, Step:    4, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:    5, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:    6, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:    7, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:    8, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   16, Step:    9, reward: -0.07, success_rate: 0.93\n",
      "Epoch:   16, Step:   10, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:   11, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   16, Step:   12, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:   13, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:   14, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:   15, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   16, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:   17, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   18, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   16, Step:   19, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:   20, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   21, reward: -0.11, success_rate: 0.89\n",
      "Epoch:   16, Step:   22, reward: -0.08, success_rate: 0.92\n",
      "Epoch:   16, Step:   23, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:   24, reward: -0.09, success_rate: 0.91\n",
      "Epoch:   16, Step:   25, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:   26, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   16, Step:   27, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:   28, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:   29, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   30, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:   31, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:   32, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:   33, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   16, Step:   34, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   35, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:   36, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:   37, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   16, Step:   38, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   16, Step:   39, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:   40, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:   41, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   16, Step:   42, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:   43, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   44, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   16, Step:   45, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   16, Step:   46, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   16, Step:   47, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   16, Step:   48, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   16, Step:   49, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:   50, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:   51, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   16, Step:   52, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   16, Step:   53, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   16, Step:   54, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   16, Step:   55, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   16, Step:   56, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   16, Step:   57, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   16, Step:   58, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   16, Step:   59, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   16, Step:   60, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   16, Step:   61, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   16, Step:   62, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   16, Step:   63, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   16, Step:   64, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   16, Step:   65, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:   66, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:   67, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   16, Step:   68, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:   69, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:   70, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   16, Step:   71, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   16, Step:   72, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   16, Step:   73, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:   74, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   75, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:   76, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:   77, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   16, Step:   78, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   16, Step:   79, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   16, Step:   80, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:   81, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:   82, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   83, reward: -0.11, success_rate: 0.89\n",
      "Epoch:   16, Step:   84, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   85, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:   86, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:   87, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:   88, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:   89, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   90, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   16, Step:   91, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:   92, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:   93, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:   94, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:   95, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:   96, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   16, Step:   97, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   16, Step:   98, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   16, Step:   99, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:  100, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:  101, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  102, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:  103, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:  104, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  105, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  106, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  107, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:  108, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  109, reward: -0.09, success_rate: 0.91\n",
      "Epoch:   16, Step:  110, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:  111, reward: -0.12, success_rate: 0.88\n",
      "Epoch:   16, Step:  112, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  113, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:  114, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:  115, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  116, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   16, Step:  117, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   16, Step:  118, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   16, Step:  119, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   16, Step:  120, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   16, Step:  121, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:  122, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  123, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:  124, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  125, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:  126, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  127, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   16, Step:  128, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  129, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  130, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:  131, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  132, reward: -0.10, success_rate: 0.90\n",
      "Epoch:   16, Step:  133, reward: -0.12, success_rate: 0.88\n",
      "Epoch:   16, Step:  134, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  135, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  136, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   16, Step:  137, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  138, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  139, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   16, Step:  140, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   16, Step:  141, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   16, Step:  142, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:  143, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  144, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  145, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  146, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:  147, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:  148, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  149, reward: -0.12, success_rate: 0.88\n",
      "Epoch:   16, Step:  150, reward: -0.13, success_rate: 0.87\n",
      "Epoch:   16, Step:  151, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:  152, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:  153, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:  154, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  155, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   16, Step:  156, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   16, Step:  157, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   16, Step:  158, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   16, Step:  159, reward: -0.12, success_rate: 0.88\n",
      "Epoch:   16, Step:  160, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:  161, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:  162, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  163, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:  164, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   16, Step:  165, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   16, Step:  166, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   16, Step:  167, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   16, Step:  168, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  169, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   16, Step:  170, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   16, Step:  171, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   16, Step:  172, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  173, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  174, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:  175, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  176, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  177, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   16, Step:  178, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   16, Step:  179, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   16, Step:  180, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  181, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:  182, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   16, Step:  183, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   16, Step:  184, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   16, Step:  185, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  186, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  187, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  188, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:  189, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   16, Step:  190, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   16, Step:  191, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   16, Step:  192, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   16, Step:  193, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   16, Step:  194, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   16, Step:  195, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   16, Step:  196, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   16, Step:  197, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   16, Step:  198, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   16, Step:  199, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:    0, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:    1, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   17, Step:    2, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:    3, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   17, Step:    4, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:    5, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   17, Step:    6, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:    7, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   17, Step:    8, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   17, Step:    9, reward: -0.47, success_rate: 0.53\n",
      "Epoch:   17, Step:   10, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   17, Step:   11, reward: -0.50, success_rate: 0.50\n",
      "Epoch:   17, Step:   12, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   17, Step:   13, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:   14, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   15, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:   16, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   17, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:   18, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   17, Step:   19, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   17, Step:   20, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   21, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   17, Step:   22, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   17, Step:   23, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   17, Step:   24, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   17, Step:   25, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:   26, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:   27, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   17, Step:   28, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   29, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   30, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   17, Step:   31, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   32, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   33, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   17, Step:   34, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   35, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   17, Step:   36, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:   37, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   38, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   17, Step:   39, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   17, Step:   40, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   41, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   42, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:   43, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   17, Step:   44, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   17, Step:   45, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:   46, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   17, Step:   47, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:   48, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   17, Step:   49, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   17, Step:   50, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   51, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   52, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   53, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   54, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   55, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   56, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   57, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   58, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:   59, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:   60, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   61, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   17, Step:   62, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:   63, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   64, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:   65, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   66, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   67, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   17, Step:   68, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   69, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   70, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   71, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   72, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:   73, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:   74, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:   75, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   17, Step:   76, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:   77, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:   78, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   17, Step:   79, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   80, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   17, Step:   81, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   17, Step:   82, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:   83, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   84, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:   85, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:   86, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   87, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   88, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   89, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   90, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:   91, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:   92, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:   93, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   17, Step:   94, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   17, Step:   95, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:   96, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:   97, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:   98, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:   99, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:  100, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   17, Step:  101, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:  102, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:  103, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  104, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:  105, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:  106, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  107, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:  108, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:  109, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:  110, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  111, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  112, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   17, Step:  113, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  114, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:  115, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  116, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  117, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:  118, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  119, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   17, Step:  120, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:  121, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:  122, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   17, Step:  123, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:  124, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:  125, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   17, Step:  126, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:  127, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:  128, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  129, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  130, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   17, Step:  131, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   17, Step:  132, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   17, Step:  133, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   17, Step:  134, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   17, Step:  135, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   17, Step:  136, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   17, Step:  137, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   17, Step:  138, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   17, Step:  139, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:  140, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:  141, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  142, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   17, Step:  143, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   17, Step:  144, reward: -0.11, success_rate: 0.89\n",
      "Epoch:   17, Step:  145, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   17, Step:  146, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   17, Step:  147, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   17, Step:  148, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:  149, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  150, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  151, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:  152, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   17, Step:  153, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   17, Step:  154, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  155, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   17, Step:  156, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:  157, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:  158, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  159, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  160, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:  161, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   17, Step:  162, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  163, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:  164, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:  165, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   17, Step:  166, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   17, Step:  167, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  168, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  169, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   17, Step:  170, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:  171, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  172, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  173, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  174, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  175, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  176, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:  177, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  178, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  179, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:  180, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  181, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   17, Step:  182, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   17, Step:  183, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   17, Step:  184, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   17, Step:  185, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  186, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   17, Step:  187, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   17, Step:  188, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   17, Step:  189, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   17, Step:  190, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   17, Step:  191, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   17, Step:  192, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   17, Step:  193, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   17, Step:  194, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:  195, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   17, Step:  196, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:  197, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   17, Step:  198, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   17, Step:  199, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:    0, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:    1, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:    2, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:    3, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:    4, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:    5, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:    6, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:    7, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:    8, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:    9, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   10, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   11, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:   12, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:   13, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:   14, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:   15, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:   16, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   18, Step:   17, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   18, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   19, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   20, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:   21, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   22, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   18, Step:   23, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   24, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:   25, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   26, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   27, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   28, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:   29, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:   30, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   31, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:   32, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:   33, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   34, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   35, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   36, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   37, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:   38, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   39, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   40, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   41, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:   42, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   43, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:   44, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:   45, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:   46, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   18, Step:   47, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:   48, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   18, Step:   49, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:   50, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:   51, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:   52, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:   53, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:   54, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:   55, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:   56, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   57, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:   58, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:   59, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:   60, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   18, Step:   61, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:   62, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   18, Step:   63, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:   64, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   18, Step:   65, reward: -0.17, success_rate: 0.83\n",
      "Epoch:   18, Step:   66, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:   67, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:   68, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   69, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:   70, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:   71, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   18, Step:   72, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:   73, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:   74, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:   75, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:   76, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:   77, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:   78, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:   79, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:   80, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:   81, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:   82, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   18, Step:   83, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:   84, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   18, Step:   85, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   18, Step:   86, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   18, Step:   87, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:   88, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:   89, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:   90, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   18, Step:   91, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   18, Step:   92, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   18, Step:   93, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   18, Step:   94, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   18, Step:   95, reward: -0.16, success_rate: 0.84\n",
      "Epoch:   18, Step:   96, reward: -0.14, success_rate: 0.86\n",
      "Epoch:   18, Step:   97, reward: -0.15, success_rate: 0.85\n",
      "Epoch:   18, Step:   98, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   18, Step:   99, reward: -0.19, success_rate: 0.81\n",
      "Epoch:   18, Step:  100, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:  101, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:  102, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:  103, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:  104, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   18, Step:  105, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   18, Step:  106, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   18, Step:  107, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:  108, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   18, Step:  109, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:  110, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:  111, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   18, Step:  112, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   18, Step:  113, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:  114, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   18, Step:  115, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  116, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:  117, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:  118, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  119, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  120, reward: -0.21, success_rate: 0.79\n",
      "Epoch:   18, Step:  121, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:  122, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   18, Step:  123, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   18, Step:  124, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  125, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  126, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:  127, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:  128, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:  129, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  130, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:  131, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  132, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  133, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  134, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:  135, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:  136, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:  137, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   18, Step:  138, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   18, Step:  139, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  140, reward: -0.47, success_rate: 0.53\n",
      "Epoch:   18, Step:  141, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  142, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   18, Step:  143, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  144, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   18, Step:  145, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:  146, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:  147, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:  148, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:  149, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  150, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  151, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  152, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  153, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:  154, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   18, Step:  155, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  156, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:  157, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  158, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  159, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  160, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   18, Step:  161, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  162, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  163, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:  164, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   18, Step:  165, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  166, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  167, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   18, Step:  168, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  169, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  170, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  171, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  172, reward: -0.47, success_rate: 0.53\n",
      "Epoch:   18, Step:  173, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   18, Step:  174, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   18, Step:  175, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   18, Step:  176, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  177, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  178, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  179, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   18, Step:  180, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  181, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  182, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   18, Step:  183, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   18, Step:  184, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   18, Step:  185, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   18, Step:  186, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   18, Step:  187, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   18, Step:  188, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  189, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   18, Step:  190, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   18, Step:  191, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   18, Step:  192, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   18, Step:  193, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   18, Step:  194, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   18, Step:  195, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   18, Step:  196, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   18, Step:  197, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   18, Step:  198, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   18, Step:  199, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   19, Step:    0, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:    1, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   19, Step:    2, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:    3, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:    4, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:    5, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:    6, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:    7, reward: -0.25, success_rate: 0.75\n",
      "Epoch:   19, Step:    8, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:    9, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:   10, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   11, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   12, reward: -0.27, success_rate: 0.73\n",
      "Epoch:   19, Step:   13, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   19, Step:   14, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   15, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   16, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   19, Step:   17, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   18, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   19, Step:   19, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   19, Step:   20, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   21, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   22, reward: -0.18, success_rate: 0.82\n",
      "Epoch:   19, Step:   23, reward: -0.24, success_rate: 0.76\n",
      "Epoch:   19, Step:   24, reward: -0.20, success_rate: 0.80\n",
      "Epoch:   19, Step:   25, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   26, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   19, Step:   27, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:   28, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   19, Step:   29, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   30, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   31, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   32, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   33, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   34, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:   35, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   36, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   37, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   38, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:   39, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   40, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   41, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   42, reward: -0.22, success_rate: 0.78\n",
      "Epoch:   19, Step:   43, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   44, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   45, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   46, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   47, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   48, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   49, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   19, Step:   50, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   51, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   19, Step:   52, reward: -0.47, success_rate: 0.53\n",
      "Epoch:   19, Step:   53, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   19, Step:   54, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   19, Step:   55, reward: -0.46, success_rate: 0.54\n",
      "Epoch:   19, Step:   56, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   19, Step:   57, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   58, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   59, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   19, Step:   60, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   61, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   62, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   19, Step:   63, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   64, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   65, reward: -0.44, success_rate: 0.56\n",
      "Epoch:   19, Step:   66, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   19, Step:   67, reward: -0.31, success_rate: 0.69\n",
      "Epoch:   19, Step:   68, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   19, Step:   69, reward: -0.39, success_rate: 0.61\n",
      "Epoch:   19, Step:   70, reward: -0.41, success_rate: 0.59\n",
      "Epoch:   19, Step:   71, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   72, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   19, Step:   73, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   19, Step:   74, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:   75, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   76, reward: -0.43, success_rate: 0.57\n",
      "Epoch:   19, Step:   77, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   19, Step:   78, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   19, Step:   79, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   19, Step:   80, reward: -0.33, success_rate: 0.67\n",
      "Epoch:   19, Step:   81, reward: -0.30, success_rate: 0.70\n",
      "Epoch:   19, Step:   82, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   83, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   19, Step:   84, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:   85, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   19, Step:   86, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   19, Step:   87, reward: -0.23, success_rate: 0.77\n",
      "Epoch:   19, Step:   88, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:   89, reward: -0.32, success_rate: 0.68\n",
      "Epoch:   19, Step:   90, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   91, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   19, Step:   92, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:   93, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   19, Step:   94, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:   95, reward: -0.40, success_rate: 0.60\n",
      "Epoch:   19, Step:   96, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:   97, reward: -0.37, success_rate: 0.63\n",
      "Epoch:   19, Step:   98, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   19, Step:   99, reward: -0.29, success_rate: 0.71\n",
      "Epoch:   19, Step:  100, reward: -0.42, success_rate: 0.58\n",
      "Epoch:   19, Step:  101, reward: -0.28, success_rate: 0.72\n",
      "Epoch:   19, Step:  102, reward: -0.35, success_rate: 0.65\n",
      "Epoch:   19, Step:  103, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:  104, reward: -0.34, success_rate: 0.66\n",
      "Epoch:   19, Step:  105, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:  106, reward: -0.38, success_rate: 0.62\n",
      "Epoch:   19, Step:  107, reward: -0.36, success_rate: 0.64\n",
      "Epoch:   19, Step:  108, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   19, Step:  109, reward: -0.49, success_rate: 0.51\n",
      "Epoch:   19, Step:  110, reward: -0.48, success_rate: 0.52\n",
      "Epoch:   19, Step:  111, reward: -0.45, success_rate: 0.55\n",
      "Epoch:   19, Step:  112, reward: -0.26, success_rate: 0.74\n",
      "Epoch:   19, Step:  113, reward: -0.28, success_rate: 0.72\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-9d0e882cb540>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Current experiment: {experiment_name}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mcurrent_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdistribution\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0msr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_agent_according_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcurrent_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m         \u001B[0mrun_num\u001B[0m\u001B[0;34m+=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-13-0ef495978937>\u001B[0m in \u001B[0;36mtrain_agent_according_config\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m   1330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1331\u001B[0m     \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAgent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1332\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-13-0ef495978937>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1067\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1068\u001B[0m                         \u001B[0;31m# feed the actions into the environment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1069\u001B[0;31m                         \u001B[0mnew_observation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1070\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1071\u001B[0m                         \u001B[0mepisode_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0machieved_goal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdesired_goal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001B[0;32m---> 18\u001B[0;31m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_max_episode_steps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/robotics/robot_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step_callback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 77\u001B[0;31m         \u001B[0mobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_obs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0mdone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/robotics/fetch_env.py\u001B[0m in \u001B[0;36m_get_obs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0mgrip_pos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_site_xpos\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"robot0:grip\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m         \u001B[0mdt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnsubsteps\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimestep\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0mgrip_velp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_site_xvelp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"robot0:grip\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m         \u001B[0mrobot_qpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrobot_qvel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrobot_get_obs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhas_object\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mwrappers.pxi\u001B[0m in \u001B[0;36mmujoco_py.cymj.PyMjData.get_site_xvelp\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mdp_sac_agent_all.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "qQFmTc45aGHn",
    "o5OZo3XyzE3c",
    "_IQN-i3OzJ3k",
    "h6rjacSoi3jY",
    "i1KqPJCvi8YC"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}